## PCA 와 LDA 

#### 출처 : 

세종대학교 유성준 교수님 -  파이썬을 활용한 기계학습 개론



* epdl

PCA : 

#### 영상 자막 :

1. 이번 시간에는 PCA와 lda에 대해 살펴보도록 하겠습니다.
2. 여기서 사용할 데이터 차원축소에 대한 개념을 이해하고, PCA와 LDA에 대한 개념을 이해합니다. 그리고 데이터를 사용해 PCA와 LDA를 실습하게 됩니다.
3. 이번 시간에는 데이터의 차원 축소 방법에 대해서 학습을 하겠습니다. 먼저 데이터의 차원이 어떤 개념인지 살펴보도록 하죠.
4. 데이터의 차원이라고 하면 독립변수의 개수를 의미를 합니다.
5. 쉽게 예를 들어서 한번 살펴보도록 할까요?
6. 타이타닉호 탑승자들의 생존 여부에 영향을 주는 요소가 탑승자들의 좌석, 그리고 성별, 나이, 이렇게 세 가지 요소가 있다고 가정을 해보죠.
7. 이때 타이타닉호 탑승자들의 생존 여부를 결정하는 파라미터 공간은 바로 독립변수인 좌석, 성별, 나이와 같은 세 가지로 이루어진 3차원 공간이 됩니다.
8. 이렇게 데이터의 독립변수 개수에 따라서 데이터의 차원이 결정이 되죠. 하지만 이 차원이 커지게 되면 효율적인 작업이 힘들어집니다.
9. 그러한 상태를 차원의 저주라고도 합니다.
10. 차원의 저주는 데이터의 차원이 커질수록, 즉 변수의 개수가 증가할수록,
11. 모델 구성에 필요한 데이터의 개수가 기하급수적으로 증가를 하는데 반해서 이때 해당 특징의 데이터가 잘 제공이 되지 않는다는 문제가 있다는 것이죠.
12. 이렇게 될 경우에는 과최적화 등의 문제가 발생을 합니다.
13. 즉 데이터가 부족해서 부족한 데이터에 맞는 기계학습 모델이 만들어지는 문제가 있는 것이죠.
14. 이러한 문제점들을 극복하기 위해서 데이터에서 핵심이 되는 특징들 또는 이들 특징들로부터 얻을 수 있는 주성분만 선택을 해서 사용을 하면 이러한 문제를 해결할 수도 있습니다.
15. 이번 시간에 학습할 대표적인 두 가지 차원 축소 방법은 PCA와 LDA라고 하는 것입니다.
16. PCA는 Principal Component Analysis의 약자로서 주성분 분석이라고 합니다.
17. PCA는 데이터 분포의 분산이 큰 임의의 축을 찾는 것이 첫 단계에서 할 일입니다.
18. 그림을 보면 붉은색 데이터와 푸른색 데이터의 분포를 나타내는 선들 중에서 데이터를 사상시켰을 경우에 가장 큰 분산을 가진 축을 찾는 작업을 하고 있음을 확인할 수가 있습니다.
19. 두 번째 대표적인 방법은 LDA라고 하는 것인데요. Linear Discriminant Analysis라고 하는 말의 약자가 되겠습니다. 선형 판별 분석이라고 하죠.
20. 데이터의 클래스 정보를 유지를 하면서 분리하는 축을 찾는 방법을 의미합니다.
21. 그림을 한번 살펴보도록 하겠습니다. 데이터를 직선에 사상시킨 결과, 붉은색 클래스와 푸른색 클래스가 적절하게 분리돼서 있는 것을 알 수가 있습니다.
22. 이와 같이 클래스 정보를 유지를 하면서 데이터를 분리할 수 있는 축을 찾는 방법을 LDA라고 합니다.
23. 지금부터 PCA, LDA, 이 두 가지 방법을 차례대로 자세히 살펴보도록 하겠습니다.
24. 먼저 PCA입니다. 앞에서 살펴보았듯이 고차원의 데이터를 저차원의 데이터로 변환하는 차원 축소 방법 중의 하나죠.
25. PCA를 사용해서 찾은 주성분은 데이터 분포의 특성을 가장 잘 설명할 수 있는 벡터가 됩니다.
26. 즉, 데이터의 분산이 가장 큰 축이 첫 번째 주성분이 되고, 두 번째로 분산이 큰 축이 두 번째 주성분이 됩니다.
27. PCA를 사용해서 찾은 요소들은 데이터의 분포를 가장 잘 나타낸 요소들이므로 이를 사용을 해서 데이터를 분석을 할 수가 있습니다.
28. 그림을 통해 조금 더 쉽게 살펴보도록 하겠습니다. 보시는 그림은 2차원 공간에 분포하고 있는 데이터의 산포도입니다.
29. 이 붉은색 점들의 분포를 가장 잘 표현하는 축 2개가 바로 2개의 파란색 선이죠. 첫 번째 PC, 여기서 1st PC라고 했는데 이 첫 번째 성분 벡터를 한번 보겠습니다.
30. 이 축에 데이터를 사상시킬 경우에 데이터의 분산이 가장 크다는 것을 알 수가 있죠.
31. 2nd PC, 즉 두 번째 주성분 벡터 또한 첫 번째 주성분 벡터 다음으로 데이터의 분산이 큰 축입니다.
32. 그럼 지금부터 이 주성분 벡터를 구하는 방법을 살펴보도록 하죠. 주성분은 고유벡터, eigenvector와 고유값 eigenvalue를 사용해서 구합니다.
33. 고유벡터와 고유값은 항상 쌍으로 존재한다고 하죠. 이때 고유벡터는 데이터의 분포를 나타내는 선입니다.
34. 그리고 고유값은 해당 고유벡터에 데이터를 사상시켰을 때 데이터가 분포하는 분산을 의미합니다.
35. 예를 들어 살펴보겠습니다. 보시는 그림에서 붉은색 점은 데이터를 의미하는데요. 이 데이터의 분포를 나타내는 선이 고유벡터입니다.
36. 그리고 이 고유벡터에 데이터를 사상시킨 다음에 분산을 구하면 이 값이 바로 고유값이 됩니다
37. 이렇게 데이터의 주성분을 구해서 데이터에 어떻게 적용을 하면 좋을까요? 우선 데이터의 고유벡터와 고유값 쌍의 개수는 데이터가 가지고 있는 속성, 차원의 개수와 같습니다.
38. 예를 들어서 데이터가 2개의 속성인 나이, 사용 시간을 가지고 있다고 할 때, 이 데이터의 고유벡터와 고유값 또한 2쌍이 되는 것이죠.
39. 따라서 2차원 데이터의 경우 2개의 고유벡터가 수직일 때 데이터가 두 고유벡터를 기준으로 큰 분산으로 분포하고 있다는 것을 의미합니다.
40. 그러면 이 두 고유벡터를 기준으로 해서 데이터를 재구성하는 것이 가능하게 되죠.
41. 그림에서 1 사분면에 분포하고 있는 데이터의 주성분 벡터는 파란색 벡터, 고유벡터 1과 고유벡터 2가 됩니다.
42. 이 2개의 고유벡터를 새로운 축 new x, new y로 삼아서 새로운 차원을 구성을 할 수가 있습니다. 이때 데이터에는 변화가 생기지 않습니다.
43. 다만 데이터의 분포를 나타내는 축만 변화하게 됩니다.
44. 다음 그림이 바로 앞에서 확인한 주성분 벡터 2개를 새로운 축으로 지정을 한 모습입니다. 두 개의 새로운 축 new x와 new y를 기준으로 데이터의 분포를 표시하고 있습니다.
45. 이와 같이 새로운 축을 기반으로 데이터가 큰 분산을 가지고 분포를 할 경우에 데이터를 통해서 정보를 찾기가 쉬워집니다.
46. 이번에는 PCA와 함께 대표적으로 사용되는 차원 축소 방법인 LDA에 대해서 학습을 해보겠습니다. LDA는 데이터를 클래스별로 잘 분리하는 벡터를 찾는 방법입니다.
47. 그림을 통해서 조금 더 쉽게 살펴보도록 하죠. 다음 그림은 두 개의 속성 feature 1과 feature 2를 축으로 해서 데이터의 분포를 나타낸 그래프입니다.
48. 여기에서 붉은색 클래스와 푸른색 클래스를 잘 분리할 수 있는 벡터를 찾아내는 방법이 바로 LDA입니다.
49. 두 클래스를 잘 분리할 벡터를 찾기 위해서 벡터 b와 벡터 d를 기준으로 해서 데이터를 분리를 합니다.
50. 그리고 각 벡터에 수직인 벡터 a와 벡터 c의 데이터를 사상시킵니다. 사상시킨 결과는 보시는 그림과 같습니다.
51. 우선 벡터 d를 기준으로 해서 데이터를 분리를 하고 여기에 수직인 벡터 a에 데이터를 사상을 시키고 첫 번째 그림을 살펴보도록 하겠습니다.
52. 그림에서 확인할 수 있듯이 두 개의 클래스가 제대로 분리되어 있지 않다는 것을 확인할 수가 있습니다.
53. 그럼 그 다음 그림을 살펴보겠습니다.
54. 벡터 d를 기준으로 해서 데이터를 분리를 하고 여기에 수직인 벡터 c에 데이터를 사상을 시킨 결과를 볼 수 있죠.
55. 여기서 우리는 벡터 c에 데이터가 클래스 별로 적절하게 분리되어 있다는 것을 확인할 수 있습니다.
56. 이와 같이 LDA는 데이터를 사상시켰을 때 다음의 조건들을 만족하는 벡터를 찾는 작업입니다.
57. 첫 번째 조건은 각 클래스의 중심 간 거리가 최대인 벡터, 즉 클래스 간 분리를 잘 이루어내야 한다는 것이죠.
58. 두 번째 조건은 각 클래스 내 데이터의 분산이 최소인 벡터, 즉 같은 클래스 내의 데이터들이 밀집되게 분리돼야 된다는 것입니다.
59. 따라서 벡터 a를 사용을 하면 두 클래스를 잘 분리시키면서 차원을 축소를 할 수가 있습니다.
60. 지금까지 우리는 PCA와 LDA가 각각 어떠한 방법으로 데이터의 차원을 축소하는지를 살펴보았습니다.
61. 그럼 지금부터 데이터를 사용을 해서 PCA와 LDA 실습 코드가 어떻게 구성이 되는지 살펴보도록 하겠습니다.
62. 우선 분석에 필요한 패키지들을 import를 해야 되겠죠. 차원 축소 결과를 시각화해서 살펴보기 위해서 첫 번째 줄에서 환경을 설정을 해봅니다.
63. 그리고 pyplot 패키지를 import를 합니다.
64. 그리고 scikit learn에서 기본으로 제공되는 데이터 셋을 사용하기 위해서 datasets 모듈을 import를 합니다.
65. 마지막으로 PCA와 LDA를 진행하기 위해서 각 모듈을 import를 합니다.
66. 이제 분석에 사용할 데이터를 살펴보도록 하겠습니다. 우리는 scikit learn에서 제공하는 toy data 중에서 유방암 데이터를 사용을 해보겠습니다.
67. 이 데이터를 사용하기 위해서 load breast cancer라고 하는 함수를 통해서 데이터를 로드를 합니다. 그리고 이것을 변수 data라고 하는 것에 저장을 합니다.
68. 그리고 데이터의 속성들을 확인하기 위해서 feature names라고 하는 함수를 사용해서 출력을 해봅니다.
69. 그 결과, 다음과 같은 속성들의 이름을 확인할 수가 있고 이 유방암 데이터가 30개의 속성, 즉 30차원의 데이터 셋이라는 것을 알 수가 있죠.
70. 이 원본 데이터의 분포를 시각화해서 살펴보기 위해서 우리는 첫 번째와 두 번째 속성, 두 가지만 선택해 사용할 예정입니다.
71. 이를 위해서 data의 data, 즉 속성 부분에서 앞 두 개의 속성을 선택을 해서 변수 x에 저장을 합니다. 그러면 x에는 mean radius와 mean texture가 저장이 되죠.
72. 다음으로 데이터의 클래스 정보를 target 함수를 사용해서 가져와가지고 변수 y에 저장을 합니다.
73. 더불어 클래스 이름을 target names라고 하는 함수를 사용해서 가져와가지고 변수 target names에 저장을 해보죠.
74. 그러면 target names에는 악성과 양성이라고 하는 정보가 저장이 됩니다.
75. 이제 준비한 원형 데이터를 산포도로 그려서 살펴보겠습니다. 우선 산포도를 그리기 위해서 figure의 크기를 fig size를 사용해서 설정을 합니다.
76. 그리고 클래스마다 다른 색상으로 표시를 하기 위해서 color 변수에 red, blue 값을 저장을 해둡니다.
77. 이제 for 문을 사용을 해서 데이터를 하나씩 산포도에 그리는 작업을 진행합니다.
78. 우선 zip 함수를 사용을 해서 색상, 클래스, 그리고 클래스 이름을 묶습니다.
79. zip 함수를 사용을 하면 동일한 개수로 이루어진 자료형을 묶을 수 있는데, 여기에서는 길이 2인 리스트 세 개를 한 묶음으로 묶습니다.
80. 그리고 색상, 클래스 정보, 클래스 이름을 for 문 내부에 적용을 합니다.
81. scatter 함수를 사용을 해서 데이터 한 건을 하나의 점으로 표현을 하는데요.
82. 속성 x의 클래스 y가 i. 즉 클래스가 0인지 1인지 확인을 한 다음에 x의 0번 속성을 x축 값, 1번 속성을 y축 값으로 지정을 합니다.
83. 그리고 해당 클래스 인덱스에 해당하는 색상으로 데이터를 표시를 하고 해당 라벨을 클래스 이름으로 설정을 하죠
84. legend 함수를 사용을 해서 도표 설명, 즉 클래스 색상과 클래스 이름 정보를 표시를 하고
85. xlabel과 ylabel 함수를 사용을 해서 각 축의 이름 mean radius와 mean texture를 표시를 해봅니다.
86. 마지막으로 이 산포도를 보기 위해서 show 함수를 사용을 합니다.
87. 이렇게 구성해 그린 결과를 확인하면 다음 그림과 같습니다.
88. 그림을 살펴보면 두 속성에 따라서 그린 산포도상에서 두 클래스 악성과 양성 데이터가 분리하기 힘들게 섞여 있는 것을 볼 수가 있습니다.
89. 이렇게 기본적으로 주어진 속성에 따라서 분리하기 어려운 데이터 셋에 차원 축소 방법을 적용을 해서 그 차이를 한번 확인을 해보죠.
90. 그러면 먼저 PCA를 적용을 해서 살펴보도록 하겠습니다. 먼저 사용할 데이터 셋을 준비해보겠습니다.
91. 데이터의 속성들을 변수 x에 저장을 하는데, 앞에서 한 방식과는 다르게 30개의 속성 모두를 변수 x에 저장을 합니다.
92. 우리는 PCA를 사용해서 30차원의 데이터를 축소하고 사용할 계획입니다.
93. 다음으로 유방암 악성과 양성 클래스 정보를 target 함수를 사용을 해서 변수 y에 저장을 하고 클래스 이름을 변수 target names에 저장을 합니다.
94. 이제 데이터 셋에 PCA를 적용을 해보도록 하겠습니다. 우선 PCA 라이브러리를 사용을 해서 주성분 분석을 하기 위한 모듈 PCA를 생성합니다.
95. 이때 주성분을 두 개 추출하기 위해서 n component를 2로 설정을 합니다. 여기에서 주성분을 두 개를 추출하는 이유는 2차원 공간에 산포도를 그려보고 비교를 하기 위해서입니다.
96. 생성된 모듈을 fit 함수를 사용을 해서 속성 데이터 x에 대해 훈련을 시켜봅니다.
97. 이 훈련된 모델에 transform 함수를 사용을 해서 데이터 x에 대한 차원 축소를 진행을 해봅니다. 그리고 그 결과를 변수 x_p에 저장을 하죠.
98. 그리고 우리는 훈련된 pca 모델의 두 주성분에 대한 분산을 확인할 수가 있습니다. explained variance ratio 함수를 사용하면 되는데요.
99. 그 결과 첫 번째 주성분의 분산은 0.98, 두 번째 주성분의 분산은 0.01임을 알 수가 있습니다. 이렇게 PCA 분석을 마쳤습니다.
100. 그 결과를 시각화해서 차이를 한번 확인을 해보겠습니다. 방법은 앞의 원본 데이터에 적용한 방법과 같습니다.
101. 다만 산포도에 표시할 데이터 셋이 x가 아닌 PCA 결과로 수집한 주성분 두 개의 x_p라는 것이 차이점이죠.
102. 그리고 주성분 두 개에 따른 데이터의 산포도이므로 그래프 정보도 수정을 합니다.
103. x축 이름은 principal component 1로, y축 이름은 principal component 2로 표시를 합니다.
104. 그 결과를 시각화하면 보시는 그림과 같습니다.
105. 30차원의 데이터를 2차원으로 축소를 해서 산포도를 그려본 결과 두 개의 클래스가 원본과는 다르게 분리되어 있다는 것을 확인할 수가 있죠.
106. 양성 클래스 데이터의 경우 한곳에 뭉쳐 분포하고 있고, 악성 클래스 데이터의 경우 상반되게 옆에 넓게 분포하고 있습니다.
107. 비록 일부 겹쳐져 있긴 하지만 원본에 비해서 두 클래스 분포가 잘 분리되어 있는 것을 알 수가 있죠.
108. 그러면 이제 두 번째 차원 축소 방법인 LDA에 대해서 실습을 해보겠습니다. 이번에도 마찬가지로 사용할 데이터 셋을 먼저 준비를 합니다.
109. PCA 데이터 셋이 30개였는데 여기도 30개를 다 사용을 합니다. 그것을 x에 저장을 합니다.
110. 클래스 정보는 변수 y에, 그리고 클래스 이름은 변수 target names에 저장을 합니다.
111. 이제 데이터 셋에 LDA를 적용을 해보겠습니다. 우선 LDA를 사용하기 위해서 라이브러리 linear discriminant analysis를 사용을 합니다.
112. 선형 판별 분석을 위해서 모듈 lda를 생성합니다. 이때 고유값을 사용을 해서 클래스를 구분하는 벡터를 구하기 위해 solve를 eigen으로 설정을 합니다.
113. 그리고 앞과 마찬가지로 2차원으로 축소를 하기 위해서 n components를 2로 설정을 합니다.
114. 이렇게 생성한 모듈 lda를 fit 함수를 사용을 해서 속성 데이터 x와 클래스 데이터 y에 대한 학습을 진행을 합니다
115. PCA와는 다르게 LDA는 클래스 정보를 가지고 진행하기 때문에 fit 함수에 클래스 데이터 y를 함께 적용합니다.
116. 훈련된 모델 lda를 사용하기 위해서 transform 함수를 사용을 하는데 x, 속성 데이터에 대해서 차원 축소를 진행을 합니다.
117. 이 차원 축소 결과, 즉 도출된 주성분 2개를 변수 x_l에 저장을 합니다
118. 이제 LDA 적용 결과를 시각화해 보도록 하겠습니다. 여기도 앞과 같은 방법으로 산포도를 그립니다.
119. 다만 차이점은 산포도에 표시할 데이터가 LDA 분석 결과인 x_l이라는 것이죠.
120. 그리고 각 축의 이름도 LD1, LD2로 바꿔서 표시를 합니다.
121. 그 결과 그려진 산포도를 살펴보면 보시는 그림과 같습니다.
122. LD1 축에 데이터를 사상시킨다고 가정을 해볼까요? 그러면 두 클래스가 각각 적절하게 분리된다는 것을 알 수가 있습니다.
123. 이렇게 원본 데이터가 두 개의 차원 축소 방법, PCA, LDA를 실습을 해보고 산포도로 비교를 해보았습니다.
124. 다음 세 개의 그림을 보면 그 차이를 더욱 확실히 느낄 수 있습니다. 세 그림은 차례대로 원본, PCA, LDA의 결과입니다.
125. 30개의 속성, 즉 30차원의 데이터에 PCA, LDA를 적용을 해서 2차원으로 축소를 해본 결과죠.
126. 원본 데이터는 두 클래스의 데이터가 분리하기 힘들게 서로 섞여서 분포하고 있는데 반면에 차원 축소 결과인 두 그림은 차이가 있어 보입니다.
127. PCA, LDA 결과 모두 두 클래스가 눈으로 확인을 할 수 있을 만큼 적절하게 분리되어 있습니다.
128. 이와 같이 데이터가 가지고 있는 기본 속성으로 구분하기 어려운 높은 차원의 데이터 셋에서 주성분을 추출을 하면,
129. 또는 차원 축소를 하게 되면 데이터를 표현하는 데 훨씬 효과적이라는 것을 확인할 수가 있었습니다.
130. 이와 같이 데이터 셋의 차원이 클 경우 발생하는 분석의 어려움을 차원 축소를 통해서 극복을 할 수 있다는 것을 보셨죠.
131. 여러분, 강의는 잘 들으셨나요? 이번 시간에 배운 내용을 정리해보는 시간을 가져보겠습니다.
132. 이번 시간에는 PCA와 LDA, 실습 순으로 학습해 보았습니다. 먼저 차원이란 독립 변수의 개수를 의미합니다.
133. 차원 축소의 방법으로는 PCA라는 방법이 있는데 고차원의 데이터를 저차원의 데이터로 변환하는 차원 축소의 방법입니다.
134. 차원축소 방법 중 LDA는 데이터를 클래스 별로 잘 분리할 수 있는 벡터를 찾는 방법을 말합니다.
135. 