## ë©€í‹° ì•”ë“œ ë°´ë”§(Multi-Armed Bandits)

#### ìˆœì„œ

##### Multi-armed Bandits

1.A k-armed Bandit Problem

2.Action-value Method

3.The 10-armed Testbed

4.Incremental Implementation

5.Tracking a Nonstationary Problem

6.Optimistic Initial values

7.Upper-Confidence-Bound Action Selection

8.Gradient Bandit Algorithms

#### Multi-Armed Bandits

Reinforcement learningëŠ” ë‹¤ë¥¸ learningê³¼ êµ¬ë¶„ì§€ì„ìˆ˜ ìˆëŠ” íŠ¹ì§•ìœ¼ë¡œëŠ” ,

actionì„ evaluate í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

ì´ë ‡ê²Œ evaluateí•˜ëŠ” ê³¼ì •ì€ ì¢‹ì€ behaviorì„ ì°¾ê¸° ìœ„í•´ active explorationì„ í•œë‹¤ëŠ” ê²ƒì´ã….

Feedbackì˜ ì¢…ë¥˜ê°€ ë‘ê°€ì§€ê°€ ìˆëŠ”ë°,

- ì²«ë²ˆì§¸ëŠ” evaluaute feedbackìœ¼ë¡œ ì„ íƒëœ actionì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” feedback
- ë‘ë²ˆì§¸ëŠ” intructive feedbackìœ¼ë¡œ ì„ íƒë˜ì–´ì§„ actionê³¼ ë…ë¦½ì ì¸ feedback

ì´ ì±•í„°ì—ì„œëŠ” Reinforcement learning ( ê°•í™”í•™ìŠµ ) ì˜ evaluate feedbackì— ëŒ€í•˜ì—¬ í•œ ìƒí™©ì— ëŒ€í•´ì„œë§Œ í–‰ë™ì„ í•˜ë„ë¡ í•™ìŠµí•˜ëŠ”

ê°„ë‹¨í•œ ì˜ˆì œë¥¼ í†µí•˜ì—¬ ë°°ìš¸ ê²ƒì´ë‹¤.

### 1. A k-armed Bandit Problem

K-armed Bandit ì˜ learning problemì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

==â€œBandit problems embody in essential form a conflict evident in all human action: choosing actions which yield immediate reward vs. choosing actions (e.g. acquiring information or preparing the ground) whose benefit will come only laterâ€ - P. Whittle (1980).==

1.k-different optionì´ë‚˜ actionì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.

2.í•œ ë²ˆ actionì„ ì„ íƒí•  ë•Œë§ˆë‹¤ stationary probability distributionì—ì„œ numberical rewardë¥¼ ë°›ëŠ”ë‹¤.

3.ëª©í‘œëŠ” í•œì •ëœ ì‹œê°„ë™ì•ˆ expected total rewardë¥¼ maximize ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.

k-armed Banditì˜ ì˜ë¯¸ëŠ” kê°œì˜ leverê°€ ìˆëŠ” slotmachineì´ë‹¤.

![2155136](https://user-images.githubusercontent.com/11300712/38657749-732b5122-3e11-11e8-810f-aed3abf5a43f.png)

http://www.primarydigit.com/

ê° actionì„ ì„ íƒí• ë•ŒëŠ” machineì˜ leverë¥¼ ë‹¹ê¸°ëŠ” ê²ƒê³¼ ê°™ê³ ,

rewarëŠ” jackpotì„ í„°íŠ¸ë ¤ ë°›ëŠ” ëˆê³¼ ê°™ë‹¤.

actionì„ selectí•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ë©° rewardë¥¼ ë§ì´ ì£¼ëŠ” ìµœê³ ì˜ lever ì°¾ì•„ ìŠ¹ë¥ ì„ ìµœëŒ€í™” ì‹œí‚¨ë‹¤.

k-armed bandit problem ì—ì„œëŠ” ê° actionì´ ì„ íƒë˜ì—ˆì„ë•Œ, ê·¸ actionì„ expected ë˜ëŠ” mean rewardë¥¼ í•œë‹¤.

ì´ì™€ ê°™ì€ ê²ƒì„ value of the action ( ê·¸ í–‰ë™ì„ í•œ ê°€ì¹˜)ë¼ê³  í•œë‹¤.

ê·¸ ì‹œê°„ì— ì„ íƒëœ actionì„ AtAt ë¼ê³  í•˜ë©°, correspondingí•œ rewardë¥¼ Reward RtRtë¼ê³  í•œë‹¤.

Action aê°€ ì„ íƒë˜ì—ˆì„ë•Œì˜ expected rewardë¥¼ qâˆ—(a)qâˆ—(a) ë¼ê³  í•˜ë©°, ë‹¤ìŒì˜ ì‹ìœ¼ë¡œ ì •ì˜í•œë‹¤.

qâˆ—(a)â‰ğ”¼[Rt|At=a]qâˆ—(a)â‰E[Rt|At=a]

**ë§Œì•½ ê° actionì˜ valueë¥¼ ì•Œê³  ìˆë‹¤ë©´, ê°€ì¥ ë†’ì€ rewardë¥¼ ë°›ëŠ” ì„ íƒë§Œ ì„ íƒ**í•˜ë¯€ë¡œ bandit ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

ì—¬ê¸°ì„œëŠ” action valueë¥¼ ëª¨ë¥¸ë‹¤ê³  ê°€ì •í•˜ë©°,

ì‹œê°„ (time ste t)ì—ì„œ ì„ íƒëœ actionì„ estimate valueë¥¼ Qt(a)Qt(a)ë¥¼ qâˆ—(a)qâˆ—(a)ê°€ ë˜ê¸°ë¥¼ ì›í•œë‹¤.

Action valueë¥¼ ì¸¡ì •í• ë•Œ í•­ìƒ ìµœì†Œí•œ í•˜ë‚˜ì˜ ationì€ ê°€ì¥ í° valueë¥¼ estimateí• ê²ƒì´ë‹¤. (ì˜ˆë¥¼ ë“¤ì–´ì„œ 10ê°œì˜ ë ˆë²„ë¥¼ ë‹¹ê²¼ì„ë•Œ ìµœì†Œí•œ í•œê°œì˜ ë ˆë²„ëŠ” ê·¸ì¤‘ ê°€ì¥ í° rewardë¥¼ ì¤„ ê²ƒì´ë‹¤.)

ìœ„ì™€ ê°™ì´ ê°€ì¥ í° valueë¥¼ ì„ íƒí•˜ëŠ” actionì„ greedyí•œ action ì´ë¼ê³  ë¶€ë¥´ë©°,

greedy í•œ actionì„ ì„ íƒí• ë•ŒëŠ” exploitingí•œë‹¤ê³  í•œë‹¤.

ë°˜ëŒ€ë¡œ, ë§Œì•½ greedy í•˜ì§€ ì•Šì€ actionì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•œë‹¤ë©´ explorating í•œë‹¤ê³  í•œë‹¤.

Exploitationìœ¼ë¡œ í•œ stepì—ì„œì˜ expected rewardë¥¼ maximizeí•˜ê¸°ìœ„í•´ greedy actionì„ ì„ íƒí•  ìˆ˜ ìˆì§€ë§Œ,

í˜„ì¬ ë‹¹ì¥ ë°›ëŠ” rewardê°€ ì ë”ë¼ë„, long termì—ì„œëŠ” explorationì„ í•¨ìœ¼ë¡œì„œ ë” ë†’ì€ rewardë¥¼ ë°›ì„ ìˆ˜ë„ ìˆë‹¤.

### 2. Action-value Methods

ACtion ì„ ì„ íƒí–ˆì„ë•Œì˜ action valueë¥¼ estimateí•˜ëŠ” ë°©ë²•ì— ëŒ€í•˜ì—¬ ìì„¸íˆ ì•Œì•„ë³´ê² ë‹¤.

ë¨¼ì € ì—¬ê¸°ì„œ True action valueì˜ ì •ì˜ëŠ” actionì´ ì„ íƒë˜ì—ˆì„ë•Œì˜ í‰ê·  rewardì´ë‹¤.

ì´ True action valueë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì€ ì‹¤ì œë¡œ ë°›ì„ rewardë¥¼ í‰ê· í•œ ê°’ì´ë‹¤.

Qt(a)â‰sumofrewardswhenatakenpriortotnumberotimesatakenpriortotQt(a)â‰sumofrewardswhenatakenpriortotnumberotimesatakenpriortot

=âˆ‘tâˆ’1i=1Riâ‹…1Ai=aâˆ‘tâˆ’1i=11Ai=a=âˆ‘i=1tâˆ’1Riâ‹…1Ai=aâˆ‘i=1tâˆ’11Ai=a

ì—¬ê¸°ì„œ 1predicate1predicate ì€ predicateê°€ trueë©´ 1, ì•„ë‹ˆë©´ 0ì´ë‹¤.

ë§Œì•½denominatorê°€ 0ì´ë©´ Qt(a)Qt(a)ëŠ” default value ì •ì˜í•œë‹¤.

ê·¸ë¦¬ê³  denominatorê°€ ë¬´í•œëŒ€ë¡œ ê°€ë©´ Qt(a)Qt(a)ëŠ” $$q_*(a)*ë¡œ convergeí•œë‹¤.

ì´ëŸ° ë°©ë²•ì„ action valueë¥¼ estimateí•œ **sample-average**ë¼ê³  í•˜ë©°, ì´ì™¸ì—ë„ action valueì„ estimateí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ actionì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì—ëŠ” ë¬´ì—‡ì´ ìˆì„ê¹Œ?

1. GreedyactionGreedyaction
2. Îµâˆ’greedyÎµâˆ’greedy

1.Greedyâˆ’actionGreedyâˆ’action

Atâ‰argmaxaQt(a)Atâ‰argmaxaQt(a)

greedy actionì€ ìœ„ì™€ê°™ì´ ì •ì˜í•˜ë©°, argmaxaargmaxa ëŠ” QtQtë¥¼ ê°€ì¥ maximize ì‹œì¼œì£¼ëŠ” action.

í˜„ì¬ì˜ ì •ë³´ë¥¼ í†µí•´ ë°”ë¡œ ë°›ëŠ” rewardë¥¼ maximze í•˜ëŠ” actionì„ ì„ íƒí•œë‹¤.

2.Îµâˆ’greedyÎµâˆ’greedy

ê°€ë” epsilonì˜ í™•ë¥ ë¡œ greedyí•œ actionì´ ì•„ë‹Œ ë‹¤ë¥¸ actionì„ ì„ íƒí•˜ëŠ” Îµâˆ’greedyÎµâˆ’greedy ë°©ë²•ì´ ìˆë‹¤.

exploitationìœ¼ë¡œ í•œ stepì—ì„œì˜ expected rewardë¥¼ maximazeí•˜ëŠ” actionì„ ì„ íƒí•  ìˆ˜ ìˆì§€ë§Œ, ì–´ì©Œë©´ long termì—ì„œëŠ” explorationì„

í•˜ì—¬ ë¯¸ë˜ì— ë” ë†’ì€ rewardë¥¼ ì–»ì„ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤.

### 3. The 10-armed Testbed

Greedyì™€ Îµâˆ’greedyÎµâˆ’greedy ë°©ë²•ì„ ì‹¤í—˜í•˜ê¸° ìœ„í•´ ë¹„êµ ì‹¤í—˜ì„ í•˜ì˜€ë‹¤.

k-arm bandit ( k = 10)ì—ì„œ 2000ê°œì˜ ë°ì´í„°ë¥¼ random í•˜ê²Œ ìƒì„±í•˜ì˜€ê³ ,( normal distribution, mean 0, variance 1ì˜ ì¡°ê±´)

![10arm](https://user-images.githubusercontent.com/11300712/38712359-08b73d6a-3f06-11e8-9b27-12e0d2d09e14.JPG)

ë‘ë²ˆì§¸ëŠ” greedy method ì™€ Îµâˆ’greedyÎµâˆ’greedy methodì˜ ë¹„êµì´ë‹¤. (ÎµÎµ= 0.1 ê³¼ ÎµÎµ= 0.01)

10 Arm banditì—ì„œ sample-average techniqueë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

ì•„ë˜ì˜ ê·¸ë˜í”„ëŠ” experience ì— ë”°ë¼ expected rewardê°€ ì¦ê°€í•˜ëŠ” ê·¸ë˜í”„ì´ë‹¤.

greedy ê°€ Îµâˆ’greedyÎµâˆ’greedy ë³´ë‹¤ average rewardê°€ ì¦ê°€í•˜ëŠ” ëŸ‰ì´ ëŠë¦¬ë‹¤.

![epsilon](https://user-images.githubusercontent.com/11300712/38712617-d9693e3a-3f07-11e8-8df5-4d61cc3f120b.JPG)

step 1000ì—ì„œì˜ average rewawrdë„ greedyë°©ë²•ì´ 1 ì •ë„ë¡œ ê°€ì¥ ë‚®ë‹¤.

gredy actionì€ suboptimal actionì— ê°‡íˆê¸° ì‰½ê²Œ ë•Œë¬¸ì— Îµâˆ’greedyÎµâˆ’greedyì— ë¹„í•´ ì„±ëŠ¥ì´ ë‚˜ì˜ë‹¤.

ë°‘ì˜ graphì—ì„œ greedyëŠ” ê¸ˆë°© optimalí•œ actionì„ ì°¾ê³  ë”ì´ìƒ í–¥ìƒë˜ì§€ ì•ŠëŠ”ë‹¤.

Îµâˆ’greedyÎµâˆ’greedy ëŠ” ê³„ì† optimal action ì„ ì°¾ê¸° ìœ„í•´ exploreí•˜ê³  ê°œì„ í•˜ì—¬ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤.

### 4.Incremental Implementation

ì§€ê¸ˆê¹Œì§€ ë‹¤ë£¬ actin-value methodëŠ” ê´€ì°°ëœ rewardë¥¼ sample averageí•˜ì—¬ estimateí•œ ê²ƒì´ë‹¤.

ì¢€ ë” íš¨ìœ¨ì ì¸ ë°©ë²•ìœ¼ë¡œ action-valueë¥¼ ì¸¡ì •í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ?

QnQnì€ actionì´ n-1ë§Œí¼ ì„ íƒëœ í›„ì— estimate ëœ action value ì´ë‹¤.

QnQnì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

Qnâ‰R1+R2+....Rnâˆ’1nâˆ’1Qnâ‰R1+R2+....Rnâˆ’1nâˆ’1

Obvious implementationì€ ëª¨ë“  rewardë¥¼ ê¸°ë¡í• ë•Œ, estimate valueê°€ í•„ìš”í•œë•Œ ì–¸ì œë¼ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

í•˜ì§€ë§Œ rewardê°€ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤ memoryì™€ computationì´ ëŠ˜ì–´ë‚  ê²ƒì´ë‹¤.

í•˜ì§€ë§Œ Incremental formularì„ ì‚¬ìš©í•˜ë©´ ìœ„ì˜ ë°©ì‹ì´ í•„ìš”ì—†ë‹¤.

Qnâ‰R1+R2+....Rnâˆ’1nâˆ’1Qnâ‰R1+R2+....Rnâˆ’1nâˆ’1

ì˜ ì‹ì„ ì•„ë˜ì™€ ê°™ì´ ë°”ê¿€ìˆ˜ ìˆë‹¤.

![qn 1](https://user-images.githubusercontent.com/11300712/38713568-4871163c-3ec1-11e8-9d5e-249ae8871b50.JPG)

ì´ Implementation ë°©ë²•ì€ QnQnê³¼ nn, RnRnì˜ ë©”ëª¨ë¦¬ë§Œ ìˆìœ¼ë©´ ëœë‹¤.

*Simple Bandit Algorithm* ì˜ PseudocodeëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![bandital](https://user-images.githubusercontent.com/11300712/38713729-2aa102a6-3ec2-11e8-8665-acc1f16c46eb.JPG)

1.Qì™€ Nì„ 0ìœ¼ë¡œ ì´ˆê¸°ê°’ì„ ì¤€ë‹¤.

2.Actionì€ Îµâˆ’greedyÎµâˆ’greedy ë¡œ ì„ íƒëœë‹¤.

3.ì„ íƒëœ Actionì„ í†µí•˜ì—¬ Rewardë¥¼ ë°›ëŠ”ë‹¤.

4.N(A)+1ë¡œ N(A)ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.

5.Reward - Q(A) ê°’ì— 1/N(A)ë¥¼ í•´ì¤€ê²ƒì— Q(A)ë¥¼ ë”í•´ì£¼ì–´ Q(A)ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.

ì—¬ê¸°ì„œ Reward - Q(A)ëŠ” estimateì˜ errorë¼ê³  í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•  ìˆ˜ ìˆë‹¤.

NewEstimateâ†OldEstimate+StepSize[Targetâˆ’OldEstimate]NewEstimateâ†OldEstimate+StepSize[Targetâˆ’OldEstimate]

[ Target - OldEstimate ] ê°€ estimateì˜ errorì´ë©°, ì´ errorëŠ” stepì„ ì§„í–‰í•˜ë©´ì„œ Targetì— ê°€ê¹Œì´ ê°ˆìˆ˜ë¡ ì¤„ì–´ë“ ë‹¤.

Targetì€ ì´ë™í•˜ì—¬ì•¼ í•˜ëŠ” ë°”ëŒì§í•œ ë°©í–¥ì´ë©°,*ì—¬ê¸°ì„œ* Targetì€ në²ˆì§¸ì˜ rewardì´ë‹¤.

Step-size parameterëŠ” time step ë§ˆë‹¤ì˜ incremental methodì´ë©°, action a ì˜ në²ˆì§¸ step sizeëŠ” 1/n ì´ë‹¤.

Î±Î±ë˜ëŠ” Î±t(a)Î±t(a) ë¼ê³  ì •ì˜í•˜ê¸°ë„ í•œë‹¤.

### 5. Tracking a Nonstationary Problem

Stationary í•œ Bandit problemì„ Averageí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì•˜ë‹¤.

(ì—¬ê¸°ì„œ stationary í•˜ë‹¤ëŠ” ê²ƒì€ rewardì˜ probabilitesê°€ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤.)

Reinforcement learningì—ì„œëŠ” ê°€ë” nonstationaryí•œ ìƒí™©ê³¼ ë§ˆì£¼ì¹˜ëŠ”ë°,

(ì˜ˆë¥¼ ë“¤ì–´ ìµœê·¼ì˜ ë°›ì€ rewardë¥¼ ì˜¤ë˜ì „ì— ë°›ì€ rewardë³´ë‹¤ weightë¥¼ ë” ë§ì´ ì¤€ë‹¤.)

ì´ê²ƒì„ í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ìœ ëª…í•œ ë°©ë²•ì€ ste-size parameterì„ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, Incremental Update rule ( n-1ì˜ rewardì˜ í‰ê·  Qnì„ update )

ë˜ëŠ” stepë§ˆë‹¤ step-size parameterì„ ë‹¤ë¥´ê²Œ í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.

ì´ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

Qn+1â‰Qn+Î±[Rnâˆ’Qn]Qn+1â‰Qn+Î±[Rnâˆ’Qn]

ì—¬ê¸°ì„œ Î±Î± ëŠ” Î±âˆˆ(0,1]Î±âˆˆ(0,1]ì´ë©°,

Qn+1Qn+1 ëŠ” weighted averageì˜ ê³¼ê±° rewardì´ë‹¤.

ìœ„ì˜ ì‹ì„ í’€ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í• ìˆ˜ ìˆë‹¤.

![asdasd](https://user-images.githubusercontent.com/11300712/38720290-aa1d6880-3f30-11e8-917e-1e6f7e0cb87c.JPG)

ì´ê²ƒì€ weighted averageë¼ê³  ë¶€ë¥´ëŠ”ë° weightì˜ í•©ì´ í•­ìƒ 1ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.

![total1](https://user-images.githubusercontent.com/11300712/38720570-b1a43754-3f31-11e8-895d-abea0fc8ac96.JPG)

Î±(1âˆ’Î±)(nâˆ’i)RiÎ±(1âˆ’Î±)(nâˆ’i)Ri ì—ì„œ RewardëŠ” n-iì— ë”°ë¼ weighted ë˜ì–´ì§„ë‹¤.

### 6. Optimistic Initial Values

Exploration ë°©ë²•ìœ¼ë¡œ epsilon-greedyë¥¼ ì‚¬ìš©í•˜ì—¬ explorationì„ í•˜ì˜€ëŠ”ë°, ì—¬ê¸°ì„œëŠ” ê¼­ ì–´ë– í•œ epsilonì˜ í™•ë¥ ë¡œ

non-greedyí•œ actionì„ ì„ íƒí•˜ëŠ” epsilon-greedy ë°©ë²•ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” Initial action valueì„ 0ì´ ì•„ë‹Œ ë†’ì€ ê°’ìœ¼ë¡œ ì£¼ëŠ” ê²ƒì´ë‹¤.

ì´ê²ƒì€ action aë¥¼ ì„ íƒí• ë•Œ starting valueê°€ rewardë³´ë‹¤ ë” í¬ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ actionì„ ì„ íƒí•˜ë©°, ê·¸ë¡œ ì¸í•´ explorationì„

encourageí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤.

ì•„ë˜ì˜ ê·¸ë˜í”„ëŠ” 10-armed bandit ì—ì„œ ì´ë¥¼ ì‹¤í—˜í•œ ê²°ê³¼ì´ë‹¤.

![differencebw5and0](https://user-images.githubusercontent.com/11300712/39024242-9131b0fc-447a-11e8-90f4-4b5109694209.JPG)

Optimistic greedy ë°©ë²•ì€ action valueë¥¼ 0ì´ ì•„ë‹Œ 5ë¡œ ì´ˆê¸°í™” í•´ì£¼ì—ˆê³  epsilon ì€ 0ìœ¼ë¡œ í•­ìƒ greedyí•œ actionë§Œ ì„ íƒí•œë‹¤.

ê·¸ë ‡ê¸°ì— ì´ˆë°˜ì— explorationì„ ë¹„êµì  ë§ì´ í•œë‹¤.

realistic, epsilon-greedy ë°©ë²•ì€ epsilon ê°’ì€ 0.1ì´ë©°, ì´ˆê¸° action value ëŠ” 0 ì´ë‹¤.

ë‘ greedy ë°©ë²•ì„ 10-armed bandit ì—ì„œ ì‹¤í—˜í•´ë³¸ ê²°ê³¼,

ì´ˆê¸° (200 steps) ê¹Œì§€ëŠ” realistic ë°©ë²•ì´ optimistic ë°©ë²•ì— ë¹„í•´ optimal actionì„ ì°¾ì„ í™•ë¥ ì´ ë” ë†’ì•˜ì§€ë§Œ,

200stepsì´í›„ì—ëŠ” optimistic ë°©ë²•ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

ì´ëŠ” optimistic ë°©ë²•ì´ exploreë¥¼ ë§ì´ í•˜ê¸° ë•Œë¬¸ì— ì²˜ìŒì—ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ì§€ë§Œ, ì‹œê°„ì— ë”°ë¼ explorationì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— ê²°êµ­

ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

### 7. Upper-Confidence-Bound Action selection

action -value estimationì˜ ì •í™•ë„ëŠ” uncertaintyí•˜ê¸°ì— explorationì€ í•­ìƒ í•„ìš”í•˜ë‹¤.

Greedy actionì€ í˜„ì¬ì—ì„œ Bestí•œ actionì„ ì„ íƒí•œë‹¤.

í˜„ì¬ rewardë¥¼ ê°€ì¥ ë§ì´ ë°›ëŠ” actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë¯¸ë˜ì˜ rewardë„ ê°€ì¥ ë§ì´ ë°›ëŠ” action ì´ë¼ê³  í™•ì‹¤í• ìˆ˜ ìˆì„ê¹Œ?

ì–´ì©Œë©´ í˜„ì¬ rewardëŠ” ì‘ê²Œ ë°›ëŠ” actionì´ë¼ë„, ë¯¸ë˜ì—ëŠ” ë” ì¢‹ì€ rewardë¥¼ ë°›ëŠ” actionì¼ìˆ˜ë„ ìˆë‹¤.

Epsilon-greedy action selectionì€ ê°•ì œë¡œ non-greedyí•œ actionì„ ì„ íƒí•œë‹¤.

í•˜ì§€ë§Œ actionì„ preferenceë¥¼ í•˜ì§€ ì•Šê³  ë¬´ì‘ìœ„ë¡œ ê²°ì •ì„ í•œë‹¤.

non-greedy actionì„ ì„ íƒí• ë•Œ ì‹¤ì œë¡œ optimalì´ ë  ì ì¬ë ¥ì´ ë†’ì€ actionì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë”ìš± íš¨ìœ¨ì ì´ì§€ ì•Šì„ê¹Œ?

Atâ‰argmaxa[Qt(a)+clntNt(a)â€¾â€¾â€¾â€¾â€¾â€¾âˆš]Atâ‰argmaxa[Qt(a)+clntNt(a)]

ìœ„ì˜ ì‹ìœ¼ë¡œ [Qt(a)+clntNt(a)â€¾â€¾â€¾â€¾â€¾âˆš][Qt(a)+clntNt(a)] ì„ ì¶”ê°€í•˜ì—¬ actionì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì

ì—¬ê¸°ì„œ lntlntëŠ” natural logarithmì˜ t ì´ë©°, Nt(a)Nt(a) Nt(a)Nt(a)ëŠ” prior ë¶€í„° time t ê¹Œì§€ action aê°€ ì„ íƒëœ íšŸìˆ˜ì´ë‹¤.

C>0C>0ëŠ” exlporationì˜ ì •ë„ë¥¼ ì¡°ì •í•˜ë©°, Nt(a)Nt(a) ê°€ 0 ì¼ë•ŒëŠ” maximizing actionì„ ì„ íƒí•œë‹¤.

ìœ„ì™€ ê°™ì€ ì•„ì´ë””ëŸ¬ë¥¼ Upeer confidence bound (UCB)action selection valueë¼ê³  í•œë‹¤. ì—¬ê¸°ì„œ [Qt(a)+clntNt(a)â€¾â€¾â€¾â€¾â€¾âˆš][Qt(a)+clntNt(a)] ì€ uncertaintyë¥¼ ì¸¡ì • ë˜ëŠ” a valueì˜ variance ê°’ì„ ì¸¡ì •í•œë‹¤.

Nt(a)Nt(a)ê°€ ì»¤ì§ˆìˆ˜ë¡ uncertainty termì´ decreaseí•˜ë©°, ì•„ë˜ì˜ ê·¸ë˜í”„ì™€ ê°™ì´ performì´ ì¢‹ì•„ë³´ì´ì§€ë§Œ

ì‚¬ì‹¤ì€ banditë¬¸ì œë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ reinforcement learningì—ì„œëŠ” epsilon-greedy ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤.

![ucbdiff](https://user-images.githubusercontent.com/11300712/39026793-df8db124-4489-11e8-8d91-d3dbfec5071c.JPG)

### 8. Gradient Bandit Algorithms

ì§€ê¸ˆê¹Œì§€ëŠ” actionì˜ valueë¥¼ estimateí•œ ê°’ì„ êµ¬í•˜ì˜€ê³ , actionì„ ì„ íƒí• ë•Œ ê·¸ action valueë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ì˜€ë‹¤.

ë‹¤ë¥¸ approachëŠ” numberical preferenceë¥¼ ë°°ìš°ìëŠ” ëª©ì ì´ë©°, ( ê°ê° actionì˜ preference )

denoteëŠ” ë‹¤ìŒê³¼ ê°™ì´ Ht(a)Ht(a)ë¡œ í•©ë‹ˆë‹¤.

preferenceê°€ ì»¤ì§€ë©´ ê·¸ actionì€ ë” ë§ì´ ì„ íƒë  ê²ƒì´ê³ , í•˜ì§€ë§Œ rewardì—ëŠ” preferenceê°€ ì—†ë‹¤.

Gradient ë°©ë²•ì—ì„œëŠ” actionì„ ì„ íƒí• ë•Œ ì•„ë˜ì™€ ê°™ì´ soft-max distributionì„ ë”°ë¥¸ë‹¤.

Pr{At=a}â‰eHt(a)âˆ‘kb=1eHt(b)â‰Ï€t(a)Pr{At=a}â‰eHt(a)âˆ‘b=1keHt(b)â‰Ï€t(a)

Ï€t(a)Ï€t(a) ëŠ” time tì—ì„œ action aë¥¼ ì„ íƒí•  í™•ë¥ ì´ë‹¤. ëª¨ë“  preferenceì˜ ì´ˆê¸°ê°’ì€ ê°™ê¸° ë•Œë¬¸ì— (ì˜ˆë¥¼ ë“¤ì–´ H1(a)=0H1(a)=0) ì´ˆê¸°í™” í–ˆì„ë•Œ ëª¨ë“  actionì€ ê°™ì€ í™•ë¥ ë¡œ ì„ íƒë˜ì–´ì§„ë‹¤.

Stochastic gradient ascentì˜ ideaë¥¼ ê¸°ë°˜ìœ¼ë¡œ natual learning algorithm ì´ ìˆë‹¤.

ê° Stepë§ˆë‹¤, AtAtë¥¼ ì„ íƒí•œ í›„ RtRtë¥¼ ë°›ê³ , ë‹¤ìŒì˜ ì‹ì„ í†µí•˜ì—¬ preferenceê°€ updateëœë‹¤.

![updateru](https://user-images.githubusercontent.com/11300712/39028363-3a355e18-4491-11e8-8a5a-9f7048544fe9.JPG)

Î±>0Î±>0 ì€ step-size parameterì„ RÂ¯âˆˆâ„RÂ¯âˆˆR time të¥¼ í¬í•¨í•œ ëª¨ë“  rewardì˜ averageëŠ” ìœ„ì—ì„œ ë´¤ë˜

update ruleì— ì˜í•˜ì—¬ ê³„ì‚°ëœë‹¤.

NewEstimateâ†OldEstimate+StepSize[Targetâˆ’OldEstimate]NewEstimateâ†OldEstimate+StepSize[Targetâˆ’OldEstimate]

RÂ¯tRÂ¯t ëŠ” rewardê°€ ë¹„êµë˜ëŠ” baselineìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

ë§Œì•½ ê·¸ rewardê°€ baselineë³´ë‹¤ ë†’ìœ¼ë©´ At(a)At(a)ê°€ ì„ íƒë  í™•ë¥ ì´ ë†’ì•„ì§€ë©°, ë°˜ëŒ€ë¡œ rewardê°€ baselineë³´ë‹¤ ë‚®ìœ¼ë©´ ê·¸ At(a)At(a)ê°€ ì„ íƒë  í™•ë¥ ì´ ì¤„ì–´ë“ ë‹¤.

ì•„ë˜ì˜ ê·¸ë˜í”„ëŠ” baselineì„ ì‚¬ìš©í–ˆì„ë•Œì™€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ë•Œì˜ ë¹„êµì´ë‹¤.

stepsì— ë”°ë¼ optimal actionì„ ì„ íƒí•  í™•ë¥ ì¸ë°,

![optimalbaseline](https://user-images.githubusercontent.com/11300712/39030705-a677713a-449e-11e8-8446-79818b8bf57d.JPG)

baselineì„ ì‚¬ìš©í•œ ê²½ìš°ê°€ ê·¸ëŸ¬ì§€ ì•Šì•˜ì„ë•Œë³´ë‹¤ ì›”ë“±íˆ optimal actionì„ ì„ íƒí•  í™•ë¥ ì´ ë†’ì•˜ë‹¤.

### ì •ë¦¬

ì§€ê¸ˆê¹Œì§€ Multi-armed Banditì— ëŒ€í•˜ì—¬ ì•Œì•„ë³´ì•˜ë‹¤.

1.A k-armed Bandit Problem

2.Action-value Method

3.The 10-armed Testbed

4.Incremental Implementation

5.Tracking a Nonstationary Problem

6.Optimistic Initial values

7.Upper-Confidence-Bound Action Selection

8.Gradient Bandit Algorithms

ì´ ì±•í„°ì—ì„œ Multi-armed bandit ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•˜ì—¬ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ì‹œë„ ë˜ì—ˆê³ ,

ì´ëŠ” ê³ ì „ì•Œê³ ë¦¬ì¦˜ì˜ ì „ì²´ì˜ ì•„ì´ë””ì–´ì™€ ë™ì¼í•˜ë‹¤ê³  ìƒê°í•œë‹¤.

ë‹¤ìŒ ì±•í„°ëŠ” MDP ( Markov decision process )ë¡œ Bandit ì²˜ëŸ¼ actionì„ ì„ íƒí–ˆì„ë•Œ rewardë§Œ ë°›ëŠ”ê²ƒì´ ì•„ë‹Œ,

actionì„ ì„ íƒí•˜ë¯€ë¡œì„œ state ( ìƒíƒœ ) ê°€ ë‹¬ë¼ì§„ë‹¤.

ì—¬ê¸°ì„œ Agent, State, Environmentì˜ ê°œë…ì´ ì¶”ê°€ë˜ëŠ”ë°,

ìì„¸í•œ ê²ƒì€ ë‹¤ìŒ ì±•í„°ì— ì•Œì•„ë³´ê² ë‹¤.

## Reference

- Reinforcement Learning: An Introduction Richard S. Sutton and Andrew G. Barto Second Edition, in progress MIT Press, Cambridge, MA, 2017

