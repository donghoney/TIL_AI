MDP 

순차적 행동 결정 문제를 수학적으로 정의한 것이 MDP 입니다.

MDP는 상태, 행동, 보상, 상태 변환 확률, 감가율, 정책으로 구성돼 있습니다.

순차적 행동 결정 문제를 푸는 과정은 더 좋은 정책을 찾는 과정입니다.



가치함수

에이전트가 어떤 정책이 더 좋은 정책인지 판단하는 기준이 가치함수이다.

가치함수는 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합입니다.



에이전트는 정책을 업데이트할 때 가치함수를 사용할 텐데, 보통 가치함수보다는 에이전트가 선택할 각 행동의 가치를 직접적으로 나타내는 큐함수를 사용합니다.



벨만 방정식

현재 상태의 가치함수와 다음 상태 가치함수의 관계식이 벨만 방정식입니다.

벨만 기대 방정식은 특정 정책을 따라갓을 때 가치함수 사이의 관계식이다.



더 좋은 정책을 찾아가다 보면 최적의 정책을 찾을 것입니다.

최적의 정책은 최적의 가치함수를 받게 하는 정책이며, 그 때 가치함수 사이의 관계씩이 벨만 최적 방정식입니다.