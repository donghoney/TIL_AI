## 파이썬과 케라스를 이용한 딥러닝 / 강화학습 주식투자

### 01장 . 배경이론 1 : 딥러닝이란?

### 1.1 딥러닝 개요

#### 1.1.1 딥러닝의 정의와 역사

머신러닝 : ML

인공 신경망 : ANN

인공지능 : AI

1950년대 퍼셉트론 등장 -> 1960년대 퍼셉트론 한계 증명 -> 1970년대 인공신경망 암흑기 -> 1980년대 인공 신경망 부활기 (오차 역전파 적용, DNN, RNN, CNN 발전) -> 1990년대 LSTM등장,LeNet-5등장 -> 2000년대 가트너 10대 전략 기술 -> 2010년대 알파고

#### 1.1.2 딥러닝이 최근에 주목받는 이유

컴퓨터 연산능력이 좋아졌음 , 빅데이터 등장

#### 1.1.3 딥러닝으로 풀고자 하는 문제

문제 : 분류 , 군집화, 회귀

학습 방법 : 지도학습, 비지도 학습, 강화학습

```
강화학습 : 에이전트가 어떠한 환경에서 행동을 수행했을 때 보상을 함으로써 , 에이전트는 그 보상을 최대로 하는 행동을 수행하도록 학습하게 하는 방법. 
행동 결정 분류 문제나 보상 예측의 회귀 문제를 다룸.
레이블이 없는 데이터를 학습할 수 있지만, 에이전트와 환경을 구성하는 추가적인 비용을 필요로 함.
```

### 1.2 딥러닝의 발전 과정

#### 1.2.1 퍼셉트론

```
퍼셉트론 : 입력값(input)과 편향값(bias)에 가중치(weight)를 곱하여 합한 값이 threshold를 넘어가면 1, 그렇지 않으면 0을 출력하는 활성화 함수를 가지는 구조
```



#### 1.2.2 인공 신경망

```
* 인공 신경망(ANN) : input layer , hidden layer, output layer로 구성된 연결망
* 은닉층(hidden layer)에서 활성화 함수를 사용.
```

* ##### 활성화 함수 종류 

  step function, ReLU function, leaky ReLU function, sigmoid function, hyperbolic tangent(tanh) function

  #### forward activation functions

  * step function
    * $$h(z) = \begin{cases} 1,&z>0이면 \\ -1,&그\ 외\end{cases}​$$
  * ReLU function
    * $$h(z)=\begin{cases}z,&z>0이면\\ 0,&그\ 외\end{cases}$$

  * LeakyReLU function
    * $$h(z)=\begin{cases}z,&z>0이면\\z*\alpha,&그\ 외 \end{cases}$$
  * sigmoid function
    * $$h(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{e^z+1} $$
  * Hyperbolic tangent(tanh) function
    * $$h(z)=\frac{sinhz}{coshz}=\frac{\frac{1-e^{-2z}}{2e^{-2z}}}{\frac{1+e^{-2x}{}}{2e^{-z}}}=\frac{1-e^{-2z}}{1+e^{-2z}}​$$

* 출력층에서도 활성화 함수 사용

  * Sigmoid 또는 Softmax 사용
  * 출력층에서의 sigmoid : 이항 분류 문제에서 출력 값을 확률화
  * 출력층에서의 softmax : 다항 분류 문제에서 출력 값을 확률화

  

#### 1.2.3 심층 신경망

```
심층 신경망(DNN): 은닉층이 2개 이상인 ANN
```

### 1.3 딥러닝에 필요한 핵심 기술

#### 1.3.1 오차 역전파 기법

```
오차 역전파 : forward된 weight를 편미분한 값을 backward 형태로 곱해 나가면서 그 최종 역전파 값을 가중치에 더해서 조정

```

* #### back propagation functions
  * Step function
    * $$\frac{\delta y}{\delta z}=\begin{cases}0,&z>0이면\\ 0,& 그\ 외 \end{cases} =0​$$
  * ReLU function
    * $$\frac{\delta y}{\delta z}=\begin{cases}1,&z>0이면\\ 0,& 그\ 외 \end{cases}$$
  * sigmoid function
    * $$\frac{\delta y}{\delta z}=y(1-y)​$$
  * Hyperbolic tangent(tanh) function
    * $$\frac{\delta y}{\delta z}=1-y^2$$
  * Softmax function
    * $$\frac{\delta y_i}{\delta z_i}=y_i-t_i(t_i는\ i번\ 째\ 레이블)$$

  ```
  * 역전파 함수 설명
  sigmoid function의 순전파를 미분하면 모두 0이 되어 가중치에 영향을 주지 않음.
  ReLU function은 가중치 곱의 합이 양수일 때만 뒤쪽 계층에서 넘어온 역전파를 그대로 넘김.
  sigmoid function와 hyperbolic tangent function은 순전파 값에만 영향이 있음.
  softmax function에서 i번째 레이블에 대한 역전파 값은 i번째 레이블에 대한 순전파 값의 i번째 레이블 값(정답 값)이 됨.
  ```

  

#### 1.3.2 최적해 탐색 기법

신경망의 출력값과 레이블과의 차이를 줄여감.

SGD, RmsProp, Adam, NAG의 optimizer가 있음.

SGD는 GD와 학습방법은 동일하지만, 학습 수행 시점과 그 양에서 차이가 있음.

GD는 샘플 하나하나를 가중치 갱신에 사용하지만, SGD는 미니 배치에서 가중치 갱신이 이루어짐.

따라서, GD는 SGD보다 학습량이 많으며, 수행속도가 더 느리다.

학습 데이터가 적으면, GD를 학습데이터가 많으면, SGD를 사용하는 것이 좋다.

#### 1.3.3 과적합 해결 기법

```
오버피팅을 방지하기 위한 대표적 두가지 기법 : regularization(정규화)와 dropout(드롭아웃)
```

* Regularization : $$L(w) = \frac{1}{m}\sum_{i=1}^m(h(z_i)-t_i)^2+\lambda\sum w^2$$
* Dropout : 일부 뉴런을 생략하고 학습을 진행 -> 테스트할 때는 뉴런을 전부 사용하여 테스트

#### 1.4 고급 신경망 구조

* RNN, CNN

#### 1.4.1 순환 신경망

* RNN : 은닉층에서의 출력 값을 다음 학습 때 다시 입력에 추가하는 형태

#### 1.4.2 LSTM 신경망

- LSTM : RNN의 일종으로 이전 상태들을 더 잘 기억할 수 있도록 개선된 형태
- LSTM의 3가지 게이트 : 망각 게이트(forget gate), 입력 게이트(input gate), 출력 게이트(output gate) - sigmoid function을 통해 잊을 값, 받아들일 값, 내보낼 값을 정함.

#### 1.4.3 합성곱 신경망

* convolution계층과 pooling 계층을 이용하여 feature를 추출하는 형태
* fullyconnected 계층이 마지막부분에 이어지고 최종적으로 softmax 계층을 거쳐 결과가 출력됨
* Avg pooling과 Max pooling 으로 pooling 종류는 크게 두 가지가 존재.

#### 1.5 딥러닝 적용 사례

기계 번역, 음성인식, 이미지 인식

#### 1.5.1 기계 번역

#### 1.5.2 음성 인식

#### 1.5.3 이미지 인식



### 02장 . 배경 이론 2: 강화 학습이란?

```
강화학습은 머신러닝 기법의 한 가지로, 어떠한 환경에서 행동에 대한 상벌을 주면서 학습하는 분야.
구성요소로는 에이전트와 환경이 있음.
에이전트는 특정 환경에서 행동을 결정하고, 환경을 그 결정에 대한 보상을 내린다. 이 보상은 행동 즉시 결정되기 보다 여러 행동들을 취한 후에 한꺼번에 결정되는 경우가 많음.
특정 행동을 취했을 때 바로 그행동에 대한 평가를 내릴 수 없는 경우가 많기 때문.
에이전트가 행동을 결정하고 환경이 주는 보상으로 스스로 학습 할때, 주로 딥러닝에서 다룬 인공 신경망을 사용함. 환경과 에이전트의 상태 등을 입력값으로 인공 신경망이 행동을 결정하고 보상이 있으면, 이전의 입력 값과 행동들을 긍정적으로 학습함.
```



#### 2.1 강화학습의 기초가 된 마르코프 의사결정 과정

```
마르코프 의사결정(Markov decision process, MDP)에 학습 개념을 추가한 것이 강화학습
```



#### 2.1.1 마르코프 가정

```
마르코프 가정(Markov assumption)은 상태가 연속적인 시간에 따라 이어질 때, 어떠한 시점의 상태는 그 시점 바로 이전의 상태에만 영향을 받는다는 가정. 직관적으로 현재 -> 이전에 영향 , 이전 -> 더 이전에 영향, .... 으로 단순화 시킴.
```

$$
P(x_t \ | \ x_1,x_2,x_3,...,x_{t-1}) \ = \ (x_t \ | \ x_{t-1})
$$

$$ 좌변$$ 은 어떠한 시점 t에서의 상태 $$x_t$$ 는 최초의 상태 $$x_1$$ 에서 바로 이전의 상태 $$x_{t-1}$$ 까지에 영향을 받는다는 뜻이고 연속적으로 존재하며, 이를 실제로 계산하기는 어렵다. 그래서 상태 $$x_t$$ 는 바로 이전 상태인 $$x_{t-1}$$ 에 가장 큰 영향을 받고 $$x_{t-1}$$ 에서 $$x_{t-2}$$ 를 반영하는 등 연쇄적으로 모든 이전 상태들이 반영된다고 가정하는 마르코프 가정을 적용하면 $$우변$$ 과 같이 단순화 된다.

#### 2.1.2 마르코프 과정

* Markov process 는 마르코프 가정을 만족하는 연속적인 일련의 상태
* Markov process 는 일련의 상태 $$ <x_1,x_2,x_3,...,x_n>$$와 상태 전이 확률(state transition probability) $$p$$ 로 구성됨
* 상태 전이 확률 $$p_{ij}=P(x_{t+1}=j|x_t=i)$$ 은 어떠한 상태가 $$i$$ 일 때 그 다음 상태가 $$j$$가 될 확률을 의미



#### 2.1.3 마르코프 의사결정 과정

```
마르코프 의사결정 과정(Markov decision process, MDP)은 마르코프 과정을 기반으로한 의사결정 모델이다.
```

$$
MDP = (S,A,P,R,\gamma)
$$

MDP 는 상태(state) 집합 $$S$$ , 행동(action) 집합 $$A$$ , 상태 전이 확률(state transition probability) $$P$$, 보상(reword) 함수 $$ R$$ , 할인 요인(discount factor) $$\gamma$$ 로 구성되어 있음

* 상태 집합은 MDP에서 가질 수 있는 모든 상태의 집합 $$\{s_1,s_2,...,s_{|S|}\}​$$

* 행동 집합은 행동 주체인 에이전트가 할 수 있는 모든 행동들의 집합 $$\{a_1,a_2,...,a_{|A|}\}$$

* 상태 전이 확률은 마르코프 과정의 상태 전이 확률보다 조금 더 복잡하다. $$P_{s_i,s_j}^a=\ P(x_{t+1} = s_j|\ x_t=s_i,A_t=a)​$$  

* $$P_{s_i,s_j}^a$$ 는 에이전트가 어떠한 상태 $$s_i$$에서 행동 $$a$$ 를 취했을 때, 상태 $$s_j​$$ 로 변할 확률

* 보상 함수는 에이전트가 어떠한 상태에서 취한 행동에 대한 보상을 내리기 위한 함수

  $$R_s^a:s,a\rightarrow\mathbb{R}​$$ 

  보상 함수$$ R_s^a$$ 는 상태 $$s$$ 에서 행동 $$a$$ 를 했을 때의 보상을 수치로 반환

* 할인 요인은 과거의 행동들을 얼마나 반영할지를 정하는 값 $$\rightarrow$$ 0에서 1사이의 값

  과거 5번의 행동에 대한 보상을 1씩 받았다고 했을 때, 할인 요인 $$\gamma$$ 이 1이면, <1,1,1,1,1> 이 되고,

  할인 요인 $$\gamma$$ 이 0.9 이면, <1, 0.9, 0.81, 0.729, 0.6561> 이 된다. 즉 먼 과거에 대한 보상일 수록 깎아서 반영한다.

#### 2.2 주요 강화학습 기법



#### 2.2.1 Q러닝 강화학습

#### 2.2.2 정책 경사 강화학습

#### 2.3 강화학습 적용 사례

#### 2.3.1 벽돌 깨기

#### 2.3.2 알파고

#### 2.4 이번 장의 요점



### 03장 . 배경 이론 3 : 강화학습을 이용한 주식투자란?

#### 3.1 직관적으로 강화학습 전략 알아보기

#### 3.1.1 강화학습을 이용한 주식투자 구조

#### 3.1.2 차트 데이터 이용하기

#### 3.1.3 차트 데이터를 바탕으로 강화학습을 하는 방식

#### 3.1.4 거래 수수료와 거래세

#### 3.1.5 무작위 행동 결정(탐험)과 무작위 행동 결정 비율(엡실론)

#### 3.2 강화학습 효과를 차별화하는 요인들

#### 3.2.1 차별화 요인 1 : 학습 데이터 구성

#### 3.2.2 차별화 요인 2 : 보상 규칙

#### 3.2.3 차별화 요인 3 : 행동 종류

#### 3.2.4 차별화 요인 4 : 정책 신경망

#### 3.2.5 차별화 요인 5 : 강화학습 기법인 Q러닝과 정책 경사

#### 3.3 차트 데이터와 학습데이터 살펴보기

#### 3.3.1 차트 데이터

#### 3.3.2 학습 데이터

#### 3.4 주식투자 강화학습 절차

#### 3.4.1 주식투자 강화학습 순서도

#### 3.4.2 행동 결정

#### 3.4.3 결정된 행동 수행

#### 3.4.4 배치 학습 데이터 생성 및 정책 신경망 업데이트

#### 3.5 주식투자 강화학습 과정 및 결과 확인 방법

#### 3.5.1 강화학습 과정 확인의 필요성

#### 3.5.2 강화학습 과정을 로그로 남기기

#### 3.5.3 강화학습 과정을 이미지로 가시화하기

#### 3.6 이번 장의 요점



### 04장 모듈 개발 : 강화학습 기반 주식투자 시스템 개발

#### 4.1 RLTrader 개발에 필요한 환경

#### 4.1.1 아나콘다 설치

#### 4.1.2 텐서플로와 케라스 설치

#### 4.2 RLTrader의 구조

#### 4.2.1 모듈 구조

#### 4.2.2 디렉터리 구조

#### 4.2.3 에이전트 모듈 개요

#### 4.2.4 환경 모듈 개요

#### 4.2.5 정책 신경망 모듈 개요

#### 4.2.6 가시화기 모듈 개요

#### 4.2.7 정책 학습기 모듈 개요

#### 4.3 환경 모듈 개발

#### 4.3.1 환경 모듈의 주요 속성과 함수

#### 4.3.2 코드 조각 : 환경 클래스의 전체 소스코드

#### 4.4 에이전트 모듈 개발

#### 4.4.1 에이전트 모듈의 주요 속성과 함수

#### 4.4.2 코드 조각 1 : 에이전트 클래스의 상수 선언 부분

#### 4.4.3 코드 조각 2 : 에이전트 클래스의 생성자 부분

#### 4.4.4 코드 조각 3 : 에이전트 클래스의 함수 부분

#### 4.5 정책 신경망 모듈 개발

#### 4.5.1 정책 신경망 모듈의 주요 속성과 함수

#### 4.5.2 정책 신경망에서 사용하는 LSTM 신경망의 구조

#### 4.5.3 코드 조각 1 : 정책 신경망 클래스의 생성자 부분

#### 4.5.4 코드 조각 2 : 정책 신경망 클래스의 함수 선언 부분

#### 4.6 가시화기 모듈 개발

#### 4.6.1 가시화기 모듈의 주요 속성과 함수

#### 4.6.2 가시화기 모듈이 만들어내는 정보

#### 4.6.3 코드 조각 1 : 가시화기 클래스의 생성자 부분

#### 4.6.4 코드 조각 2 : 일봉 차트 가시화 함수 부분

#### 4.6.5 코드 조각 3 : 전체 차트 가시화 함수 선언 부분

#### 4.6.6 코드 조각 4 : 에이전트 상태 가시화 부분

#### 4.6.7 코드 조각 5 : 정책 신경망 출력 결과 및 탐험 수행 가시화 부분

#### 4.6.8 코드 조각 6 : 포트폴리오 가치 및 기타 정보 가시화 부분

#### 4.6.9 코드 조각 7 : 차트 초기화 및 저장 함수 부분

#### 4.7 정책 학습기 모듈 개발

#### 4.7.1 코드 조각 1 : 정책 학습기 모듈의 의존성 임포트 부분

#### 4.7.2 코드 조각 2 : 정책 학습기 클래스의 생성자 부분

#### 4.7.3 코드 조각 3 : 에포크 초기화 함수 부분

#### 4.7.4 코드 조각 4 : 학습 함수 선언 부분

#### 4.7.5 코드 조각 5 : 학습 함수 초반 부분

#### 4.7.6 코드 조각 6 : 학습 함수의 로컬 변수 초기화 부분

#### 4.7.7 코드 조각 7 : 학습 함수의 연관 객체 초기화 및 탐험 비율 설정 부분

#### 4.7.8 코드 조각 8 : 학습 함수의 에포크 구행 while 문 초반부

#### 4.7.9 코드 조각 9 : 학습 함수의 행동과 그 결과를 저장하는 부분

#### 4.7.10 코드 조각 10 : 학습 함수의 반복 정보 갱신 부분

#### 4.7.11 코드 조각 11 : 학습 함수의 정책 신경망 학습 부분

#### 4.7.12 코드 조각 12 : 에포크 결과 가시화 부분

#### 4.7.13 코드 조각 13 : 에포크 결과 로그 기록 부분

#### 4.7.14 코드 조각 14 : 학습 통계 정보 갱신 부분

#### 4.7.15 코드 조각 15 : 최종 학습 결과 통계 정보 로그 기록 부분

#### 4.7.16 코드 조각 16 : 미니 배치 데이터 생성 함수 부분

#### 4.7.17 코드 조각 17 : 학습 데이터 샘플 생성 부분

#### 4.7.18 코드 조각 18 : 투자 시뮬레이션을 하는 trade()함수 부분

### 05장 데이터 준비 : 주식 데이터 획득

#### 5.1 방법 1. 증권사 HTS 사용

#### 5.1.1 증권사 HTS 다운로드

#### 5.1.2 증권 계좌 개설

#### 5.1.3 종목 차트 데이터 확인

#### 5.1.4 일별 데이터 엑셀 파일 저장

#### 5.2 방법 2. 증권사 API 사용

#### 5.2.1 증권사 API 설치

#### 5.2.2 대신증권 크레온 API 사용 환경 준비

#### 5.2.3 대신증권 크레온 HTS 실행

#### 5.2.4 대신증권 크레온 API를 이용한 차트 데이터 획득 프로그램 작성

#### 5.3 방법 3. 포털 사이트 사용

#### 5.3.1 pandas-datareader, fix_yahoo_finance 설치하기

#### 5.3.2 Google Finance에서 주식 데이터 획득하기

#### 5.3.3 Yahoo Finance에서 주식 데이터 획득하기

#### 5.4 이번 장의 요점



### 06장 모델 구축 : 투자 시뮬레이션

#### 6.1 주식 데이터 전처리

#### 6.1.1 코드 조각 1 : CSV 파일을 읽는 부분

#### 6.1.2 코드 조각 2 : 종가와 거래량의 이동 평균 구하기

#### 6.1.3 코드 조각 3 : 주가와 거래량의 비율 구하기

#### 6.1.4 코드 조각 4 : 주가와 거래량의 이동 평균 비율 구하기

#### 6.2 주식 데이터 학습

#### 6.2.1 코드 조각 1 : 강화학습을 실행하는 메인(main)모듈

#### 6.2.2 코드 조각 2 : 강화학습에 필요한 주식 데이터 준비 부분

#### 6.2.3 코드 조각 3 : 데이터를 차트 데이터와 학습 데이터로 분류하는 부분

#### 6.2.4 코드 조각 4 : 강화학습을 시작하는 부분

#### 6.3 학습 과정 및 결과 확인

#### 6.3.1 콘솔에 출력되는 로그의 의미

#### 6.3.2 가시화 결과가 저장되는 그림 파일

#### 6.4 이번 장의 요점



### 07장 모델 검증 : 투자 시뮬레이션

#### 7.1 투자 시뮬레이션 결과 1 : 삼성전자(005930)

#### 7.1.1 종목의 개요

#### 7.1.2 주식 데이터 전처리

#### 7.1.3 학습 파라미터 설정

#### 7.1.4 에포크 10일 때의 결과

#### 7.1.5 에포크 200일 때의 결과

#### 7.1.6 에포크 600일 때의 결과

#### 7.1.7 에포크 1000일 때의 결과

#### 7.1.8 총평

#### 7.2 투자 시뮬레이션 결과 2 : SK하이닉스(000660)

#### 7.2.1 종목의 개요

#### 7.2.2 주식 데이터 전처리

#### 7.2.3 학습 파라미터 설정

#### 7.2.4 에포크 10일 때의 결과

#### 7.2.5 에포크 200일 때의 결과

#### 7.2.6 에포크 600일 때의 결과

#### 7.2.7 에포크 1000일 때의 결과

#### 7.2.8 총평

#### 7.3 투자 시뮬레이션 결과 3 : 현대차(005380)

#### 7.3.1 종목의 개요

#### 7.3.2 주식 데이터 전처리

#### 7.3.3 학습 파라미터 설정

#### 7.3.4 에포크 10일 때의 결과

#### 7.3.5 에포크 200일 때의 결과

#### 7.3.6 에포크 600일 때의 결과

#### 7.3.7 에포크 1000일 때의 결과

#### 7.3.8 총평

#### 7.4 투자 시뮬레이션 결과 4 : LG화학(051910)

#### 7.4.1 종목의 개요

#### 7.4.2 주식 데이터 전처리

#### 7.4.3 학습 파라미터 설정

#### 7.4.4 에포크 10일 때의 결과

#### 7.4.5 에포크 200일 때의 결과

#### 7.4.6 에포크 600일 때의 결과

#### 7.4.7 에포크 1000일 때의 결과

#### 7.4.8 총평

#### 7.5 투자 시뮬레이션 결과 5 : 네이버(035420)

#### 7.5.1 종목의 개요

#### 7.5.2 주식 데이터 전처리

#### 7.5.3 학습 파라미터 설정

#### 7.5.4 에포크 10일 때의 결과

#### 7.5.5 에포크 200일 때의 결과

#### 7.5.6 에포크 600일 때의 결과

#### 7.5.7 에포크 1000일 때의 결과

#### 7.5.8 총평

#### 7.6 투자 시뮬레이션 결과 6 : KT(030200)

#### 7.6.1 종목의 개요

#### 7.6.2 주식 데이터 전처리

#### 7.6.3 학습 파라미터 설정

#### 7.6.4 에포크 10일 때의 결과

#### 7.6.5 에포크 200일 때의 결과

#### 7.6.6 에포크 600일 때의 결과

#### 7.6.7 에포크 1000일 때의 결과

#### 7.6.8 총평

#### 7.7 투자 시뮬레이션 결과 정리 및 원숭이 투자와의 비교

#### 7.8 이번 장의 요점



#### 08장 모델 활용 : 학습된 정책 신경망 모델을 사용한 투자 시뮬레이션

#### 8.1 모델 학습과 모델 활용의 차이점

#### 8.1.1 시뮬레이션 과정 차이점

#### 8.1.2 소스코드의 차이점

#### 8.2 학습된 정책 신경망 모델을 사용한 투자 시뮬레이션

#### 8.2.1 학습된 모델 적용 1 : 삼성전자(005930)

#### 8.2.2 학습된 모델 적용 2 : SK하이닉스(000660)

#### 8.2.3 학습된 모델 적용 3 : 현대차(005380)

#### 8.2.4 학습된 모델 적용 4 : LG화학(051910)

#### 8.2.5 학습된 모델 적용 5 : NAVER(035420)

#### 8.2.6 학습된 모델 적용 6 : KT(030200)

#### 8.2.7 총평

#### 8.3 투자 시뮬레이션 결과 정리 및 원숭이 투자와의 비교

#### 8.4 이번 장의 요점









