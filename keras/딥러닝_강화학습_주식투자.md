## 파이썬과 케라스를 이용한 딥러닝 / 강화학습 주식투자

### 01장 . 배경이론 1 : 딥러닝이란?

### 1.1 딥러닝 개요

#### 1.1.1 딥러닝의 정의와 역사

```
머신러닝 : ML(Machine Learning)

인공 신경망 : ANN(Artificial Neural Network)

인공지능 : AI(Artificial Intelligence)

심층 신경망 : DNN(Deep Neural Network)
```

1950년대 퍼셉트론 등장 -> 1960년대 퍼셉트론 한계 증명 -> 1970년대 인공신경망 암흑기 -> 1980년대 인공 신경망 부활기 (오차 역전파 적용, DNN, RNN, CNN 발전) -> 1990년대 LSTM등장,LeNet-5등장 -> 2000년대 가트너 10대 전략 기술 -> 2010년대 알파고

#### 1.1.2 딥러닝이 최근에 주목받는 이유

컴퓨터 연산능력이 좋아졌음 , 빅데이터 등장

#### 1.1.3 딥러닝으로 풀고자 하는 문제

문제 : 분류(classification) , 군집화(clustering), 회귀(regression)

학습 방법 : 지도학습, 비지도 학습, 강화학습

```
강화학습 : 에이전트가 어떠한 환경에서 행동을 수행했을 때 보상을 함으로써 , 에이전트는 그 보상을 최대로 하는 행동을 수행하도록 학습하게 하는 방법. 
행동 결정 분류 문제나 보상 예측의 회귀 문제를 다룸.
레이블이 없는 데이터를 학습할 수 있지만, 에이전트와 환경을 구성하는 추가적인 비용을 필요로 함.
```

### 1.2 딥러닝의 발전 과정

#### 1.2.1 퍼셉트론

```
퍼셉트론 : 입력값(input)과 편향값(bias)에 가중치(weight)를 곱하여 합한 값이 threshold를 넘어가면 1, 그렇지 않으면 0을 출력하는 활성화 함수를 가지는 구조
```



#### 1.2.2 인공 신경망

```
* 인공 신경망(ANN) : input layer , hidden layer, output layer로 구성된 연결망
* 은닉층(hidden layer)에서 활성화 함수를 사용.
```

* ##### 활성화 함수 종류 

  step function, ReLU function, leaky ReLU function, sigmoid function, hyperbolic tangent(tanh) function

  #### forward activation functions

  * step function
    * $$h(z) = \begin{cases} 1,&z>0이면 \\ -1,&그\ 외\end{cases}​$$
  * ReLU function
    * $$h(z)=\begin{cases}z,&z>0이면\\ 0,&그\ 외\end{cases}$$

  * LeakyReLU function
    * $$h(z)=\begin{cases}z,&z>0이면\\z*\alpha,&그\ 외 \end{cases}$$
  * sigmoid function
    * $$h(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{e^z+1} $$
  * Hyperbolic tangent(tanh) function
    * $$h(z)=\frac{sinhz}{coshz}=\frac{\frac{1-e^{-2z}}{2e^{-2z}}}{\frac{1+e^{-2x}{}}{2e^{-z}}}=\frac{1-e^{-2z}}{1+e^{-2z}}​$$

* 출력층에서도 활성화 함수 사용

  * Sigmoid 또는 Softmax 사용
  * 출력층에서의 sigmoid : 이항 분류 문제에서 출력 값을 확률화
  * 출력층에서의 softmax : 다항 분류 문제에서 출력 값을 확률화

  

#### 1.2.3 심층 신경망

```
심층 신경망(DNN): 은닉층이 2개 이상인 ANN
```

### 1.3 딥러닝에 필요한 핵심 기술

#### 1.3.1 오차 역전파 기법

```
오차 역전파(backpropagation) : forward된 weight를 편미분한 값을 backward 형태로 곱해 나가면서 그 최종 역전파 값을 가중치에 더해서 조정
```

* #### back propagation functions

  * Step function
    * $$\frac{\delta y}{\delta z}=\begin{cases}0,&z>0이면\\ 0,& 그\ 외 \end{cases} =0​$$
  * ReLU function
    * $$\frac{\delta y}{\delta z}=\begin{cases}1,&z>0이면\\ 0,& 그\ 외 \end{cases}$$
  * sigmoid function
    * $$\frac{\delta y}{\delta z}=y(1-y)​$$
  * Hyperbolic tangent(tanh) function
    * $$\frac{\delta y}{\delta z}=1-y^2$$
  * Softmax function
    * $$\frac{\delta y_i}{\delta z_i}=y_i-t_i(t_i는\ i번\ 째\ 레이블)$$

  ```
  * 역전파 함수 설명
  sigmoid function의 순전파를 미분하면 모두 0이 되어 가중치에 영향을 주지 않음.
  ReLU function은 가중치 곱의 합이 양수일 때만 뒤쪽 계층에서 넘어온 역전파를 그대로 넘김.
  sigmoid function와 hyperbolic tangent function은 순전파 값에만 영향이 있음.
  softmax function에서 i번째 레이블에 대한 역전파 값은 i번째 레이블에 대한 순전파 값의 i번째 레이블 값(정답 값)이 됨.
  ```

  

#### 1.3.2 최적해 탐색 기법

신경망의 출력값과 레이블과의 차이를 줄여감.

SGD, RmsProp, Adam, NAG의 optimizer가 있음.

SGD는 GD와 학습방법은 동일하지만, 학습 수행 시점과 그 양에서 차이가 있음.

GD는 샘플 하나하나를 가중치 갱신에 사용하지만, SGD는 미니 배치에서 가중치 갱신이 이루어짐.

따라서, GD는 SGD보다 학습량이 많으며, 수행속도가 더 느리다.

학습 데이터가 적으면, GD를 학습데이터가 많으면, SGD를 사용하는 것이 좋다.

#### 1.3.3 과적합 해결 기법

```
오버피팅을 방지하기 위한 대표적 두가지 기법 : regularization(정규화)와 dropout(드롭아웃)
```

* Regularization : $$L(w) = \frac{1}{m}\sum_{i=1}^m(h(z_i)-t_i)^2+\lambda\sum w^2$$
* Dropout : 일부 뉴런을 생략하고 학습을 진행 -> 테스트할 때는 뉴런을 전부 사용하여 테스트

#### 1.4 고급 신경망 구조

* RNN, CNN

#### 1.4.1 순환 신경망

* RNN : 은닉층에서의 출력 값을 다음 학습 때 다시 입력에 추가하는 형태

#### 1.4.2 LSTM 신경망

- LSTM : RNN의 일종으로 이전 상태들을 더 잘 기억할 수 있도록 개선된 형태
- LSTM의 3가지 게이트 : 망각 게이트(forget gate), 입력 게이트(input gate), 출력 게이트(output gate) - sigmoid function을 통해 잊을 값, 받아들일 값, 내보낼 값을 정함.

#### 1.4.3 합성곱 신경망

* convolution계층과 pooling 계층을 이용하여 feature를 추출하는 형태
* fullyconnected 계층이 마지막부분에 이어지고 최종적으로 softmax 계층을 거쳐 결과가 출력됨
* Avg pooling과 Max pooling 으로 pooling 종류는 크게 두 가지가 존재.

#### 1.5 딥러닝 적용 사례

기계 번역, 음성인식, 이미지 인식

#### 1.5.1 기계 번역

#### 1.5.2 음성 인식

#### 1.5.3 이미지 인식



### 02장 . 배경 이론 2: 강화 학습이란?

```
강화학습(RL)은 머신러닝 기법의 한 가지로, 어떠한 환경에서 행동에 대한 상벌을 주면서 학습하는 분야.
구성요소로는 에이전트와 환경이 있음.
에이전트는 특정 환경에서 행동을 결정하고, 환경을 그 결정에 대한 보상을 내린다. 이 보상은 행동 즉시 결정되기 보다 여러 행동들을 취한 후에 한꺼번에 결정되는 경우가 많음.
특정 행동을 취했을 때 바로 그행동에 대한 평가를 내릴 수 없는 경우가 많기 때문.
에이전트가 행동을 결정하고 환경이 주는 보상으로 스스로 학습 할때, 주로 딥러닝에서 다룬 인공 신경망을 사용함. 환경과 에이전트의 상태 등을 입력값으로 인공 신경망이 행동을 결정하고 보상이 있으면, 이전의 입력 값과 행동들을 긍정적으로 학습함.
```



#### 2.1 강화학습의 기초가 된 마르코프 의사결정 과정

```
마르코프 의사결정(Markov decision process, MDP)에 학습 개념을 추가한 것이 강화학습
```



#### 2.1.1 마르코프 가정

```
마르코프 가정(Markov assumption)은 상태가 연속적인 시간에 따라 이어질 때, 어떠한 시점의 상태는 그 시점 바로 이전의 상태에만 영향을 받는다는 가정. 직관적으로 현재 -> 이전에 영향 , 이전 -> 더 이전에 영향, .... 으로 단순화 시킴.
```

$$
P(x_t \ | \ x_1,x_2,x_3,...,x_{t-1}) \ = \ (x_t \ | \ x_{t-1})
$$

$$ 좌변$$ 은 어떠한 시점 t에서의 상태 $$x_t$$ 는 최초의 상태 $$x_1$$ 에서 바로 이전의 상태 $$x_{t-1}$$ 까지에 영향을 받는다는 뜻이고 연속적으로 존재하며, 이를 실제로 계산하기는 어렵다. 그래서 상태 $$x_t$$ 는 바로 이전 상태인 $$x_{t-1}$$ 에 가장 큰 영향을 받고 $$x_{t-1}$$ 에서 $$x_{t-2}$$ 를 반영하는 등 연쇄적으로 모든 이전 상태들이 반영된다고 가정하는 마르코프 가정을 적용하면 $$우변$$ 과 같이 단순화 된다.

#### 2.1.2 마르코프 과정

* Markov process 는 마르코프 가정을 만족하는 연속적인 일련의 상태
* Markov process 는 일련의 상태 $$ <x_1,x_2,x_3,...,x_n>$$와 상태 전이 확률(state transition probability) $$p$$ 로 구성됨
* 상태 전이 확률 $$p_{ij}=P(x_{t+1}=j|x_t=i)$$ 은 어떠한 상태가 $$i$$ 일 때 그 다음 상태가 $$j$$가 될 확률을 의미



#### 2.1.3 마르코프 의사결정 과정

```
마르코프 의사결정 과정(Markov decision process, MDP)은 마르코프 과정을 기반으로한 의사결정 모델이다.
```

$$
MDP = (S,A,P,R,\gamma)
$$

MDP 는 상태(state) 집합 $$S$$ , 행동(action) 집합 $$A$$ , 상태 전이 확률(state transition probability) $$P$$, 보상(reword) 함수 $$ R$$ , 할인 요인(discount factor) $$\gamma$$ 로 구성되어 있음

* 상태 집합은 MDP에서 가질 수 있는 모든 상태의 집합 $$\{s_1,s_2,...,s_{|S|}\}​$$

* 행동 집합은 행동 주체인 에이전트가 할 수 있는 모든 행동들의 집합 $$\{a_1,a_2,...,a_{|A|}\}$$

* 상태 전이 확률은 마르코프 과정의 상태 전이 확률보다 조금 더 복잡하다. $$P_{s_i,s_j}^a=\ P(x_{t+1} = s_j|\ x_t=s_i,A_t=a)​$$  

* $$P_{s_i,s_j}^a$$ 는 에이전트가 어떠한 상태 $$s_i$$에서 행동 $$a$$ 를 취했을 때, 상태 $$s_j​$$ 로 변할 확률

* 보상 함수는 에이전트가 어떠한 상태에서 취한 행동에 대한 보상을 내리기 위한 함수

  $$R_s^a:s,a\rightarrow\mathbb{R}​$$ 

  보상 함수$$ R_s^a$$ 는 상태 $$s$$ 에서 행동 $$a$$ 를 했을 때의 보상을 수치로 반환

* 할인 요인은 과거의 행동들을 얼마나 반영할지를 정하는 값 $$\rightarrow$$ 0에서 1사이의 값

  과거 5번의 행동에 대한 보상을 1씩 받았다고 했을 때, 할인 요인 $$\gamma$$ 이 1이면, <1,1,1,1,1> 이 되고,

  할인 요인 $$\gamma$$ 이 0.9 이면, <1, 0.9, 0.81, 0.729, 0.6561> 이 된다. 즉 먼 과거에 대한 보상일 수록 깎아서 반영한다.

#### 2.2 주요 강화학습 기법

```
복잡한 문제를 풀기 위한 강화학습의 특징 : 1.상태 집합이 (거의) 무한하다. 2.상태 전이 확률을 인공 신경망으로 구한다.
```

상태를 구성하는 요소가 많아질수록 상태의 공간이 방대해진다. 그래서 인공신경망을 사용한다.

강화학습 기법은 Q러닝과 정책 경사(policy gradient)가 있음.

#### 2.2.1 Q러닝 강화학습

```
Q러닝은 회귀(regression)문제를 푸는 강화학습으로 볼 수 있다.
어떠한 상태에서 어떠한 행동을 했을 때, 예상 가치(value)를 상태-행동가치(state-action value)라고 부른다.
Q러닝은 state-action value가 가장 높은 행동을 취하고, 복잡한 문제에서 상태 공간이 매우 크므로, state-action value를 ANN으로 구현한다.
```



#### 2.2.2 정책 경사 강화학습

```
정책 경사(policy gradient)는 분류(classification)문제를 푸는 강화학습으로 볼 수 있다.
어떠한 상태에서 어떠한 행동을 결정하는 것이 가장 좋을지를 판단하는 것이다.
정책 경사도 어떠한 상태에서의 행동에 대한 확률 값들을 ANN으로 구현한다.
```



#### 2.3 강화학습 적용 사례

```
벽돌 깨기, 알파고
* 벽돌 깨기
구글 딥마인드에서 2013년도 발표한 <Playing Atari with Deep Reinforcement Learning>이라는 논문에서 DQN(Deep Q network)을 처음 공개
* 알파고
2016년 3월 이세돌과의 대국으로 5전 4승 1패를 기록
```

#### 2.3.1 벽돌 깨기

#### 2.3.2 알파고

#### 2.4 이번 장의 요점



### 03장 . 배경 이론 3 : 강화학습을 이용한 주식투자란?

#### 3.1 직관적으로 강화학습 전략 알아보기

```
강화학습으로 무작정 주식 투자했을 때의 일일히 학습 데이터를 만드는 수고를 없애면서 효과적으로 할 수 있는 전략을 구상해보자.
```

#### 3.1.1 강화학습을 이용한 주식투자 구조

```
주식투자도 어떠한 환경에서 매수(buy), 매도(sell), 관망(hold) 등을 판단하는 문제로서 강화학습을 적용할 수 있다.
에이전트 : 투자자
환경 : 차트 데이터
행동 : 매수, 매도 , 관망
보상 : 손익
```



#### 3.1.2 차트 데이터 이해하기

```
1. 차트 데이터는 어떠한 종목의 시가, 고가, 저가, 종가, 거래량등의 있는 그대로의 데이터이다.
2. 차트 데이터로 투자 손익을 계산한다.
3. 학습 데이터는 모델을 학습할 목적으로 가공한 데이터이다.
4. 주식의 차트는 주로 봉 차트(캔들 차트)로 그린다.
5. 한국의 봉 차트에서는 양봉을 빨간색, 음봉을 파란색으로 그린다. 미국에서는 주로 양봉을 초록색, 음봉을 빨간색으로 표시하니 혼동하지 말것!
6. 양봉에서 굵은 부분의 아래쪽이 시가로 봉의 기간 중에서 가장 처음 거래된 가격이다. 위 꼬리 부분은 고가로 봉의 기간 동안 거래된 가격 중에 가장 높은 가격을 의미한다. 아래 꼬리는 저가로 봉의 기간 동안 가장 낮게 거래된 가격을 의미한다. 굵은 부분의 위쪽은 종가로 봉의 기간 중에서 가장 마지막에 거래된 가격이다.
7. 음봉은 굵은 부분의 위쪽이 시가로 봉의 기간 중에서 가장 처음 거래된 가격이고, 굵은 부분의 아래쪽은 종가로 봉의 기간 중에서 가장 마지막에 거래된 가격이다.
```



#### 3.1.3 차트 데이터를 바탕으로 강화학습을 하는 방식

```
초기 자본으로 1000만원이 있고, 거래에 대한 수수료와 세금은 고려하지 않고 1주씩만 매수하거나 매도한다고 제한한다. 그리고 2%의 손익이 발생 했을 때, 보상을 준다. 
```

```
누적 손익률 = 현 시점 평가금/시작 평가금

이전 보상지점 대비, 누적 손익률이 2%이상 수익이 발생한 지점에서 보상 +1

이전 보상지점 대비, 누적 손익률이 2%이상 손실이 발생한 지점에서 보상 -1

이전에 받은 보상을 기반으로 매수와 매도를 결정
```

* 학습을 반복하더라도 어느지점에서 더이상 보상이 +1또는 -1되는 지점이 일정할 경우, ''더이상 학습이 되지않음' '을 알 수 있음.



#### 3.1.4 거래 수수료와 거래세

```
실제 투자에서는 거래 수수료와 거래세를 비용으로 고려해야 함.
매수 수수료 : 0.015% (매수 시 발생하는 증권사가 취하는 금액)
매도 수수료 : 0.015% (매도 시 발생하는 증권사가 취하는 금액)
거래세 : 0.3% (매도 시 발생하는 국가가 취하는 금액)
```

* 거래를 적게 하는 것이 비용을 줄이는 길

#### 3.1.5 무작위 행동 결정(탐험)과 무작위 행동 결정 비율(엡실론)

```
실제로는 사람이 추적할 수 없을 정도의 수많은 반복과 방대한 데이터를 사용하고 일부 지점에서 무작위 투자를 진행하여, 학습 정체를 없앤다.
무작위로 행동하는 것을 탐험(exploration)이라고 하고,
무작위로 행동을 결정하는 비율을 엡실론(epsilon) 으로 표시한다.
```



#### 3.2 강화학습 효과를 차별화하는 요인들

```
학습 데이터, 보상 규칙, 행동의 종류, 정책 신경망, 강화학습 기법
```



#### 3.2.1 차별화 요인 1 : 학습 데이터 구성

```
당일 주가 및 거래량의 이전 주가 및 거래량 대비 비율 등등
```

* 전일 종가 대비 당일 시가 비율 ( open / last close )
* 당일 종가 대비 당일 고가 비율 ( high / close )
* 당일 종가 대비 당일 저가 비율( low / close )
* 당일 종가 대비 전일 종가 비율( close / last close )
* 전일 거래량 대비 당일 거래량 비율 ( volume / last volume )
* 5일 평균 종가 대비 당일 종가 비율 ( close / MA5 close )
* 10일 평균 종가 대비 당일 종가 비율 ( close / MA10 close )
* 20일 평균 종가 대비 당일 종가 비율 ( close / MA20 close )
* 60일 평균 종가 대비 당일 종가 비율 ( close / MA60 close )
* 120일 평균 종가 대비 당일 종가 비율 ( close / MA120 close )
* 5일 평균 거래량 대비 당일 거래량 비율 ( volume / MA5 volume )
* 10일 평균 거래량 대비 당일 거래량 비율( volume / MA10 volume )
* 20일 평균 거래량 대비 당일 거래량 비율 ( volume / MA20 volume )
* 60일 평균 거래량 대비 당일 거래량 비율 ( volume / MA60 volume )
* 120일 평균 거래량 대비 당일 거래량 비율 ( volume / MA120 volume )
* 주식 보유 비율 ( 현재 보유 주식 수 / 최대 보유 가능 주식 수 ) - 높으면, 매도 관점 / 낮으면, 매수 관점
* 포트폴리오 가치 비율 ( 현재 포트폴리오 가치 / 기준 포트폴리오 가치 ) - 정하기 나름
* 에이전트가 연속으로 매수한 횟수
* 에이전트가 연속으로 매도한 횟수
* 20 거래일 동안의 매수 비율
* 20 거래일 동안의 매도 비율
* 이외의 더 많은 학습 데이터

#### 3.2.2 차별화 요인 2 : 보상 규칙

손익률 1%, 2%, 5%, 10% 등으로 정할 수 있음.

이익률을 5% 초과하거나 손실률을 3% 초과할 때 학습을 진행하게 할 수도 있음. 

#### 3.2.3 차별화 요인 3 : 행동 종류

```
매수, 매도, 관망
```

공격적 매수, 방어적 매수, 공격적 매도, 방어적 매도, 관망

매매할 최대 및 최소 투자 단위를 결정해 놓고 확률에 따라 최소에서 최대 사이의 단위로 투자를 진행할 수 있음. 

* 확률과 투자금액 비례

#### 3.2.4 차별화 요인 4 : 정책 신경망

정책 신경망 - 에이전트가 행동을 결정하기 위한 두뇌 역할을 하는 인공 신경망(Artificial Neural Network)

기본적으로 CNN과 RNN을 혼용해서 씀

```
RNN : 현재 상태와 이전에 결정한 행동을 함께 고려함. 매수와 매도의 정신없는 투자를 피하고 투자 일관성을 가질 수 있음.

CNN : 5일, 10일 등 특정 기간 동안의 일련의 상태를 한꺼번에 고려할 수 있음(feature extraction)

RNN+CNN : 절대 다수결로 결과를 결정할 수 있음. 이러한 방법을 앙상블이라고 하고, 비슷한 방법으로 배깅(bagging)과 부스팅(boosting)등의 방법이 있음.
```

#### 3.2.5 차별화 요인 5 : 강화학습 기법인 Q러닝과 정책 경사

```
Q러닝 : 특정 행동을 취했을 때 예측되는 포트폴리오 가치를 회귀(regression)하고자 사용함.
예측한 값이 높은 쪽으로 행동을 취하면 됌. 즉, Q러닝을 사용할 경우, 행동들마다 기대 손익을 수치적으로 예측함.

정책 경사(policy gradient) : 어떤 행동이 현재 상태에서 가장 좋을지를 확률적으로 판단함.
Q러닝과 다르게 기대 손익을 예측하는 것이 아니라, 단순히 현 상황에서 어떤 행동이 더 좋은지를 판단함.
```

* 주식 투자 시, 일반적으로 매수/매도 했을 때, 수익률을 수치적으로 예측하기 보다는 현재 상태에서 매수해야 좋을지 아니면 매도해야 좋을지를 판단하는 편이 더 쉽고 신뢰성이 높다고 생각하여 여러 강화 기법 중에 정책 경사를 주식투자에 적용함. 
* 여러 강화학습 기법을 한꺼번에 사용할 수도 있음!

#### 3.3 차트 데이터와 학습데이터 살펴보기

차트 데이터 : 있는 그대로의 데이터(Raw data)

학습 데이터 : 전처리한 데이터(Pre-precessed data), 학습에 사용할 데이터

#### 3.3.1 차트 데이터

차트 데이터는 2차원 데이터로 체결일(date), 시가(open), 고가(high), 저가(low), 종가(close), 거래량(volume)이 연속되는 (idx 0 .... Idx N)식으로 구성된 데이터

* 차트데이터를 60분봉, 20분봉, 5분봉, 1분봉 등으로 구성할 수도 있음.

#### 3.3.2 학습 데이터

모델에 input할 수 있도록 전처리하여 준비 해야함.

```
1. 5일, 10일, 20일, 60일, 120일 평균 종가와 평균 거래량을 구함
2. 5일 평균 데이터는 인덱스 4부터, 10일 평균 데이터는 인덱스 9부터, 20일 평균 데이터는 인덱스 19부터, 60일 평균 데이터는 인덱스 59부터, 120일 평균 데이터는 인덱스 119부터 계산이 가능함.
3. 주가와 거래량 단위가 '원'과 '주'로서 서로 다르기 때문에 비율 값으로 구성함.
```

#### 3.4 주식투자 강화학습 절차

강화학습은 환경, 에이전트, 정책 신경망이 서로 상호 작용하면서 학습을 수행하기 때문에 그 과정이 일반적인 머신러닝에 비해 복잡함.

#### 3.4.1 주식투자 강화학습 순서도

```flow
start=>start: 시작
end=>end: 끝
o1=>operation: 탐험(exploration) 결정
o2=>operation: 무작위로 행동 결정
o3=>operation: 정책신경망으로 행동 결정
o4=>operation: 결정된 행동 수행
o5=>operation: 배치 학습데이터 생성
o6=>operation: 정책신경망 업데이트
o7=>operation: 환경 초기화
o8=>operation: 학습 결과 확인
c1=>condition: 탐험?
c2=>condition: 지연보상 발생?
c3=>condition: epoch 종료?
c4=>condition: 강화학습 종료?

start->o1->c1
c1(no)->o3->o4
c1(yes)->o2->o4
o4->c2
c2(yes)->o5->o6->c3
c2(no)->c3
c3(yes)->c4
c3(no)->o1
c4(yes)->o8->end
c4(no)->o7->o1



```



#### 3.4.2 행동 결정

```
한 epoch에서 경험을 얻기 위해서 무작위로 행동을 해봐야 하는데, 이를 탐험(exploration)이라고 한다.

일반적으로 강화학습 초반에는 탐험을 많이 하고, 후반으로 갈수록 탐험을 적게 해 나간다.

탐험 비율을 엡실론(epsilon)이라고 한다. 

1만 번 반복한다고 했을 때, 1 epoch에서는 앱실론을 30%로 정하고, 점점 엡실론을 줄여서 1만 번째 epoch에서는 0%가 되게 하면, epoch이 커질수록 무작위로 하는 행동이 줄어들게 된다.
```

* 무작위로 행동하지 않을 때는 정책 신경망으로 행동을 결정한다.

#### 3.4.3 결정된 행동 수행

```
무작위든, 정책 신경망으로 결정한 행동이든 에이전트는 결정된 행동을 수행한다.
```

* 매수의 경우, 에이전트는 주식을 사들일 현금이 있는지 확인하고, 매수가 가능할 경우 매수를 수행하고 그렇지 않으면 관망한다.
* 매수했을 경우, 매수금만큼 현금을 줄이고 매수한 주식 수 만큼 보유 주식 수를 늘려준다.
* 매도의 경우, 에이전트는 보유한 주식이 있는지 확인하고 보유한 주식이 있을 경우 매도를 수행하고 그렇지 않으면 관망한다. 
* 매도했을 경우, 매도한 주식 수만큼 보유 주식 수에서 빼주고 매도금만큼 현금보유액에 더해준다.

#### 3.4.4 배치 학습 데이터 생성 및 정책 신경망 업데이트

주식 투자를 해 나가면서 지연 보상을 줄 수 있을지 판단한다.

예를 들어, 2% 이상의 이익 또는 손실을 지연 보상 기준으로 정해보자.

```
즉, 투자를 하다가 2% 이상의 이익이나 2% 이상의 손실이 발생하면 이때까지 상황과 행동들을 학습 데이터로서 생성한다.
이 학습 데이터들을 한꺼번에 적용하여, 정책 신경망을 업데이트한다.

이런 학습 방법을 배치(batch) 학습이라고 부른다.

* 학습을 진행하고 나면, 정책 신경망의 가중치들이 업데이트되어 이후에 진행되는 투자부터 바로 업데이트된 정책 신경망의 결과가 반영된다.
```

* 지연 보상 기준을 낮게 잡으면 작은 배치 학습 데이터로 자주 학습을 수행할 가능성이 높으며, 지연 보상 기준이 높으면 큰 배치 학습 데이터로 덜 자주 학습을 진행할 가능성이 높다.



#### 3.5 주식투자 강화학습 과정 및 결과 확인 방법

* 주식투자 강화학습을 진행하면, 정해진 환경에서 매 순간마다 무작위로 행동을 결정하거나 정책 신경망으로 행동을 결정한 다음에 에이전트는 결정된 행동을 수행하고 그 결과로 에이전트의 상태가 변경된다.

#### 3.5.1 강화학습 과정 확인의 필요성

```
1. 정책 신경망이 출력 값이 어떤지
2. 에이전트 상태 변화 추세
```

* 학습이 진행되어 가는데, 에이전트가 매수만 하거나 매도만 할 경우 -> 정책 신경망의 학습이 제대로 되지 않았기 때문.



#### 3.5.2 강화학습 과정을 로그로 남기기

* 어떠한 과정을 텍스트로 남기는 것 : 로그(log)

```
*로그의 종류

1. 하이퍼 파라미터 : learning rate, discount factor, 최소/최대 투자단위(trading unit), 지연보상 임계치(delayed reward threshold)

2. epoch 결과 : 탐험률, 탐험 횟수, 매수 횟수, 매도 횟수, 관망 횟수, 보유 주식 수, 포트폴리오 가치, 긍정적 학습 횟수, 부정적 학습 횟수, 정책신경망 loss값

3. 최종 학습 결과 : 최대 포트폴리오 가치, 수익 발생 횟수, 최종 통계치 , etc
```



#### 3.5.3 강화학습 과정을 이미지로 가시화하기

* 각 epoch마다 보유 주식 수, 행동, 모델의 출력 값, 탐험, 수익과 손실을 가시화

```
1. 주식 종목 일봉 차트 : 강화학습의 환경(environment)
2. 에이전트가 수행한 행동과 보유 주식 수 차트 : 실선 - 보유 주식 수 /  배경 색 - 매수(빨강) , 매도(파랑)
3. 신경망 출력 값 차트 : 점 - 매수 확률(빨강) , 매도 확률(파랑) / 배경 색 - 매수(빨강), 매도(파랑), 탐험(노랑)
4. 포트폴리오 가치 차트 : 실선 - 포트폴리오 가치 / 배경 색(세로 줄) : 긍정 보상(빨강), 부정 보상(파랑) / 배경 색(기준 선과 그래프사이의 넓이) : 수익(빨강), 손실(파랑)
```

* 학습 평가의 질 => 가시화

#### 3.6 이번 장의 요점



### 04장 모듈 개발 : 강화학습 기반 주식투자 시스템 개발

Reinforcement Learning Trader 의 소스코드 개발

#### 4.1 RLTrader 개발에 필요한 환경

아나콘다, Numpy, Pandas, Matplotlib, Tensorflow, Keras

#### 4.1.1 아나콘다 설치

#### 4.1.2 텐서플로와 케라스 설치

#### 4.2 RLTrader의 구조

정책 학습기, 가시화기, 에이전트, 환경, 정책 신경망

#### 4.2.1 모듈 구조

```sequence
가시화기(visualizer.py)->정책학습기(policy_learner.py): 
정책학습기(policy_learner.py)->에이전트(agent.py): 
정책학습기(policy_learner.py)->환경(environment.py): 
정책학습기(policy_learner.py)->정책 신경망(policy_network.py): 
```



#### 4.2.2 디렉터리 구조

```
.rltrader
├── chart_data/
├── logs/
├── epochs_summary/
├── models/
├── environment.py
├── agent.py
├── policy_network.py
├── visualizer.py
└── policy_learner.py
```



#### 4.2.3 에이전트 모듈 개요

```
agent.py : 주식을 매도하거나 매수하는 투자자 역할을 함.
초기 자본금, 현금 잔고, 주식 잔고라는 상태가 있음.

* 포트폴리오 가치(Portfolio value) : 현금 잔고와 주식 잔고의 평가액을 합한 금액 , 줄여서 PV라고 부름.
```

* 투자의 목표는 PV를 높여나가는 것

#### 4.2.4 환경 모듈 개요

```
environment.py : Environment 클래스가 있음.
에이전트가 투자할 종목의 차트 데이터를 관리함. 
환경 클래스에 전체 차트 데이터가 있지만, 과거 시점부터 가장 최근 시점까지 순차적으로 데이터를 제공함.
즉, 과거로 돌아간 에이전트가 미래의 차트 데이터는 알 수 없다.
```



#### 4.2.5 정책 신경망 모듈 개요

```
policy_network.py : PolicyNetwork 클래스가 있음.
특정 시점의 주식 데이터(sample)이 제공되었을 때 매수할지, 매도할지를 결정하는 에이전트의 뇌와 같은 역할을 함.
LSTM 구조로 되어있고, 매수와 매도 행위에 대해서 PV를 높일 수 있을지의 확률을 계산함.
```

* 주식 잔고 多, 주가 $$\uparrow$$ $$\Rightarrow$$ PV 큰 폭 $$\uparrow$$ 
* 주가 상승 예상 $$\Rightarrow$$ 에이전트의 행위 : 주식 잔고 $$\uparrow​$$
* 주식 잔고 多, 주가 $$\downarrow$$ $$\Rightarrow $$ PV 큰 폭 $$\downarrow$$ 
* 주가 하락 예상 $$\Rightarrow$$ 에이전트의 행위 : 주식 잔고 $$\downarrow$$ 
* 정책 신경망의 출력은 매수와 매도에 대한 확률이 나옴 $$\Rightarrow$$ 매수 if 매수P > 매도P else 매도

#### 4.2.6 가시화기 모듈 개요

```
visualizer.py : 환경, 에이전트 상태, 정책 신경망 출력을 가시화

* 충분한 epoch이 지났음에도 PV가 높아지지 않는다면, 학습이 제대로 이루어 지지 않음을 파악하고 알고리즘 개선
```



#### 4.2.7 정책 학습기 모듈 개요

```
policy_learner.py : PolicyLearner 클래스가 있음.
RLTrader의 몸체가 되고, 학습 데이터를 가지고 있고, 보상이 결정되었을 때 학습데이터로 정책 신경망을 학습시킴.
```



#### 4.3 환경 모듈 개발

* environment.py

#### 4.3.1 환경 모듈의 주요 속성과 함수

```
environment.py 는 투자할 종목의 차트 데이터를 관리하는 작은 모듈.

* 속성
1. chart_data : 주식 종목의 차트 데이터
2. observation : 현재 관측치
3. idx : 차트 데이터에서의 현재 위치
* 함수
1. reset() : idx와 observation을 초기화
2. observe() : idx를 다음 위치로 이동하고 observation을 업데이트
3. get_price() : 현재 observation에서 종가를 획득
```



#### 4.3.2 코드 조각 : 환경 클래스의 전체 소스코드

* environment.py

```python
class Environment : 
    PRICE_IDX = 4 # 종가의 위치
    
    def __init__(self,chart_data=None):
        self.chart_data = chart_data
        self.observation = None
        self.idx = -1
    
    def reset(self):
        self.observation = None
        self.idx = -1
        
    def observe(self):
        if len(self.chart_data) > self.idx + 1:
            self.idx +=1
            self.observation = self.chart_data.iloc[self.idx]
            return self.observation
        return None
    
    def get_price(self):
        if self.observation is not None:
            return self.observation[self.PRICE_IDX]
        return None
```

- iloc() 함수는 DataFrame 함수로 특정 행의 데이터를 가져옴.

#### 4.4 에이전트 모듈 개발

* agent.py

#### 4.4.1 에이전트 모듈의 주요 속성과 함수

```
agent.py 는 투자 행동을 수행하고 투자금과 보유 주식을 관리하기 위한 에이전트 클래스(Agent)를 가짐.

* 속성
1. initial_balance : 초기 투자금
2. balance : 현금 잔고
3. num_stocks : 보유 주식 수
4. portfolio_value : 포트폴리오 가치(투자금 잔고+주식 현재가*보유 주식 수)

* 함수
1. reset() : 에이전트의 상태를 초기화
2. set_balance() : 초기 자본금을 설정
3. get_states() : 에이전트의 상태를 획득
4. decide_action() : 탐험 또는 정책 신경망에 의한 행동 결정
5. validate_action() : 행동의 유효성 판단
6. decide_trading_unit() : 매수 또는 매도할 주식 수 결정
7. act() : 행동 수행
```



#### 4.4.2 코드 조각 1 : 에이전트 클래스의 상수 선언 부분

* agent.py의 상수 선언 부분

```python
import numpy as np

class Agent:
    #에이전트 상태가 구성하는 값 개수
    STATE_DIM = 2 #주식 보유 비율, 포트폴리오 가치 비율
    
    #매매 수수료 및 세금
    TRADING_CHARGE = 0 #거래 수수료 미고려 (일반적으로 0.015%)
    TRADING_TAX = 0 # 거래세 미고려(실제 0.3%)
    
    #행동
    ACTION_BUY = 0 #매수
    ACTION_SELL = 1 #매도
    ACTION_HOLD = 2 #관망
    ACTIONS = [ ACTION_BUY, ACTION_SELL ] #인공 신경망에서 확률을 구할 행동들
    NUM_ACTIONS = len(ACTIONS) #인공 신경망에서 고려할 출력 값의 개수
```

* 매수와 매도 중에서 결정한 행동을 할 수 없을 때만 관망 행동을 함.

#### 4.4.3 코드 조각 2 : 에이전트 클래스의 생성자 부분

* agent.py의 클래스 생성자 부분

```python
    def __init__(
        self, environment, min_trading_unit = 1, max_trading_unit = 2, delayed_reward_threshold= .05):
        # Environment 객체
        self.environment = environment # 현재 주식 가격을 가져오기 위해 환경 참조
        
        # 최소 매매단위 , 최대 매매단위, 지연 보상 임계치
        self.min_trading_unit = min_trading_unit # 최소 단일 거래 단위
        self.max_trading_unit = max_trading_unit # 최대 단일 거래 단위
        self.delayed_reward_threshold = delayed_reward_threshold # 지연 보상 임계치
        
        # Agent 클래스의 속성
        self.balance = 0 # 현재 현금 잔고
        self.num_stocks = 0 # 보유 주식 수
        self.portfolio_value = 0 # balance + num_stocks * {현재 주식 가격}
        self.base_portfolio_value = 0 # 직전 학습 시점의 PV
        self.num_buy = 0 # 매수 횟수
        self.num_sell = 0 # 매도 횟수
        self.num_hold = 0 # 관망 횟수
        self.immediate_reward = 0 # 즉시 보상 
        
        # Agent 클래스의 상태
        self.ratio_hold = 0 # 주식 보유 비율
        self.ratio_portfolio_value = 0 # 포트폴리오 가치 비율
    
```

* immediate_reward : 즉시 보상 - 행동을 수행한 시점에서 수익이 발생한 상태면 1을, 아니었으면 -1을 줌.

* ratio_hold : 주식 보율 비율 - 최대로 보유할 수 있는 주식 수 대비 현재 보유하고 있는 주식 수의 비율
* ratio_portfolio_value : 직전 지연 보상이 발생했을 때의 포트폴리오 가치 대비 현재의 포트폴리오 가치의 비율

#### 4.4.4 코드 조각 3 : 에이전트 클래스의 함수 부분

* agent.py의 함수 부분

```python
	def reset(self):
        self.balance = self.initial_balance
        self.num_stocks = 0
        self.portfolio_value = self.initial_balance
        self.base_portfolio_value = self.initial_balance
        self.num_buy = 0
        self.num_sell = 0
        self.num_hold = 0
        self.num_immediate_reward = 0
        self.ratio_hold = 0
        self.ratio_portfolio_value = 0
        
    def set_balance(self, balance):
        self.initial_balance = balance
        
    def get_states(self):
        self.ratio_hold = self.num_stocks / int(self.portfolio_value / self.environment.get_price())
        self.ratio_portfolio_value = self.portfolio_value / self.initial_balance
        return (self.ratio_hold, self.ratio_portfolio_value)
    def decide_action(self, policy_network, sample, epsilon):
        confidence = 0.
        # 탐험 결정
        if np.random.rand() < epsilon:
            exploration = True
            action = np.random.randint(self.NUM_ACTIONS) #무작위로 행동 결정
        else :
            exploration = False
            probs = policy_network.predict(sample) # 각 행동에 대한 확률
            action = np.argmax(probs)
            confidence = probs[action]
        return action, confidence , exploration
    
    def validate_action(self, action):
        validity = True
        if action == Agent.ACTION_BUY:
			#적어도 1주를 살 수 있는지 확인
            if self.balance < self.environment.get_price() * (1+self.TRADING_CHARGE)*self.min_trading_unit:
                validity = False
        elif action == Agent.ACTION_SELL:
            # 주식 잔고가 있는지 확인
            if self.num_stocks <=0:
                validity = False
        return validity
    
    def decide_trading_unit(self, confidence):
        if np.isnan(confidence):
            return self.min_trading_unit
        added_trading = max(min(int(confidence * (self.max_trading_unit - self.min_trading_unit)),self.max_trading_unit - self.min_trading_unit),0)
        return self.min_trading_unit + added_trading
    
    
    def act(self, action, confidence):
        if not self.validate_action(action):
            action = Agent.ACTION_HOLD
            
        #환경에서 현재 가격 얻기
        curr_price = self.environment.get_price()
        
        #즉시 보상 초기화
        self.immediate_reward = 0
        
        #매수
        if action == Agent.ACTION_BUY :
            #매수할 단위를 판단
            trading_unit = self.decide_trading_unit(confidence)
            balance = self.balance - curr_price * (1 +self.TRADING_CHARGE) * trading_unit
            # 보유 현금이 모자랄 경우, 보유 현금으로 가능한 만큼 최대한 매수
            if balance < 0 :
                trading_unit = max(
                    min(
                    int(self.balance / (curr_price * (1 +self.TRADING_CHARGE))),
                    self.max_trading_unit
                    ),
                    self.min_trading_unit
                )
            #수수료를 적용하여 총 매수 금액 산정
            invest_amount = curr_price * (1 + self.TRADING_CHARGE) * trading_unit
            self.balance -= invest_amount # 보유 현금을 갱신
            self.num_stocks +=trading_unit # 보유 주식 수를 갱신
            self.num_buy += 1 # 매수 횟수 증가
            
        # 매도
        elif action == Agent.ACTION_SELL :
            # 매도할 단위를 판단
            trading_unit = self.decide_trading_unit(confidence)
            # 보유 주식이 모자랄 경우 가능한 만큼 최대한 매도
            trading_unit = min(trading_unit, self.num_stocks)
            # 매도
            invest_amount = curr_price * (
            	1 - (self.TRADING_TAX + self.TRADING_CHARGE)) * trading_unit
            self.num_stocks -= trading_unit # 보유 주식 수를 갱신
            self.balance +=invest_amount # 보유 현금을 갱신
            self.num_sell += 1 #매도 횟수 증가
            
        # 관망
        elif action == Agent.ACTION_HOLD:
            self.num_hold += 1 # 관망 횟수 증가
            
        # 포트폴리오 가치 갱신
        self.portfolio_value = self.balance + curr_price * self.num_stocks
        profitloss = (
        (self.portfolio_value - self.base_portfolio_value) / self.base_portfolio_value)
        
        # 즉시 보상 판단 
        self.immediate_reward = 1 if profitloss >= 0 else -1
        
        # 지역 보상 판단
        if profitloss > self.delayed_reward_threshold :
			delayed_reward = 1
            # 목표 수익률을 달성하여 기준 포트폴리오 가치 갱신
            self.base_portfolio_value = self.portfolio_value
        elif profitloss < -self.delayed_reward_threshold:
            delayed_reward = -1
            #손실 기준치를 초과하여 기준 포트폴리오 가치 갱신
            self.base_portfolio_value = self.portfolio_value
            
        else :
            delayed_reward = 0
        return self.immediate_reward , delayed_reward
            
```

* 주식 보유 비율 (ratio_hold) : 보유 주식 수 / ( 포트폴리오 가치 / 현재 주가 ) 
* ratio_hold가 0이면, 주식은 하나도 보유하지 않은 것, 0.5면 최대 가질 수 있는 주식 대비 절반의 주식을 보유하고 있는 것
* action 은 매수와 매도를 의미하는 0또는 1의 값
* confidence 는 정책 신경망을 통해 결정한 경우 결정한 행동에 대한 소프트맥스 확률 값
* validate_action(action)으로 행동 유효성을 검사하고 할 수 없는 경우, 아무 행동도 하지 않도록 관망(hold)한다.



#### 4.5 정책 신경망 모듈 개발

* Policy_network.py

#### 4.5.1 정책 신경망 모듈의 주요 속성과 함수

```
policy_network.py 는 투자 행동을 결정하기 위해 신경망을 관리하는 PolicyNetwork 클래스를 가집니다.

* 속성
1. model : 케라스 라이브러리로 구성한 LSTM 신경망 모델
2. prob : 가장 최근에 계산한 투자 행동별 확률

* 함수
1. reset() : prob 변수를 초기화
2. predict() : 신경망을 통해 투자 행동별 확률 계산
3. train_on_batch() : 배치 학습을 위한 데이터 생성
4. save_model() : 학습한 신경망을 파일로 저장
5. load_model() : 파일로 저장한 신경망을 로드
```



#### 4.5.2 정책 신경망에서 사용하는 LSTM 신경망의 구조

* 5개의 레이어
* input layer : 17차원
* output layer : 2차원
* hidden layer : LSTM unit 256차원



#### 4.5.3 코드 조각 1 : 정책 신경망 클래스의 생성자 부분

* PolicyNetwork.py

```python
import numpy as np
from keras.models import Sequential
from keras.layers import Activation , LSTM , Dense, BatchNormalization
from keras.optimizers import sgd


class PolicyNetwork:
    def __init__(self, input_dim=0, output_dim=0,ler=0.01):
        self.input_dim = input_dim
        self.lr = lr
        
        #LSTM 신경망
        self.model = Sequential()
        
        self.model.add(LSTM(256, input_shape=(1,input_dim),
                           return_sequences=True, stateful=False, dropout=0.5))
        self.model.add(BatchNormalization())
        self.model.add(LSTM(256, return_sequences=True, stateful=False, dropout=0.5))
        self.model.add(BatchNormalization())
        self.model.add(LSTM(256, return_sequences=True, stateful=False, dropout=0.5))
        self.model.add(BatchNormalization())
        self.model.add(Dense(output_dim))
        self.model.add(Activation('sigmoid'))
        
        self.model.compile(optimizer=sgd(lr=lr), loss='mse')
        self.prob = None
        
```



#### 4.5.4 코드 조각 2 : 정책 신경망 클래스의 함수 선언 부분

```python
	def reset(self):
    self.prob = None
   
	def predict(self, sample):
        self.prob = self.model.predict(np.array(sample).reshape((1,-1,self.input_dim)))[0]
        return self.prob
    
    def train_on_batch(self, x, y):
        return self.model.train_on_batch(x,y)
    
    def save_model(self, model_path):
        if model_path is not None and self.model is not None:
            self.model.save_weights(model_path, overwrite=True)
            
    def load_model(self, model_path):
        if model_path is not None:
            self.model.load_weights(model_path)
            
```



#### 4.6 가시화기 모듈 개발

* visualizer.py

#### 4.6.1 가시화기 모듈의 주요 속성과 함수

```
visualizer.py 는 정책 신경망을 학습하는 과정에서 에이전트의 투자 상황, 정책 신경망의 투자 결정 상황, 포트폴리오 가치의 상황을 time series 로 시각화하는 Visualizer 클래스를 가짐.

* 속성
1. fig : 캔버스 같은 역할을 하는 Matplotlib의 Figure 클래스 객체
2. axes : 차트를 그리기 위한 Matplotlib의 Axes 클래스 객체
* 함수
1. prepare() : Figure를 초기화하고 일봉차트를 출력
2. plot() : 일봉 차트를 제외한 나머지 차트를 출력
3. save() : Figure를 그림 파일로 저장
4. clear() : 일봉 차트를 제외한 나머지 차트들을 초기화
```



#### 4.6.2 가시화기 모듈이 만들어내는 정보

* Figure 제목 : epoch 및 탐험률
* Axes 1 : 종목의 일봉 차트
* Axes 2 : 보유 주식 수 및 에이전트 행동 차트
* Axes 3 : 정책 신경망 출력 및 탐험 차트
* Axes 4 : 포트폴리오 가치 차트

#### 4.6.3 코드 조각 1 : 가시화기 클래스의 생성자 부분

* 우선, mpl_finance 을 사용하기 위해

  https://github.com/matplotlib/mpl_finance 

  git clone 후, 해당 디렉토리에서 python setup.py install



```python
import numpy as. p
import matplotlib.pyplot as plt
from mpl_finance import candlestick_ohlc


class Visualizer:

    def __init__(self):
        self.fig = None  # 캔버스 같은 역할을 하는 Matplotlib의 Figure 클래스 객체
        self.axes = None  # 차트를 그리기 위한 Matplotlib의 Axes 클래스 객체

```



#### 4.6.4 코드 조각 2 : 일봉 차트 가시화 함수 부분

```python
    def prepare(self, chart_data):
        # 캔버스를 초기화하고 4개의 차트를 그릴 준비
        self.fig, self.axes = plt.subplots(nrows=4, ncols=1, facecolor='w', sharex=True)
        for ax in self.axes:
            # 보기 어려운 과학적 표기 비활성화
            ax.get_xaxis().get_major_formatter().set_scientific(False)
            ax.get_yaxis().get_major_formatter().set_scientific(False)
        # 차트 1. 일봉 차트
        self.axes[0].set_ylabel('Env.')  # y 축 레이블 표시
        # 거래량 가시화
        x = np.arange(len(chart_data))
        volume = np.array(chart_data)[:, -1].tolist()
        self.axes[0].bar(x, volume, color='b', alpha=0.3)
        # ohlc란 open, high, low, close의 약자로 이 순서로된 2차원 배열
        ax = self.axes[0].twinx()
        ohlc = np.hstack((x.reshape(-1, 1), np.array(chart_data)[:, 1:-1]))
        # self.axes[0]에 봉차트 출력
        # 양봉은 빨간색으로 음봉은 파란색으로 표시
        candlestick_ohlc(ax, ohlc, colorup='r', colordown='b')
```



#### 4.6.5 코드 조각 3 : 전체 차트 가시화 함수 선언 부분

```python
    def plot(self, epoch_str=None, num_epoches=None, epsilon=None, 
            action_list=None, actions=None, num_stocks=None, 
            outvals=None, exps=None, learning=None,
            initial_balance=None, pvs=None):
        x = np.arange(len(actions))  # 모든 차트가 공유할 x축 데이터
        actions = np.array(actions)  # 에이전트의 행동 배열
        outvals = np.array(outvals)  # 정책 신경망의 출력 배열
        pvs_base = np.zeros(len(actions)) + initial_balance  # 초기 자본금 배열

 
```

- epoch_str : Figure 제목으로 표시할 epoch
- num_epoches : 총 수행할 epoch 수
- epsilon : 탐험률
- action_list : 에이전트가 수행할 수 있는 전체 행동 리스트
- actions : 에이전트가 수행한 행동 배열
- num_stochs : 주식 보유 수 배열
- outvals : 정책 신경망의 출력 배열
- exps : 탐험 여부 배열
- initial_balance : 초기 자본금
- pvs : 포트폴리오 가치 배열

#### 4.6.6 코드 조각 4 : 에이전트 상태 가시화 부분

```python
       # 차트 2. 에이전트 상태 (행동, 보유 주식 수)
        colors = ['r', 'b']
        for actiontype, color in zip(action_list, colors):
            for i in x[actions == actiontype]:
                self.axes[1].axvline(i, color=color, alpha=0.1)  # 배경 색으로 행동 표시
        self.axes[1].plot(x, num_stocks, '-k')  # 보유 주식 수 그리기


```



#### 4.6.7 코드 조각 5 : 정책 신경망 출력 결과 및 탐험 수행 가시화 부분

```python
        # 차트 3. 정책 신경망의 출력 및 탐험
        for exp_idx in exps:
            # 탐험을 노란색 배경으로 그리기
            self.axes[2].axvline(exp_idx, color='y')
        for idx, outval in zip(x, outvals):
            color = 'white'
            if outval.argmax() == 0:
                color = 'r'  # 매수면 빨간색
            elif outval.argmax() == 1:
                color = 'b'  # 매도면 파란색
            # 행동을 빨간색 또는 파란색 배경으로 그리기
            self.axes[2].axvline(idx, color=color, alpha=0.1)
        styles = ['.r', '.b']
        for action, style in zip(action_list, styles):
            # 정책 신경망의 출력을 빨간색, 파란색 점으로 그리기
            self.axes[2].plot(x, outvals[:, action], style)


```



#### 4.6.8 코드 조각 6 : 포트폴리오 가치 및 기타 정보 가시화 부분

```python
        # 차트 4. 포트폴리오 가치
        self.axes[3].axhline(initial_balance, linestyle='-', color='gray')
        self.axes[3].fill_between(x, pvs, pvs_base,
                                  where=pvs > pvs_base, facecolor='r', alpha=0.1)
        self.axes[3].fill_between(x, pvs, pvs_base,
                                  where=pvs < pvs_base, facecolor='b', alpha=0.1)
        self.axes[3].plot(x, pvs, '-k')
        for learning_idx, delayed_reward in learning:
            # 학습 위치를 초록색으로 그리기
            if delayed_reward > 0:
                self.axes[3].axvline(learning_idx, color='r', alpha=0.1)
            else:
                self.axes[3].axvline(learning_idx, color='b', alpha=0.1)

        # 에포크 및 탐험 비율
        self.fig.suptitle('Epoch %s/%s (e=%.2f)' % (epoch_str, num_epoches, epsilon))
        # 캔버스 레이아웃 조정
        plt.tight_layout()
        plt.subplots_adjust(top=.9)


```



#### 4.6.9 코드 조각 7 : 차트 초기화 및 저장 함수 부분

```python
    def clear(self, xlim):
        for ax in self.axes[1:]:
            ax.cla()  # 그린 차트 지우기
            ax.relim()  # limit를 초기화
            ax.autoscale()  # 스케일 재설정
        # y축 레이블 재설정
        self.axes[1].set_ylabel('Agent')
        self.axes[2].set_ylabel('PG')
        self.axes[3].set_ylabel('PV')
        for ax in self.axes:
            ax.set_xlim(xlim)  # x축 limit 재설정
            ax.get_xaxis().get_major_formatter().set_scientific(False)  # 과학적 표기 비활성화
            ax.get_yaxis().get_major_formatter().set_scientific(False)  # 과학적 표기 비활성화
            ax.ticklabel_format(useOffset=False)  # x축 간격을 일정하게 설정
            
    def save(self, path):
        plt.savefig(path)
```



#### 4.7 정책 학습기 모듈 개발

* Policy_learner.py

#### 4.7.1 코드 조각 1 : 정책 학습기 모듈의 의존성 임포트 부분

```python
import os
import locale
import logging
import time
import datetime
import numpy as np 
import settings
from environment import Environment
from agent import Agent
from policy_network import PolicyNetwork
from visualizer import Visualizer

logger = logging.getLogger(__name__)
```

* os는 폴더 생성, 파일 경로 준비를 위해 사용
* locale은 통화 문자열 포맷을 위해서 사용
* logging은 학습 과정 중에 정보를 기록하기 위해서 사용
* time 과 datetime 은 시간 값을 얻어오고 시간 문자열 포맷을 위해 사용

#### 4.7.2 코드 조각 2 : 정책 학습기 클래스의 생성자 부분

```python
class PolicyLearner:
    
    def __init__(self, stock_code, chart_data, training_data = None, min_trading_unit=1, max_trading_unit=2, delayed_reward_threshold=.05, lr=0.01):
        self.stock_code = stock_code # 종목 코드
        self.chart_data = chart_data
        self.environment = Environment(chart_data) # 환경 객체
        # 에이전트 객체
        self.agent = Agent(self.environment, min_trading_unit=min_trading_unit,
                          max_trading_unit=max_trading_unit,
                          delayed_reward_threshold=delayed_reward_threshold)
        self.tranning_data = training_data # 학습 데이터
        self.sample = None
        self.training_data_idx = -1
        # 정책 신경망 ; 입력 크기(17차원) = 학습데이터의 크기(15개 특징) + 에이전트 상태 크기(2개 특징)
        self.num_features = self.training_data.shape[1] +self.agent.STATE_DIM
        self.policy_network = PolicyNetwork(
        input_dim = self.num_features, output_dim = self.agent.NUM_ACTIONS, lr=lr)
        self.visualizer = Visualizer() # 가시화 모듈
        
```

* 인자로 받은 chart_data는 environment 객체를 생성할 때 넣어줌
* Environment 클래스는 차트 데이터를 순서대로 읽으면서 주가, 거래량 등의 환경을 제공
* 학습 데이터는 학습에 사용할 feature들을 포함
* Feature 개수는 학습 데이터에 포함된 15개의 특징과 에이전트의 상태인 2개 특징을 더해서 17개이다.



#### 4.7.3 코드 조각 3 : 에포크 초기화 함수 부분

```python
	def reset(self):
        self.sample = None
        self.training_data_idx = -1
        
```

* 학습데이터를 읽어가면서 training_data_idx 는 1씩 증가함.
* 읽어 온 데이터는 sample에 저장되는데 초기화 단계에서는 읽어온 학습 데이터가 없기 때문에 None으로 할당함.

#### 4.7.4 코드 조각 4 : 학습 함수 선언 부분

```python
	def fit(
    	self, num_epoches=1000, max_memory=60, balance=1000000,
    	discount_factor = 0, start_epsilon=.5,learning=True):
        logger.info('LR : {lr}, DF : {discount_factor},'
                   'TU : [{min_trading_unit}, {max_trading_unit}],'
                   'DRT: {delayed_reward_threshold}'.format(
                   lr=self.policy_network.lr,
                   discount_factor=discount_factor,
                   min_trading_unit=self.agent.min_trading_unit,
                   max_trading_unit=self.agent.max_trading_unit,
                   delayed_reward_threshold=self.agent.delayed_reward_threshold
                   ))
```

* num_epoches 는 수행할 반복 학습의 전체 횟수이다. 
* max_memory 는 배치 학습 데이터를 만들기 위해 과거 데이터를 저장할 배열이다.
  * 배치 학습 데이터를 만드는 함수인 _get_batch()
* balance 는 초기 자본금
  * 보유 현금이 부족하면  정책 신경망 결과 매수가 좋아도 관망함.
* 지연 보상이 발생했을 때 그 이전 지연 보상이 발생한 시점과 현재 지연 보상이 발생한 시점 사이에서 수행한 행동들 전체에 현재의 지연 보상을 적용함.
  * 이 때, 과거로 갈 수록 현재 지연 보상을 적용할 판단 근거가 흐려지기 때문에 먼 과거의 행동일수록 할인 요인을 적용하여 지연 보상을 약하게 적용함.
  * discount_factor 가 할인 요인.
* start_epsilon 은 초기 탐험 비율을 의미함.
* learning 은 학습 유무를 정하는 boolean 값이다. 학습을 마치면, 학습된 정책 신경망이 만들어진다.
* 학습을 해서 정책 신경망 모델을 만들고자 한다면 learning을 True 로, 학습된 모델을 가지고 투자 시뮬레이션만 하려 한다면 learning 을 False 로 준다.

#### 4.7.5 코드 조각 5 : 학습 함수 초반 부분

```python
		# 가시화 준비
    	# 차트 데이터는 변하지 않으므로 미리 가시화
        self.visualizer.prepare(self.environment.chart_data)
        
        # 가시화 결과 저장할 폴더 준비
        epoch_summary_dir = os.path.join(
        settings.BASE_DIR, 'epoch_summary/%s/epoch_summary_%s' % (self.stock_code, settings.timestr))
        if not os.path.isdir(epoch_summary_dir):
            os.makedirs(epoch_summary_dir)
            
        # 에이전트 초기 자본금 설정 
        self.agent.set_balance(balance)
        
        # 학습에 대한 정보 초기화
        max_portfolio_value = 0
        epoch_win_cnt = 0
```

* 가시화 결과를 저장할 폴더를 준비한다.
* settings.timestr 은 폴더 이름에 포함할 날짜와 시간이다. Ex) 199301201230
* 에이전트 초기 자본금을 설정한다.
* 최대 포트폴리오 가치를 저장하는 변수를 초기화하고 수익이 발생한 epoch 수를 저장하는 변수를 초기화한다.

#### 4.7.6 코드 조각 6 : 학습 함수의 로컬 변수 초기화 부분

```python
		# 학습 반복
    	for epoch in range(num_epoches):
    		# epoch 관련 정보 초기화 
            loss = 0.
            itr_cnt = 0
            win_cnt = 0
            exploration_cnt = 0
            batch_size = 0
            pos_learning_cnt =0
            neg_learning_cnt = 0
            # 메모리 초기화
            memory_sample = []
            memory_action = []
            memory_reward = []
            memory_prob = []
            memory_pv = []
            memory_num_stocks = []
            memory_exp_idx = []
            memory_learning_idx = []

```

* loss는 정책 신경망의 결과가 학습 데이터와 얼마나 차이가 있는지를 저장하는 변수
* itr_cnt 는 수행한 epoch 수
* win_cnt 는 수행한 epoch 중에 수익이 발생한 epoch 수
* exploration_cnt 는 무작위 투자를 수행한 수
* pos_learning_cnt 는 수익이 발생하여 긍적적 지연 보상을 준 수
* neg_learning_cnt 는 손실이 발생하여 부정적 지연 보상을 준 수
* 메모리 리스트에 저장하는 데이터는 샘플, 행동 , 즉시보상, 정책 신경망의 출력, 포트폴리오 가치, 보유 주식 수, 탐험 위치, 학습 위치

#### 4.7.7 코드 조각 7 : 학습 함수의 연관 객체 초기화 및 탐험 비율 설정 부분

```python
            # 환경, 에이전트, 정책 신경망 초기화
            self.environment.reset()
            self.agent.reset()
            self.policy_network.reset()
            self.reset()

            # 가시화기 초기화
            self.visualizer.clear([0, len(self.chart_data)])
        
        	# 학습을 진행할 수록 탐험 비율 감소
            if learning:
                epsilon = start_epsilon * (1. - (float(epoch)/(num_epoches - 1))
            else:
                epsilon = 0
```

* 환경, 에이전트, 정책 신경망 reset()
* 가시화기의 clear()
* epsilon 값을 정하는데, 최초 탐험율 start_epsilon 값에 현재 epoch 수에 학습 진행률을 곱해서 정한다.

#### 4.7.8 코드 조각 8 : 학습 함수의 에포크 구행 while 문 초반부

```python
			while True:
        		# 샘플 생성
            	next_sample = self._build_sample()
                if next_sample is None :
                    break
                    
                # 정책 신경망 또는 탐험에 의한 행동 결정
                action, confidence, exploration = self.agent.decide_action(
                self.policy_network, self.sample, epsilon)
                
                # 결정한 행동을 수행하고 즉시 보상과 지연 보상 획득
                immediate_reward , delayed_reward = self.agent.act(action, confidence)
```

* next_sample이 None 이라면, 마지막까지 데이터를 다 읽은 것이므로, 반복문을 종료한다.
* 투자 행동을 결정한다. 매수와 매도 중 하나를 정한다.
* 탐험률에 의해 탐험 또는 정책 신경망의 출력을 통해 결정한다.
* 매수에 대한 정책 신경망 출력이 매도에 대한 출력보다 높으면 매수를, 그 반대의 경우는 매도를 선택한다.
* decide_action() 함수가 반환하는 값은 결정한 행동인 action, 결정에 대한 확신도인 confidence, 무작위 투자 유무인 exploration 이다.
* 행동을 수행하고 즉시 보상과 지연 보상을 반환한다.

#### 4.7.9 코드 조각 9 : 학습 함수의 행동과 그 결과를 저장하는 부분

```python
				# 행동 및 행동에 대한 결과를 기억
    			memory_sample.append(next_sample)
    			memory_action.append(action)
            	memory_reward.append(immediate_reward)
                memory_pv.append(self.agent.portfolio_value)
                memory_num_stocks.append(self.agent.num_stocks)
                memory = [(
                	memory_sample[i],
                	memory_action[i],
                	memory_reward[i])
                	for i in list(range(len(memory_action)))[-max_memory:]
                ]
                if exploration:
                    memory_exp_idx.append(itr_cnt)
                    memory_prob.append([np.nan] * Agent.NUM_ACTIONS)
                else :
                    memory_prob.append(self.policy_network.prob)
                    

```

* Memory 는 학습 데이터의 샘플, 에이전트 행동, 즉시보상, 포트폴리오 가치, 보유 주식 수를 저장하는 배열이다.
* 각 데이터를 메모리에 추가
* 무작위 투자로 행동을 결정한 경우에 현재의 인덱스를 memory_exp_id에 저장한다.
* Memory_prob은 정책 신경망의 출력을 그대로 저장하는 배열이다.
* 무작위 투자에서는 정책 신경망의 출력이 없기 때문에 nan값으로 넣어준다.
* 메모리 변수들은 $$1)$$학습에서 배치 학습 데이터로 사용하고 $$2)$$가시화기에서 차트를 그릴 때 사용한다.

#### 4.7.10 코드 조각 10 : 학습 함수의 반복 정보 갱신 부분

```python
                    
                # 반복에 대한 정보 갱신
               	batch_size += 1
                itr_cnt += 1
                exploration_cnt += 1 if exploration else 0 
                win_cnt += 1 if delayed_reward > 0 else 0
                
                # 학습 모드이고, 지연 보상이 존재할 경우 정책 신경망 갱신
                if delayed_reward ==0 and batch_size >= max_memory:
                    delayed_reward = immediate_reward
                    self.agent.base_portfolio_value = self.agent.portfolio_value
                if learning and delayed_reward != 0:
                    # 배치 학습 데이터 크기
                    batch_size = min(batch_size , max_memory)
                    # 배치 학습 데이터 생성
                    x , y = self._get_batch(
                    	memory, batch_size, discount_factor, delayed_reward)
                    
```

* 배치 크기 batch_size, 반복 카운팅 횟수 itr_cnt, 무작위 투자 횟수 exploration_cnt, 수익이 발생한 횟수 win_cnt를 증가시킨다.
* exploration_cnt 의 경우 탐험을 한 경우에만 1을 증가시키고 그렇지 않으면 0을 더해서 변화가 없게 한다.
* win_cnt도 지연 보상이 0보다 큰 경우에만 1을 증가시키고 그렇지 않으면 0을 더해서 변화가 없게 한다.

#### 4.7.11 코드 조각 11 : 학습 함수의 정책 신경망 학습 부분

```python
				# 학습 모드이고 지연 보상이 존재할 경우 정책 신경망 갱신
    			if delayed_reward == 0 and batch_size >= max_memory :
        		    delayed_reward = immediate_reward
                if learning and delayed_reward !=0:
                    # 배치 학습 데이터 크기
                    batch_size = min(batch_size, max_memory)
                    # 배치 학습 데이터 생성
                    x, y = self._get_batch(
                    	memory, batch_size, discount_factor, delayed_reward)
                    if len(x) > 0 :
                        if delayed_reward > 0:
                            pos_leargning_cnt +=1
                        else :
                            neg_learning_cnt +=1
                        # 정책 신경망 갱신
                        loss += self.policy_network.train_on_batch(x,y)
                        memory_learning_idx.append([itr_cnt, delayed_reward])
                    batch_size =0

```

* 학습 없이 메모리가 최대 크기만큼 다 찼을 경우 즉시 보상으로 지연 보상을 대체하여 학습을 진행한다.
* 배치 학습에 사용할 배치 데이터 크기를 결정한다.
* _get_batch() 함수를 통해 배치 데이터를 준비한다.
* 학습 데이터 샘플, 에이전트 행동, 즉시 보상을 담고 있는 memory, 생성할 배치 데이터 크기, 할인 요인, 지연 보상을 인자로 넣어준다.
* 지연 보상이 긍정이면, 긍정 지연보상 횟수를 세고, 부정일 경우, 부정 지연 보상 횟수를 센다.
* 학습은 train_on_batch() 함수로 수행한다. 학습 인덱스는 memory_learning_idx에 저장한다.

#### 4.7.12 코드 조각 12 : 에포크 결과 가시화 부분

```python
                    # 에포크 관련 정보 가시화
    				num_epoches_digit = len(str(num_epoches))
        			epoch_str = str(epoch +1).rjust(num_epoches_digit, '0')
            		
                	self.visualizer.plot(
                    	epoch_str = epoch_str, num_epoches=num_epoches, epsilon=epsilon,
                    	action_list=Agent.ACTIONS, actions=memory_action,
                    	num_stocks=memory_num_stocks, outvals=memory_prob,
                    	exps=memory_exp_idx, learning=memory_learning_idx,
                    	initial_balance=self.agent.initial_balance,pvs=memory_pv)
                    
                    self.visualizer.save(os.path.join(
                    	epoch_summary_dir,'epoch_summary_%s_%s.png' %(
                        settings.timestr, epoch_str)))

```

* 총 에폭 수의 문자열 길이를 확인한다. 총 에폭 수가 1000이면 길이는 4가 된다.
* 현재 에폭 수를 4자리 문자열로 만들어 준다.
* 가시화기의 plot()함수를 호출하여 에폭 수행 결과를 가시화 한다.
* 가시화한 에폭 수행 결과를 파일로 저장한다.

#### 4.7.13 코드 조각 13 : 에포크 결과 로그 기록 부분

```python
                    # epoch 관련 정보 로그 기록
    				if pos_learning_cnt + neg_learning_cnt > 0:
            			loss /= pos_learning_cnt + neg_learning_cnt
                	logger.info('[Epoch %s/%s]\tEpsilon:%.4f\t#expl.:%d/%d\t'
                               '#Buy :%d\r#Sell:%d\t#Hold:%d\t'
                               '#Stocks:%d\tPV:%s\t'
                               'POS:%s\tNEG:%s\tLoss:%10.6f' % (
                               epoch_str,num_epoches, epsilon, exploration_cnt, itr_cnt, 								self.agent.num_buy, self.agent.num_sell, self.agent.num_hold, self.agent.num_Stocks,
                               locale.currency(self.agent.portfolio_value, grouping=True),pos_learning_cnt,neg_learning_cnt,loss))

```

* Epoch 수, 탐험률, 탐험 횟수, 매수 횟수,매도 횟수, 관망 횟수, 보유 주식 수, 포트폴리오 가치, 긍정적 학습 횟수, 부정적 학습 횟수, 학습 손실을 로그로 남긴다.

#### 4.7.14 코드 조각 14 : 학습 통계 정보 갱신 부분

```python
                    # 학습 관련 정보 갱신
    				mx_portfolio_value = max(
                    	max_portfolio_value, self.agent.portfolio_value)
        			if self.agent.portfolio_value > self.agent.initial_balance:
                		epoch_win_cnt += 1

```

* 최대 포트폴리오 가치 max_portfolio_value 와 수익이 발생한 에폭의 수 epoch_win_cnt이다.
* 학습이 끝나지 않으면 코드조각 6의 for문의 epoch으로 반복문이 돌게 된다.

#### 4.7.15 코드 조각 15 : 최종 학습 결과 통계 정보 로그 기록 부분

```python
                    # 학습 관련 정보 로그 기록
    				logger.info('Max PV :%s, \t # Win: %d' % (
                    	locale.currency(max_portfolio_value, grouping=True), epoch_win_cnt))
```

* 최대 포트폴리오 가치 max_portfolio_value와 수익이 발생한 에폭의 수 epoch_win_cnt를 로깅한다.

#### 4.7.16 코드 조각 16 : 미니 배치 데이터 생성 함수 부분

```python
			def _get_batch(self, memory, batch_size, discount_factor, delayed_reward):
        		x = np.zeros((batch_size, 1, self.num_features))
            	y = np.full((batch_size, self.agent.NUM_ACTIONS),0.5)
                for i , (sample, action, reward) in enumerate(
                	reversed(memory[-batch_size:])):
                    x[i] = np.array(sample).reshape((-1,1,self.num_features))
                    y[i, action] =  (delayed_reward +1) /2
                    if discount_factor > 0:
						y[i, action] *= discount_factor **i
                return x, y

```

* x 는 일련의 학습 데이터 및 에이전트 상태이고, y는 일련의 지연 보상이다. 
* x 배열의 형태는 배치 데이터 크기, 학습 데이터 특징 크기로 2차원으로 구성된다.
* y 배열의 형태는 배치 데이터 크기, 정책 신경망이 결정하는 에이전트 행동의 수로 2차원으로 구성된다 .
* y 는 0.5로 일괄적으로 채워놓는다.
* Numpy의 full() 함수는 첫 번째 인자로 배열의 형태인 SHAPE를 받아서 두 번쨰 인자로 입력된 값으로 채워진 NUMPY 배열을 반환한다.
* 학습 데이터 특징 크기와 에이전트 행동 수는 2로 고정되어 있다.
* 지연 보상으로 정답을 설정하여 Y 데이터를 구성한다.
* 지연보상이 1인 경우 1로, 지연 보상이 -1 인 경우 0으로 레이블링을 한다.

#### 4.7.17 코드 조각 17 : 학습 데이터 샘플 생성 부분

```python
            def _build_sample(self):
        		self.environment.observe()
            	if len(self.training_data) > self.training_data_idx +1 :
					self.training_data_idx +=1
                    self.sample = self.training_data.iloc[self.training_data_idx].tolist()
                    self.sample.extend(self.agent.get_states())
                    return self.sample
             	return None
```

* 환경 객체의 observe() 함수를 호출하여 차트 데이터의 현재 인덱스에서 다음 인덱스 데이터를 읽는다.
* 다음 인덱스가 존재하는지 검사한다.
* 다음 인덱스 데이터가 존재하면 TRAINING_DATA_idx 변수를 1 증가시키고 training_data 배열에서 training_data_idx 인덱스의 데이터를 받아와서 sample에 저장한다.
* 이 샘플의 크기는 15개의 특징을 포함하고 있다.
* sample에 에이전트 상태를 추가하여 17개의 값으로 구성하도록 한다. 그리고 이 Sample을 반환한다.

#### 4.7.18 코드 조각 18 : 투자 시뮬레이션을 하는 trade()함수 부분

* 주식 투자 시뮬레이션을 진행하기 위한 trade() 함수를 보여준다.

```python
			def trade(self, model_path=None, balance=200000):
        		if model_path is None:
                	return
                self.policy_network.load_model(model_path=model_path)
                self.fit(balance=balance, num_epoches=1, learning=Flase)
                
```

* 먼저 학습된 정책 신경망 모델을 정책 신경망 객체의 load_model로 적용시켜 준다.
* 학습된 정책 신경망으로 투자 시뮬레이션을 하는 것이므로 반복 투자를 할 필요가 없다.
* Learning 인자에 False를 넘겨주고 Num_epoches를 1로 준다
* 이렇게 하면 학습을 진행하지 않고 정책 신경망에만 의존하여 투자 시뮬레이션을 진행한다. 무작위 투자는 수행하지 않는다.

### 05장 데이터 준비 : 주식 데이터 획득

#### 5.1 방법 1. 증권사 HTS 사용

#### 5.1.1 증권사 HTS 다운로드

#### 5.1.2 증권 계좌 개설

#### 5.1.3 종목 차트 데이터 확인

#### 5.1.4 일별 데이터 엑셀 파일 저장

#### 5.2 방법 2. 증권사 API 사용

#### 5.2.1 증권사 API 설치

#### 5.2.2 대신증권 크레온 API 사용 환경 준비

#### 5.2.3 대신증권 크레온 HTS 실행

#### 5.2.4 대신증권 크레온 API를 이용한 차트 데이터 획득 프로그램 작성

#### 5.3 방법 3. 포털 사이트 사용

#### 5.3.1 pandas-datareader, fix_yahoo_finance 설치하기

#### 5.3.2 Google Finance에서 주식 데이터 획득하기

#### 5.3.3 Yahoo Finance에서 주식 데이터 획득하기

#### 5.4 이번 장의 요점



### 06장 모델 구축 : 투자 시뮬레이션

#### 6.1 주식 데이터 전처리



#### 6.1.1 코드 조각 1 : CSV 파일을 읽는 부분

#### 6.1.2 코드 조각 2 : 종가와 거래량의 이동 평균 구하기

#### 6.1.3 코드 조각 3 : 주가와 거래량의 비율 구하기

#### 6.1.4 코드 조각 4 : 주가와 거래량의 이동 평균 비율 구하기

#### 6.2 주식 데이터 학습

#### 6.2.1 코드 조각 1 : 강화학습을 실행하는 메인(main)모듈

#### 6.2.2 코드 조각 2 : 강화학습에 필요한 주식 데이터 준비 부분

#### 6.2.3 코드 조각 3 : 데이터를 차트 데이터와 학습 데이터로 분류하는 부분

#### 6.2.4 코드 조각 4 : 강화학습을 시작하는 부분

#### 6.3 학습 과정 및 결과 확인

#### 6.3.1 콘솔에 출력되는 로그의 의미

#### 6.3.2 가시화 결과가 저장되는 그림 파일

#### 6.4 이번 장의 요점



### 07장 모델 검증 : 투자 시뮬레이션

#### 7.1 투자 시뮬레이션 결과 1 : 삼성전자(005930)

#### 7.1.1 종목의 개요

#### 7.1.2 주식 데이터 전처리

#### 7.1.3 학습 파라미터 설정

#### 7.1.4 에포크 10일 때의 결과

#### 7.1.5 에포크 200일 때의 결과

#### 7.1.6 에포크 600일 때의 결과

#### 7.1.7 에포크 1000일 때의 결과

#### 7.1.8 총평

#### 7.2 투자 시뮬레이션 결과 2 : SK하이닉스(000660)

#### 7.2.1 종목의 개요

#### 7.2.2 주식 데이터 전처리

#### 7.2.3 학습 파라미터 설정

#### 7.2.4 에포크 10일 때의 결과

#### 7.2.5 에포크 200일 때의 결과

#### 7.2.6 에포크 600일 때의 결과

#### 7.2.7 에포크 1000일 때의 결과

#### 7.2.8 총평

#### 7.3 투자 시뮬레이션 결과 3 : 현대차(005380)

#### 7.3.1 종목의 개요

#### 7.3.2 주식 데이터 전처리

#### 7.3.3 학습 파라미터 설정

#### 7.3.4 에포크 10일 때의 결과

#### 7.3.5 에포크 200일 때의 결과

#### 7.3.6 에포크 600일 때의 결과

#### 7.3.7 에포크 1000일 때의 결과

#### 7.3.8 총평

#### 7.4 투자 시뮬레이션 결과 4 : LG화학(051910)

#### 7.4.1 종목의 개요

#### 7.4.2 주식 데이터 전처리

#### 7.4.3 학습 파라미터 설정

#### 7.4.4 에포크 10일 때의 결과

#### 7.4.5 에포크 200일 때의 결과

#### 7.4.6 에포크 600일 때의 결과

#### 7.4.7 에포크 1000일 때의 결과

#### 7.4.8 총평

#### 7.5 투자 시뮬레이션 결과 5 : 네이버(035420)

#### 7.5.1 종목의 개요

#### 7.5.2 주식 데이터 전처리

#### 7.5.3 학습 파라미터 설정

#### 7.5.4 에포크 10일 때의 결과

#### 7.5.5 에포크 200일 때의 결과

#### 7.5.6 에포크 600일 때의 결과

#### 7.5.7 에포크 1000일 때의 결과

#### 7.5.8 총평

#### 7.6 투자 시뮬레이션 결과 6 : KT(030200)

#### 7.6.1 종목의 개요

#### 7.6.2 주식 데이터 전처리

#### 7.6.3 학습 파라미터 설정

#### 7.6.4 에포크 10일 때의 결과

#### 7.6.5 에포크 200일 때의 결과

#### 7.6.6 에포크 600일 때의 결과

#### 7.6.7 에포크 1000일 때의 결과

#### 7.6.8 총평

#### 7.7 투자 시뮬레이션 결과 정리 및 원숭이 투자와의 비교

#### 7.8 이번 장의 요점



#### 08장 모델 활용 : 학습된 정책 신경망 모델을 사용한 투자 시뮬레이션

#### 8.1 모델 학습과 모델 활용의 차이점

#### 8.1.1 시뮬레이션 과정 차이점

#### 8.1.2 소스코드의 차이점

#### 8.2 학습된 정책 신경망 모델을 사용한 투자 시뮬레이션

#### 8.2.1 학습된 모델 적용 1 : 삼성전자(005930)

#### 8.2.2 학습된 모델 적용 2 : SK하이닉스(000660)

#### 8.2.3 학습된 모델 적용 3 : 현대차(005380)

#### 8.2.4 학습된 모델 적용 4 : LG화학(051910)

#### 8.2.5 학습된 모델 적용 5 : NAVER(035420)

#### 8.2.6 학습된 모델 적용 6 : KT(030200)

#### 8.2.7 총평

#### 8.3 투자 시뮬레이션 결과 정리 및 원숭이 투자와의 비교

#### 8.4 이번 장의 요점









