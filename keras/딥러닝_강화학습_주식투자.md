## 파이썬과 케라스를 이용한 딥러닝 / 강화학습 주식투자

### 01장 . 배경이론 1 : 딥러닝이란?

### 1.1 딥러닝 개요

#### 1.1.1 딥러닝의 정의와 역사

```
머신러닝 : ML(Machine Learning)

인공 신경망 : ANN(Artificial Neural Network)

인공지능 : AI(Artificial Intelligence)

심층 신경망 : DNN(Deep Neural Network)
```

1950년대 퍼셉트론 등장 -> 1960년대 퍼셉트론 한계 증명 -> 1970년대 인공신경망 암흑기 -> 1980년대 인공 신경망 부활기 (오차 역전파 적용, DNN, RNN, CNN 발전) -> 1990년대 LSTM등장,LeNet-5등장 -> 2000년대 가트너 10대 전략 기술 -> 2010년대 알파고

#### 1.1.2 딥러닝이 최근에 주목받는 이유

컴퓨터 연산능력이 좋아졌음 , 빅데이터 등장

#### 1.1.3 딥러닝으로 풀고자 하는 문제

문제 : 분류(classification) , 군집화(clustering), 회귀(regression)

학습 방법 : 지도학습, 비지도 학습, 강화학습

```
강화학습 : 에이전트가 어떠한 환경에서 행동을 수행했을 때 보상을 함으로써 , 에이전트는 그 보상을 최대로 하는 행동을 수행하도록 학습하게 하는 방법. 
행동 결정 분류 문제나 보상 예측의 회귀 문제를 다룸.
레이블이 없는 데이터를 학습할 수 있지만, 에이전트와 환경을 구성하는 추가적인 비용을 필요로 함.
```

### 1.2 딥러닝의 발전 과정

#### 1.2.1 퍼셉트론

```
퍼셉트론 : 입력값(input)과 편향값(bias)에 가중치(weight)를 곱하여 합한 값이 threshold를 넘어가면 1, 그렇지 않으면 0을 출력하는 활성화 함수를 가지는 구조
```



#### 1.2.2 인공 신경망

```
* 인공 신경망(ANN) : input layer , hidden layer, output layer로 구성된 연결망
* 은닉층(hidden layer)에서 활성화 함수를 사용.
```

* ##### 활성화 함수 종류 

  step function, ReLU function, leaky ReLU function, sigmoid function, hyperbolic tangent(tanh) function

  #### forward activation functions

  * step function
    * $$h(z) = \begin{cases} 1,&z>0이면 \\ -1,&그\ 외\end{cases}​$$
  * ReLU function
    * $$h(z)=\begin{cases}z,&z>0이면\\ 0,&그\ 외\end{cases}$$

  * LeakyReLU function
    * $$h(z)=\begin{cases}z,&z>0이면\\z*\alpha,&그\ 외 \end{cases}$$
  * sigmoid function
    * $$h(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{e^z+1} $$
  * Hyperbolic tangent(tanh) function
    * $$h(z)=\frac{sinhz}{coshz}=\frac{\frac{1-e^{-2z}}{2e^{-2z}}}{\frac{1+e^{-2x}{}}{2e^{-z}}}=\frac{1-e^{-2z}}{1+e^{-2z}}​$$

* 출력층에서도 활성화 함수 사용

  * Sigmoid 또는 Softmax 사용
  * 출력층에서의 sigmoid : 이항 분류 문제에서 출력 값을 확률화
  * 출력층에서의 softmax : 다항 분류 문제에서 출력 값을 확률화

  

#### 1.2.3 심층 신경망

```
심층 신경망(DNN): 은닉층이 2개 이상인 ANN
```

### 1.3 딥러닝에 필요한 핵심 기술

#### 1.3.1 오차 역전파 기법

```
오차 역전파(backpropagation) : forward된 weight를 편미분한 값을 backward 형태로 곱해 나가면서 그 최종 역전파 값을 가중치에 더해서 조정
```

* #### back propagation functions

  * Step function
    * $$\frac{\delta y}{\delta z}=\begin{cases}0,&z>0이면\\ 0,& 그\ 외 \end{cases} =0​$$
  * ReLU function
    * $$\frac{\delta y}{\delta z}=\begin{cases}1,&z>0이면\\ 0,& 그\ 외 \end{cases}$$
  * sigmoid function
    * $$\frac{\delta y}{\delta z}=y(1-y)​$$
  * Hyperbolic tangent(tanh) function
    * $$\frac{\delta y}{\delta z}=1-y^2$$
  * Softmax function
    * $$\frac{\delta y_i}{\delta z_i}=y_i-t_i(t_i는\ i번\ 째\ 레이블)$$

  ```
  * 역전파 함수 설명
  sigmoid function의 순전파를 미분하면 모두 0이 되어 가중치에 영향을 주지 않음.
  ReLU function은 가중치 곱의 합이 양수일 때만 뒤쪽 계층에서 넘어온 역전파를 그대로 넘김.
  sigmoid function와 hyperbolic tangent function은 순전파 값에만 영향이 있음.
  softmax function에서 i번째 레이블에 대한 역전파 값은 i번째 레이블에 대한 순전파 값의 i번째 레이블 값(정답 값)이 됨.
  ```

  

#### 1.3.2 최적해 탐색 기법

신경망의 출력값과 레이블과의 차이를 줄여감.

SGD, RmsProp, Adam, NAG의 optimizer가 있음.

SGD는 GD와 학습방법은 동일하지만, 학습 수행 시점과 그 양에서 차이가 있음.

GD는 샘플 하나하나를 가중치 갱신에 사용하지만, SGD는 미니 배치에서 가중치 갱신이 이루어짐.

따라서, GD는 SGD보다 학습량이 많으며, 수행속도가 더 느리다.

학습 데이터가 적으면, GD를 학습데이터가 많으면, SGD를 사용하는 것이 좋다.

#### 1.3.3 과적합 해결 기법

```
오버피팅을 방지하기 위한 대표적 두가지 기법 : regularization(정규화)와 dropout(드롭아웃)
```

* Regularization : $$L(w) = \frac{1}{m}\sum_{i=1}^m(h(z_i)-t_i)^2+\lambda\sum w^2$$
* Dropout : 일부 뉴런을 생략하고 학습을 진행 -> 테스트할 때는 뉴런을 전부 사용하여 테스트

#### 1.4 고급 신경망 구조

* RNN, CNN

#### 1.4.1 순환 신경망

* RNN : 은닉층에서의 출력 값을 다음 학습 때 다시 입력에 추가하는 형태

#### 1.4.2 LSTM 신경망

- LSTM : RNN의 일종으로 이전 상태들을 더 잘 기억할 수 있도록 개선된 형태
- LSTM의 3가지 게이트 : 망각 게이트(forget gate), 입력 게이트(input gate), 출력 게이트(output gate) - sigmoid function을 통해 잊을 값, 받아들일 값, 내보낼 값을 정함.

#### 1.4.3 합성곱 신경망

* convolution계층과 pooling 계층을 이용하여 feature를 추출하는 형태
* fullyconnected 계층이 마지막부분에 이어지고 최종적으로 softmax 계층을 거쳐 결과가 출력됨
* Avg pooling과 Max pooling 으로 pooling 종류는 크게 두 가지가 존재.

#### 1.5 딥러닝 적용 사례

기계 번역, 음성인식, 이미지 인식

#### 1.5.1 기계 번역

#### 1.5.2 음성 인식

#### 1.5.3 이미지 인식



### 02장 . 배경 이론 2: 강화 학습이란?

```
강화학습(RL)은 머신러닝 기법의 한 가지로, 어떠한 환경에서 행동에 대한 상벌을 주면서 학습하는 분야.
구성요소로는 에이전트와 환경이 있음.
에이전트는 특정 환경에서 행동을 결정하고, 환경을 그 결정에 대한 보상을 내린다. 이 보상은 행동 즉시 결정되기 보다 여러 행동들을 취한 후에 한꺼번에 결정되는 경우가 많음.
특정 행동을 취했을 때 바로 그행동에 대한 평가를 내릴 수 없는 경우가 많기 때문.
에이전트가 행동을 결정하고 환경이 주는 보상으로 스스로 학습 할때, 주로 딥러닝에서 다룬 인공 신경망을 사용함. 환경과 에이전트의 상태 등을 입력값으로 인공 신경망이 행동을 결정하고 보상이 있으면, 이전의 입력 값과 행동들을 긍정적으로 학습함.
```



#### 2.1 강화학습의 기초가 된 마르코프 의사결정 과정

```
마르코프 의사결정(Markov decision process, MDP)에 학습 개념을 추가한 것이 강화학습
```



#### 2.1.1 마르코프 가정

```
마르코프 가정(Markov assumption)은 상태가 연속적인 시간에 따라 이어질 때, 어떠한 시점의 상태는 그 시점 바로 이전의 상태에만 영향을 받는다는 가정. 직관적으로 현재 -> 이전에 영향 , 이전 -> 더 이전에 영향, .... 으로 단순화 시킴.
```

$$
P(x_t \ | \ x_1,x_2,x_3,...,x_{t-1}) \ = \ (x_t \ | \ x_{t-1})
$$

$$ 좌변$$ 은 어떠한 시점 t에서의 상태 $$x_t$$ 는 최초의 상태 $$x_1$$ 에서 바로 이전의 상태 $$x_{t-1}$$ 까지에 영향을 받는다는 뜻이고 연속적으로 존재하며, 이를 실제로 계산하기는 어렵다. 그래서 상태 $$x_t$$ 는 바로 이전 상태인 $$x_{t-1}$$ 에 가장 큰 영향을 받고 $$x_{t-1}$$ 에서 $$x_{t-2}$$ 를 반영하는 등 연쇄적으로 모든 이전 상태들이 반영된다고 가정하는 마르코프 가정을 적용하면 $$우변$$ 과 같이 단순화 된다.

#### 2.1.2 마르코프 과정

* Markov process 는 마르코프 가정을 만족하는 연속적인 일련의 상태
* Markov process 는 일련의 상태 $$ <x_1,x_2,x_3,...,x_n>$$와 상태 전이 확률(state transition probability) $$p$$ 로 구성됨
* 상태 전이 확률 $$p_{ij}=P(x_{t+1}=j|x_t=i)$$ 은 어떠한 상태가 $$i$$ 일 때 그 다음 상태가 $$j$$가 될 확률을 의미



#### 2.1.3 마르코프 의사결정 과정

```
마르코프 의사결정 과정(Markov decision process, MDP)은 마르코프 과정을 기반으로한 의사결정 모델이다.
```

$$
MDP = (S,A,P,R,\gamma)
$$

MDP 는 상태(state) 집합 $$S$$ , 행동(action) 집합 $$A$$ , 상태 전이 확률(state transition probability) $$P$$, 보상(reword) 함수 $$ R$$ , 할인 요인(discount factor) $$\gamma$$ 로 구성되어 있음

* 상태 집합은 MDP에서 가질 수 있는 모든 상태의 집합 $$\{s_1,s_2,...,s_{|S|}\}​$$

* 행동 집합은 행동 주체인 에이전트가 할 수 있는 모든 행동들의 집합 $$\{a_1,a_2,...,a_{|A|}\}$$

* 상태 전이 확률은 마르코프 과정의 상태 전이 확률보다 조금 더 복잡하다. $$P_{s_i,s_j}^a=\ P(x_{t+1} = s_j|\ x_t=s_i,A_t=a)​$$  

* $$P_{s_i,s_j}^a$$ 는 에이전트가 어떠한 상태 $$s_i$$에서 행동 $$a$$ 를 취했을 때, 상태 $$s_j​$$ 로 변할 확률

* 보상 함수는 에이전트가 어떠한 상태에서 취한 행동에 대한 보상을 내리기 위한 함수

  $$R_s^a:s,a\rightarrow\mathbb{R}​$$ 

  보상 함수$$ R_s^a$$ 는 상태 $$s$$ 에서 행동 $$a$$ 를 했을 때의 보상을 수치로 반환

* 할인 요인은 과거의 행동들을 얼마나 반영할지를 정하는 값 $$\rightarrow$$ 0에서 1사이의 값

  과거 5번의 행동에 대한 보상을 1씩 받았다고 했을 때, 할인 요인 $$\gamma$$ 이 1이면, <1,1,1,1,1> 이 되고,

  할인 요인 $$\gamma$$ 이 0.9 이면, <1, 0.9, 0.81, 0.729, 0.6561> 이 된다. 즉 먼 과거에 대한 보상일 수록 깎아서 반영한다.

#### 2.2 주요 강화학습 기법

```
복잡한 문제를 풀기 위한 강화학습의 특징 : 1.상태 집합이 (거의) 무한하다. 2.상태 전이 확률을 인공 신경망으로 구한다.
```

상태를 구성하는 요소가 많아질수록 상태의 공간이 방대해진다. 그래서 인공신경망을 사용한다.

강화학습 기법은 Q러닝과 정책 경사(policy gradient)가 있음.

#### 2.2.1 Q러닝 강화학습

```
Q러닝은 회귀(regression)문제를 푸는 강화학습으로 볼 수 있다.
어떠한 상태에서 어떠한 행동을 했을 때, 예상 가치(value)를 상태-행동가치(state-action value)라고 부른다.
Q러닝은 state-action value가 가장 높은 행동을 취하고, 복잡한 문제에서 상태 공간이 매우 크므로, state-action value를 ANN으로 구현한다.
```



#### 2.2.2 정책 경사 강화학습

```
정책 경사(policy gradient)는 분류(classification)문제를 푸는 강화학습으로 볼 수 있다.
어떠한 상태에서 어떠한 행동을 결정하는 것이 가장 좋을지를 판단하는 것이다.
정책 경사도 어떠한 상태에서의 행동에 대한 확률 값들을 ANN으로 구현한다.
```



#### 2.3 강화학습 적용 사례

```
벽돌 깨기, 알파고
* 벽돌 깨기
구글 딥마인드에서 2013년도 발표한 <Playing Atari with Deep Reinforcement Learning>이라는 논문에서 DQN(Deep Q network)을 처음 공개
* 알파고
2016년 3월 이세돌과의 대국으로 5전 4승 1패를 기록
```

#### 2.3.1 벽돌 깨기

#### 2.3.2 알파고

#### 2.4 이번 장의 요점



### 03장 . 배경 이론 3 : 강화학습을 이용한 주식투자란?

#### 3.1 직관적으로 강화학습 전략 알아보기

```
강화학습으로 무작정 주식 투자했을 때의 일일히 학습 데이터를 만드는 수고를 없애면서 효과적으로 할 수 있는 전략을 구상해보자.
```

#### 3.1.1 강화학습을 이용한 주식투자 구조

```
주식투자도 어떠한 환경에서 매수(buy), 매도(sell), 관망(hold) 등을 판단하는 문제로서 강화학습을 적용할 수 있다.
에이전트 : 투자자
환경 : 차트 데이터
행동 : 매수, 매도 , 관망
보상 : 손익
```



#### 3.1.2 차트 데이터 이해하기

```
1. 차트 데이터는 어떠한 종목의 시가, 고가, 저가, 종가, 거래량등의 있는 그대로의 데이터이다.
2. 차트 데이터로 투자 손익을 계산한다.
3. 학습 데이터는 모델을 학습할 목적으로 가공한 데이터이다.
4. 주식의 차트는 주로 봉 차트(캔들 차트)로 그린다.
5. 한국의 봉 차트에서는 양봉을 빨간색, 음봉을 파란색으로 그린다. 미국에서는 주로 양봉을 초록색, 음봉을 빨간색으로 표시하니 혼동하지 말것!
6. 양봉에서 굵은 부분의 아래쪽이 시가로 봉의 기간 중에서 가장 처음 거래된 가격이다. 위 꼬리 부분은 고가로 봉의 기간 동안 거래된 가격 중에 가장 높은 가격을 의미한다. 아래 꼬리는 저가로 봉의 기간 동안 가장 낮게 거래된 가격을 의미한다. 굵은 부분의 위쪽은 종가로 봉의 기간 중에서 가장 마지막에 거래된 가격이다.
7. 음봉은 굵은 부분의 위쪽이 시가로 봉의 기간 중에서 가장 처음 거래된 가격이고, 굵은 부분의 아래쪽은 종가로 봉의 기간 중에서 가장 마지막에 거래된 가격이다.
```



#### 3.1.3 차트 데이터를 바탕으로 강화학습을 하는 방식

```
초기 자본으로 1000만원이 있고, 거래에 대한 수수료와 세금은 고려하지 않고 1주씩만 매수하거나 매도한다고 제한한다. 그리고 2%의 손익이 발생 했을 때, 보상을 준다. 
```

```
누적 손익률 = 현 시점 평가금/시작 평가금

이전 보상지점 대비, 누적 손익률이 2%이상 수익이 발생한 지점에서 보상 +1

이전 보상지점 대비, 누적 손익률이 2%이상 손실이 발생한 지점에서 보상 -1

이전에 받은 보상을 기반으로 매수와 매도를 결정
```

* 학습을 반복하더라도 어느지점에서 더이상 보상이 +1또는 -1되는 지점이 일정할 경우, ''더이상 학습이 되지않음' '을 알 수 있음.



#### 3.1.4 거래 수수료와 거래세

```
실제 투자에서는 거래 수수료와 거래세를 비용으로 고려해야 함.
매수 수수료 : 0.015% (매수 시 발생하는 증권사가 취하는 금액)
매도 수수료 : 0.015% (매도 시 발생하는 증권사가 취하는 금액)
거래세 : 0.3% (매도 시 발생하는 국가가 취하는 금액)
```

* 거래를 적게 하는 것이 비용을 줄이는 길

#### 3.1.5 무작위 행동 결정(탐험)과 무작위 행동 결정 비율(엡실론)

```
실제로는 사람이 추적할 수 없을 정도의 수많은 반복과 방대한 데이터를 사용하고 일부 지점에서 무작위 투자를 진행하여, 학습 정체를 없앤다.
무작위로 행동하는 것을 탐험(exploration)이라고 하고,
무작위로 행동을 결정하는 비율을 엡실론(epsilon) 으로 표시한다.
```



#### 3.2 강화학습 효과를 차별화하는 요인들

```
학습 데이터, 보상 규칙, 행동의 종류, 정책 신경망, 강화학습 기법
```



#### 3.2.1 차별화 요인 1 : 학습 데이터 구성

```
당일 주가 및 거래량의 이전 주가 및 거래량 대비 비율 등등
```

* 전일 종가 대비 당일 시가 비율 ( open / last close )
* 당일 종가 대비 당일 고가 비율 ( high / close )
* 당일 종가 대비 당일 저가 비율( low / close )
* 당일 종가 대비 전일 종가 비율( close / last close )
* 전일 거래량 대비 당일 거래량 비율 ( volume / last volume )
* 5일 평균 종가 대비 당일 종가 비율 ( close / MA5 close )
* 10일 평균 종가 대비 당일 종가 비율 ( close / MA10 close )
* 20일 평균 종가 대비 당일 종가 비율 ( close / MA20 close )
* 60일 평균 종가 대비 당일 종가 비율 ( close / MA60 close )
* 120일 평균 종가 대비 당일 종가 비율 ( close / MA120 close )
* 5일 평균 거래량 대비 당일 거래량 비율 ( volume / MA5 volume )
* 10일 평균 거래량 대비 당일 거래량 비율( volume / MA10 volume )
* 20일 평균 거래량 대비 당일 거래량 비율 ( volume / MA20 volume )
* 60일 평균 거래량 대비 당일 거래량 비율 ( volume / MA60 volume )
* 120일 평균 거래량 대비 당일 거래량 비율 ( volume / MA120 volume )
* 주식 보유 비율 ( 현재 보유 주식 수 / 최대 보유 가능 주식 수 ) - 높으면, 매도 관점 / 낮으면, 매수 관점
* 포트폴리오 가치 비율 ( 현재 포트폴리오 가치 / 기준 포트폴리오 가치 ) - 정하기 나름
* 에이전트가 연속으로 매수한 횟수
* 에이전트가 연속으로 매도한 횟수
* 20 거래일 동안의 매수 비율
* 20 거래일 동안의 매도 비율
* 이외의 더 많은 학습 데이터

#### 3.2.2 차별화 요인 2 : 보상 규칙

손익률 1%, 2%, 5%, 10% 등으로 정할 수 있음.

이익률을 5% 초과하거나 손실률을 3% 초과할 때 학습을 진행하게 할 수도 있음. 

#### 3.2.3 차별화 요인 3 : 행동 종류

```
매수, 매도, 관망
```

공격적 매수, 방어적 매수, 공격적 매도, 방어적 매도, 관망

매매할 최대 및 최소 투자 단위를 결정해 놓고 확률에 따라 최소에서 최대 사이의 단위로 투자를 진행할 수 있음. 

* 확률과 투자금액 비례

#### 3.2.4 차별화 요인 4 : 정책 신경망

정책 신경망 - 에이전트가 행동을 결정하기 위한 두뇌 역할을 하는 인공 신경망(Artificial Neural Network)

기본적으로 CNN과 RNN을 혼용해서 씀

```
RNN : 현재 상태와 이전에 결정한 행동을 함께 고려함. 매수와 매도의 정신없는 투자를 피하고 투자 일관성을 가질 수 있음.

CNN : 5일, 10일 등 특정 기간 동안의 일련의 상태를 한꺼번에 고려할 수 있음(feature extraction)

RNN+CNN : 절대 다수결로 결과를 결정할 수 있음. 이러한 방법을 앙상블이라고 하고, 비슷한 방법으로 배깅(bagging)과 부스팅(boosting)등의 방법이 있음.
```

#### 3.2.5 차별화 요인 5 : 강화학습 기법인 Q러닝과 정책 경사

```
Q러닝 : 특정 행동을 취했을 때 예측되는 포트폴리오 가치를 회귀(regression)하고자 사용함.
예측한 값이 높은 쪽으로 행동을 취하면 됌. 즉, Q러닝을 사용할 경우, 행동들마다 기대 손익을 수치적으로 예측함.

정책 경사(policy gradient) : 어떤 행동이 현재 상태에서 가장 좋을지를 확률적으로 판단함.
Q러닝과 다르게 기대 손익을 예측하는 것이 아니라, 단순히 현 상황에서 어떤 행동이 더 좋은지를 판단함.
```

* 주식 투자 시, 일반적으로 매수/매도 했을 때, 수익률을 수치적으로 예측하기 보다는 현재 상태에서 매수해야 좋을지 아니면 매도해야 좋을지를 판단하는 편이 더 쉽고 신뢰성이 높다고 생각하여 여러 강화 기법 중에 정책 경사를 주식투자에 적용함. 
* 여러 강화학습 기법을 한꺼번에 사용할 수도 있음!

#### 3.3 차트 데이터와 학습데이터 살펴보기

차트 데이터 : 있는 그대로의 데이터(Raw data)

학습 데이터 : 전처리한 데이터(Pre-precessed data), 학습에 사용할 데이터

#### 3.3.1 차트 데이터

차트 데이터는 2차원 데이터로 체결일(date), 시가(open), 고가(high), 저가(low), 종가(close), 거래량(volume)이 연속되는 (idx 0 .... Idx N)식으로 구성된 데이터

* 차트데이터를 60분봉, 20분봉, 5분봉, 1분봉 등으로 구성할 수도 있음.

#### 3.3.2 학습 데이터

모델에 input할 수 있도록 전처리하여 준비 해야함.

```
1. 5일, 10일, 20일, 60일, 120일 평균 종가와 평균 거래량을 구함
2. 5일 평균 데이터는 인덱스 4부터, 10일 평균 데이터는 인덱스 9부터, 20일 평균 데이터는 인덱스 19부터, 60일 평균 데이터는 인덱스 59부터, 120일 평균 데이터는 인덱스 119부터 계산이 가능함.
3. 주가와 거래량 단위가 '원'과 '주'로서 서로 다르기 때문에 비율 값으로 구성함.
```

#### 3.4 주식투자 강화학습 절차

강화학습은 환경, 에이전트, 정책 신경망이 서로 상호 작용하면서 학습을 수행하기 때문에 그 과정이 일반적인 머신러닝에 비해 복잡함.

#### 3.4.1 주식투자 강화학습 순서도

```flow
start=>start: 시작
end=>end: 끝
o1=>operation: 탐험(exploration) 결정
o2=>operation: 무작위로 행동 결정
o3=>operation: 정책신경망으로 행동 결정
o4=>operation: 결정된 행동 수행
o5=>operation: 배치 학습데이터 생성
o6=>operation: 정책신경망 업데이트
o7=>operation: 환경 초기화
o8=>operation: 학습 결과 확인
c1=>condition: 탐험?
c2=>condition: 지연보상 발생?
c3=>condition: epoch 종료?
c4=>condition: 강화학습 종료?

start->o1->c1
c1(no)->o3->o4
c1(yes)->o2->o4
o4->c2
c2(yes)->o5->o6->c3
c2(no)->c3
c3(yes)->c4
c3(no)->o1
c4(yes)->o8->end
c4(no)->o7->o1



```



#### 3.4.2 행동 결정

```
한 epoch에서 경험을 얻기 위해서 무작위로 행동을 해봐야 하는데, 이를 탐험(exploration)이라고 한다.

일반적으로 강화학습 초반에는 탐험을 많이 하고, 후반으로 갈수록 탐험을 적게 해 나간다.

탐험 비율을 엡실론(epsilon)이라고 한다. 

1만 번 반복한다고 했을 때, 1 epoch에서는 앱실론을 30%로 정하고, 점점 엡실론을 줄여서 1만 번째 epoch에서는 0%가 되게 하면, epoch이 커질수록 무작위로 하는 행동이 줄어들게 된다.
```

* 무작위로 행동하지 않을 때는 정책 신경망으로 행동을 결정한다.

#### 3.4.3 결정된 행동 수행

```
무작위든, 정책 신경망으로 결정한 행동이든 에이전트는 결정된 행동을 수행한다.
```

* 매수의 경우, 에이전트는 주식을 사들일 현금이 있는지 확인하고, 매수가 가능할 경우 매수를 수행하고 그렇지 않으면 관망한다.
* 매수했을 경우, 매수금만큼 현금을 줄이고 매수한 주식 수 만큼 보유 주식 수를 늘려준다.
* 매도의 경우, 에이전트는 보유한 주식이 있는지 확인하고 보유한 주식이 있을 경우 매도를 수행하고 그렇지 않으면 관망한다. 
* 매도했을 경우, 매도한 주식 수만큼 보유 주식 수에서 빼주고 매도금만큼 현금보유액에 더해준다.

#### 3.4.4 배치 학습 데이터 생성 및 정책 신경망 업데이트

주식 투자를 해 나가면서 지연 보상을 줄 수 있을지 판단한다.

예를 들어, 2% 이상의 이익 또는 손실을 지연 보상 기준으로 정해보자.

```
즉, 투자를 하다가 2% 이상의 이익이나 2% 이상의 손실이 발생하면 이때까지 상황과 행동들을 학습 데이터로서 생성한다.
이 학습 데이터들을 한꺼번에 적용하여, 정책 신경망을 업데이트한다.

이런 학습 방법을 배치(batch) 학습이라고 부른다.

* 학습을 진행하고 나면, 정책 신경망의 가중치들이 업데이트되어 이후에 진행되는 투자부터 바로 업데이트된 정책 신경망의 결과가 반영된다.
```

* 지연 보상 기준을 낮게 잡으면 작은 배치 학습 데이터로 자주 학습을 수행할 가능성이 높으며, 지연 보상 기준이 높으면 큰 배치 학습 데이터로 덜 자주 학습을 진행할 가능성이 높다.



#### 3.5 주식투자 강화학습 과정 및 결과 확인 방법

* 주식투자 강화학습을 진행하면, 정해진 환경에서 매 순간마다 무작위로 행동을 결정하거나 정책 신경망으로 행동을 결정한 다음에 에이전트는 결정된 행동을 수행하고 그 결과로 에이전트의 상태가 변경된다.

#### 3.5.1 강화학습 과정 확인의 필요성

```
1. 정책 신경망이 출력 값이 어떤지
2. 에이전트 상태 변화 추세
```

* 학습이 진행되어 가는데, 에이전트가 매수만 하거나 매도만 할 경우 -> 정책 신경망의 학습이 제대로 되지 않았기 때문.



#### 3.5.2 강화학습 과정을 로그로 남기기

* 어떠한 과정을 텍스트로 남기는 것 : 로그(log)

```
*로그의 종류

1. 하이퍼 파라미터 : learning rate, discount factor, 최소/최대 투자단위(trading unit), 지연보상 임계치(delayed reward threshold)

2. epoch 결과 : 탐험률, 탐험 횟수, 매수 횟수, 매도 횟수, 관망 횟수, 보유 주식 수, 포트폴리오 가치, 긍정적 학습 횟수, 부정적 학습 횟수, 정책신경망 loss값

3. 최종 학습 결과 : 최대 포트폴리오 가치, 수익 발생 횟수, 최종 통계치 , etc
```



#### 3.5.3 강화학습 과정을 이미지로 가시화하기

* 각 epoch마다 보유 주식 수, 행동, 모델의 출력 값, 탐험, 수익과 손실을 가시화

```
1. 주식 종목 일봉 차트 : 강화학습의 환경(environment)
2. 에이전트가 수행한 행동과 보유 주식 수 차트 : 실선 - 보유 주식 수 /  배경 색 - 매수(빨강) , 매도(파랑)
3. 신경망 출력 값 차트 : 점 - 매수 확률(빨강) , 매도 확률(파랑) / 배경 색 - 매수(빨강), 매도(파랑), 탐험(노랑)
4. 포트폴리오 가치 차트 : 실선 - 포트폴리오 가치 / 배경 색(세로 줄) : 긍정 보상(빨강), 부정 보상(파랑) / 배경 색(기준 선과 그래프사이의 넓이) : 수익(빨강), 손실(파랑)
```

* 학습 평가의 질 => 가시화

#### 3.6 이번 장의 요점



### 04장 모듈 개발 : 강화학습 기반 주식투자 시스템 개발

Reinforcement Learning Trader 의 소스코드 개발

#### 4.1 RLTrader 개발에 필요한 환경

아나콘다, Numpy, Pandas, Matplotlib, Tensorflow, Keras

#### 4.1.1 아나콘다 설치

#### 4.1.2 텐서플로와 케라스 설치

#### 4.2 RLTrader의 구조

정책 학습기, 가시화기, 에이전트, 환경, 정책 신경망

#### 4.2.1 모듈 구조

```sequence
가시화기(visualizer.py)->정책학습기(policy_learner.py): 
정책학습기(policy_learner.py)->에이전트(agent.py): 
정책학습기(policy_learner.py)->환경(environment.py): 
정책학습기(policy_learner.py)->정책 신경망(policy_network.py): 
```



#### 4.2.2 디렉터리 구조

```
.rltrader
├── chart_data/
├── logs/
├── epochs_summary/
├── models/
├── environment.py
├── agent.py
├── policy_network.py
├── visualizer.py
└── policy_learner.py
```



#### 4.2.3 에이전트 모듈 개요

```
agent.py : 주식을 매도하거나 매수하는 투자자 역할을 함.
초기 자본금, 현금 잔고, 주식 잔고라는 상태가 있음.

* 포트폴리오 가치(Portfolio value) : 현금 잔고와 주식 잔고의 평가액을 합한 금액 , 줄여서 PV라고 부름.
```

* 투자의 목표는 PV를 높여나가는 것

#### 4.2.4 환경 모듈 개요

```
environment.py : Environment 클래스가 있음.
에이전트가 투자할 종목의 차트 데이터를 관리함. 
환경 클래스에 전체 차트 데이터가 있지만, 과거 시점부터 가장 최근 시점까지 순차적으로 데이터를 제공함.
즉, 과거로 돌아간 에이전트가 미래의 차트 데이터는 알 수 없다.
```



#### 4.2.5 정책 신경망 모듈 개요

```
policy_network.py : PolicyNetwork 클래스가 있음.
특정 시점의 주식 데이터(sample)이 제공되었을 때 매수할지, 매도할지를 결정하는 에이전트의 뇌와 같은 역할을 함.
LSTM 구조로 되어있고, 매수와 매도 행위에 대해서 PV를 높일 수 있을지의 확률을 계산함.
```

* 주식 잔고 多, 주가 $$\uparrow$$ $$\Rightarrow$$ PV 큰 폭 $$\uparrow$$ 
* 주가 상승 예상 $$\Rightarrow$$ 에이전트의 행위 : 주식 잔고 $$\uparrow​$$
* 주식 잔고 多, 주가 $$\downarrow$$ $$\Rightarrow $$ PV 큰 폭 $$\downarrow$$ 
* 주가 하락 예상 $$\Rightarrow$$ 에이전트의 행위 : 주식 잔고 $$\downarrow$$ 
* 정책 신경망의 출력은 매수와 매도에 대한 확률이 나옴 $$\Rightarrow$$ 매수 if 매수P > 매도P else 매도

#### 4.2.6 가시화기 모듈 개요

```
visualizer.py : 환경, 에이전트 상태, 정책 신경망 출력을 가시화

* 충분한 epoch이 지났음에도 PV가 높아지지 않는다면, 학습이 제대로 이루어 지지 않음을 파악하고 알고리즘 개선
```



#### 4.2.7 정책 학습기 모듈 개요

```
policy_learner.py : PolicyLearner 클래스가 있음.
RLTrader의 몸체가 되고, 학습 데이터를 가지고 있고, 보상이 결정되었을 때 학습데이터로 정책 신경망을 학습시킴.
```



#### 4.3 환경 모듈 개발

* environment.py

#### 4.3.1 환경 모듈의 주요 속성과 함수

```
environment.py 는 투자할 종목의 차트 데이터를 관리하는 작은 모듈.

* 속성
1. chart_data : 주식 종목의 차트 데이터
2. observation : 현재 관측치
3. idx : 차트 데이터에서의 현재 위치
* 함수
1. reset() : idx와 observation을 초기화
2. observe() : idx를 다음 위치로 이동하고 observation을 업데이트
3. get_price() : 현재 observation에서 종가를 획득
```



#### 4.3.2 코드 조각 : 환경 클래스의 전체 소스코드

* environment.py

```python
class Environment : 
    PRICE_IDX = 4 # 종가의 위치
    
    def __init__(self,chart_data=None):
        self.chart_data = chart_data
        self.observation = None
        self.idx = -1
    
    def reset(self):
        self.observation = None
        self.idx = -1
        
    def observe(self):
        if len(self.chart_data) > self.idx + 1:
            self.idx +=1
            self.observation = self.chart_data.iloc[self.idx]
            return self.observation
        return None
    
    def get_price(self):
        if self.observation is not None:
            return self.observation[self.PRICE_IDX]
        return None
```

- iloc() 함수는 DataFrame 함수로 특정 행의 데이터를 가져옴.

#### 4.4 에이전트 모듈 개발

* agent.py

#### 4.4.1 에이전트 모듈의 주요 속성과 함수

```
agent.py 는 투자 행동을 수행하고 투자금과 보유 주식을 관리하기 위한 에이전트 클래스(Agent)를 가짐.

* 속성
1. initial_balance : 초기 투자금
2. balance : 현금 잔고
3. num_stocks : 보유 주식 수
4. portfolio_value : 포트폴리오 가치(투자금 잔고+주식 현재가*보유 주식 수)

* 함수
1. reset() : 에이전트의 상태를 초기화
2. set_balance() : 초기 자본금을 설정
3. get_states() : 에이전트의 상태를 획득
4. decide_action() : 탐험 또는 정책 신경망에 의한 행동 결정
5. validate_action() : 행동의 유효성 판단
6. decide_trading_unit() : 매수 또는 매도할 주식 수 결정
7. act() : 행동 수행
```



#### 4.4.2 코드 조각 1 : 에이전트 클래스의 상수 선언 부분

* agent.py의 상수 선언 부분

```python
import numpy as np

class Agent:
    #에이전트 상태가 구성하는 값 개수
    STATE_DIM = 2 #주식 보유 비율, 포트폴리오 가치 비율
    
    #매매 수수료 및 세금
    TRADING_CHARGE = 0 #거래 수수료 미고려 (일반적으로 0.015%)
    TRADING_TAX = 0 # 거래세 미고려(실제 0.3%)
    
    #행동
    ACTION_BUY = 0 #매수
    ACTION_SELL = 1 #매도
    ACTION_HOLD = 2 #관망
    ACTIONS = [ ACTION_BUY, ACTION_SELL ] #인공 신경망에서 확률을 구할 행동들
    NUM_ACTIONS = len(ACTIONS) #인공 신경망에서 고려할 출력 값의 개수
```

* 매수와 매도 중에서 결정한 행동을 할 수 없을 때만 관망 행동을 함.

#### 4.4.3 코드 조각 2 : 에이전트 클래스의 생성자 부분

* agent.py의 클래스 생성자 부분

```python
    def __init__(
        self, environment, min_trading_unit = 1, max_trading_unit = 2, delayed_reward_threshold= .05):
        # Environment 객체
        self.environment = environment # 현재 주식 가격을 가져오기 위해 환경 참조
        
        # 최소 매매단위 , 최대 매매단위, 지연 보상 임계치
        self.min_trading_unit = min_trading_unit # 최소 단일 거래 단위
        self.max_trading_unit = max_trading_unit # 최대 단일 거래 단위
        self.delayed_reward_threshold = delayed_reward_threshold # 지연 보상 임계치
        
        # Agent 클래스의 속성
        self.balance = 0 # 현재 현금 잔고
        self.num_stocks = 0 # 보유 주식 수
        self.portfolio_value = 0 # balance + num_stocks * {현재 주식 가격}
        self.base_portfolio_value = 0 # 직전 학습 시점의 PV
        self.num_buy = 0 # 매수 횟수
        self.num_sell = 0 # 매도 횟수
        self.num_hold = 0 # 관망 횟수
        self.immediate_reward = 0 # 즉시 보상 
        
        # Agent 클래스의 상태
        self.ratio_hold = 0 # 주식 보유 비율
        self.ratio_portfolio_value = 0 # 포트폴리오 가치 비율
    
```

* immediate_reward : 즉시 보상 - 행동을 수행한 시점에서 수익이 발생한 상태면 1을, 아니었으면 -1을 줌.

* ratio_hold : 주식 보율 비율 - 최대로 보유할 수 있는 주식 수 대비 현재 보유하고 있는 주식 수의 비율
* ratio_portfolio_value : 직전 지연 보상이 발생했을 때의 포트폴리오 가치 대비 현재의 포트폴리오 가치의 비율

#### 4.4.4 코드 조각 3 : 에이전트 클래스의 함수 부분

* agent.py의 함수 부분

```python
	def reset(self):
        self.balance = self.initial_balance
        self.num_stocks = 0
        self.portfolio_value = self.initial_balance
        self.base_portfolio_value = self.initial_balance
        self.num_buy = 0
        self.num_sell = 0
        self.num_hold = 0
        self.num_immediate_reward = 0
        self.ratio_hold = 0
        self.ratio_portfolio_value = 0
        
    def set_balance(self, balance):
        self.initial_balance = balance
        
    def get_states(self):
        self.ratio_hold = self.num_stocks / int(self.portfolio_value / self.environment.get_price())
        self.ratio_portfolio_value = self.portfolio_value / self.initial_balance
        return (self.ratio_hold, self.ratio_portfolio_value)
    def decide_action(self, policy_network, sample, epsilon):
        confidence = 0.
        # 탐험 결정
        if np.random.rand() < epsilon:
            exploration = True
            action = np.random.randint(self.NUM_ACTIONS) #무작위로 행동 결정
        else :
            exploration = False
            probs = policy_network.predict(sample) # 각 행동에 대한 확률
            action = np.argmax(probs)
            confidence = probs[action]
        return action, confidence , exploration
    
    def validate_action(self, action):
        validity = True
        if action == Agent.ACTION_BUY:
			#적어도 1주를 살 수 있는지 확인
            if self.balance < self.environment.get_price() * (1+self.TRADING_CHARGE)*self.min_trading_unit:
                validity = False
        elif action == Agent.ACTION_SELL:
            # 주식 잔고가 있는지 확인
            if self.num_stocks <=0:
                validity = False
        return validity
    
    def decide_trading_unit(self, confidence):
        if np.isnan(confidence):
            return self.min_trading_unit
        added_trading = max(min(int(confidence * (self.max_trading_unit - self.min_trading_unit)),self.max_trading_unit - self.min_trading_unit),0)
        return self.min_trading_unit + added_trading
    
    
    def act(self, action, confidence):
        if not self.validate_action(action):
            action = Agent.ACTION_HOLD
            
        #환경에서 현재 가격 얻기
        curr_price = self.environment.get_price()
        
        #즉시 보상 초기화
        self.immediate_reward = 0
        
        #매수
        if action == Agent.ACTION_BUY :
            #매수할 단위를 판단
            trading_unit = self.decide_trading_unit(confidence)
            balance = self.balance - curr_price * (1 +self.TRADING_CHARGE) * trading_unit
            # 보유 현금이 모자랄 경우, 보유 현금으로 가능한 만큼 최대한 매수
            if balance < 0 :
                trading_unit = max(
                    min(
                    int(self.balance / (curr_price * (1 +self.TRADING_CHARGE))),
                    self.max_trading_unit
                    ),
                    self.min_trading_unit
                )
            #수수료를 적용하여 총 매수 금액 산정
            invest_amount = curr_price * (1 + self.TRADING_CHARGE) * trading_unit
            self.balance -= invest_amount # 보유 현금을 갱신
            self.num_stocks +=trading_unit # 보유 주식 수를 갱신
            self.num_buy += 1 # 매수 횟수 증가
            
        # 매도
        elif action == Agent.ACTION_SELL :
            # 매도할 단위를 판단
            trading_unit = self.decide_trading_unit(confidence)
            # 보유 주식이 모자랄 경우 가능한 만큼 최대한 매도
            trading_unit = min(trading_unit, self.num_stocks)
            # 매도
            
            
```

* 주식 보유 비율 (ratio_hold) : 보유 주식 수 / ( 포트폴리오 가치 / 현재 주가 ) 
* ratio_hold가 0이면, 주식은 하나도 보유하지 않은 것, 0.5면 최대 가질 수 있는 주식 대비 절반의 주식을 보유하고 있는 것
* action 은 매수와 매도를 의미하는 0또는 1의 값
* confidence 는 정책 신경망을 통해 결정한 경우 결정한 행동에 대한 소프트맥스 확률 값
* validate_action(action)으로 행동 유효성을 검사하고 할 수 없는 경우, 아무 행동도 하지 않도록 관망(hold)한다.

#### 4.5 정책 신경망 모듈 개발



#### 4.5.1 정책 신경망 모듈의 주요 속성과 함수

#### 4.5.2 정책 신경망에서 사용하는 LSTM 신경망의 구조

#### 4.5.3 코드 조각 1 : 정책 신경망 클래스의 생성자 부분

#### 4.5.4 코드 조각 2 : 정책 신경망 클래스의 함수 선언 부분

#### 4.6 가시화기 모듈 개발

#### 4.6.1 가시화기 모듈의 주요 속성과 함수

#### 4.6.2 가시화기 모듈이 만들어내는 정보

#### 4.6.3 코드 조각 1 : 가시화기 클래스의 생성자 부분

#### 4.6.4 코드 조각 2 : 일봉 차트 가시화 함수 부분

#### 4.6.5 코드 조각 3 : 전체 차트 가시화 함수 선언 부분

#### 4.6.6 코드 조각 4 : 에이전트 상태 가시화 부분

#### 4.6.7 코드 조각 5 : 정책 신경망 출력 결과 및 탐험 수행 가시화 부분

#### 4.6.8 코드 조각 6 : 포트폴리오 가치 및 기타 정보 가시화 부분

#### 4.6.9 코드 조각 7 : 차트 초기화 및 저장 함수 부분

#### 4.7 정책 학습기 모듈 개발

#### 4.7.1 코드 조각 1 : 정책 학습기 모듈의 의존성 임포트 부분

#### 4.7.2 코드 조각 2 : 정책 학습기 클래스의 생성자 부분

#### 4.7.3 코드 조각 3 : 에포크 초기화 함수 부분

#### 4.7.4 코드 조각 4 : 학습 함수 선언 부분

#### 4.7.5 코드 조각 5 : 학습 함수 초반 부분

#### 4.7.6 코드 조각 6 : 학습 함수의 로컬 변수 초기화 부분

#### 4.7.7 코드 조각 7 : 학습 함수의 연관 객체 초기화 및 탐험 비율 설정 부분

#### 4.7.8 코드 조각 8 : 학습 함수의 에포크 구행 while 문 초반부

#### 4.7.9 코드 조각 9 : 학습 함수의 행동과 그 결과를 저장하는 부분

#### 4.7.10 코드 조각 10 : 학습 함수의 반복 정보 갱신 부분

#### 4.7.11 코드 조각 11 : 학습 함수의 정책 신경망 학습 부분

#### 4.7.12 코드 조각 12 : 에포크 결과 가시화 부분

#### 4.7.13 코드 조각 13 : 에포크 결과 로그 기록 부분

#### 4.7.14 코드 조각 14 : 학습 통계 정보 갱신 부분

#### 4.7.15 코드 조각 15 : 최종 학습 결과 통계 정보 로그 기록 부분

#### 4.7.16 코드 조각 16 : 미니 배치 데이터 생성 함수 부분

#### 4.7.17 코드 조각 17 : 학습 데이터 샘플 생성 부분

#### 4.7.18 코드 조각 18 : 투자 시뮬레이션을 하는 trade()함수 부분

### 05장 데이터 준비 : 주식 데이터 획득

#### 5.1 방법 1. 증권사 HTS 사용

#### 5.1.1 증권사 HTS 다운로드

#### 5.1.2 증권 계좌 개설

#### 5.1.3 종목 차트 데이터 확인

#### 5.1.4 일별 데이터 엑셀 파일 저장

#### 5.2 방법 2. 증권사 API 사용

#### 5.2.1 증권사 API 설치

#### 5.2.2 대신증권 크레온 API 사용 환경 준비

#### 5.2.3 대신증권 크레온 HTS 실행

#### 5.2.4 대신증권 크레온 API를 이용한 차트 데이터 획득 프로그램 작성

#### 5.3 방법 3. 포털 사이트 사용

#### 5.3.1 pandas-datareader, fix_yahoo_finance 설치하기

#### 5.3.2 Google Finance에서 주식 데이터 획득하기

#### 5.3.3 Yahoo Finance에서 주식 데이터 획득하기

#### 5.4 이번 장의 요점



### 06장 모델 구축 : 투자 시뮬레이션

#### 6.1 주식 데이터 전처리

#### 6.1.1 코드 조각 1 : CSV 파일을 읽는 부분

#### 6.1.2 코드 조각 2 : 종가와 거래량의 이동 평균 구하기

#### 6.1.3 코드 조각 3 : 주가와 거래량의 비율 구하기

#### 6.1.4 코드 조각 4 : 주가와 거래량의 이동 평균 비율 구하기

#### 6.2 주식 데이터 학습

#### 6.2.1 코드 조각 1 : 강화학습을 실행하는 메인(main)모듈

#### 6.2.2 코드 조각 2 : 강화학습에 필요한 주식 데이터 준비 부분

#### 6.2.3 코드 조각 3 : 데이터를 차트 데이터와 학습 데이터로 분류하는 부분

#### 6.2.4 코드 조각 4 : 강화학습을 시작하는 부분

#### 6.3 학습 과정 및 결과 확인

#### 6.3.1 콘솔에 출력되는 로그의 의미

#### 6.3.2 가시화 결과가 저장되는 그림 파일

#### 6.4 이번 장의 요점



### 07장 모델 검증 : 투자 시뮬레이션

#### 7.1 투자 시뮬레이션 결과 1 : 삼성전자(005930)

#### 7.1.1 종목의 개요

#### 7.1.2 주식 데이터 전처리

#### 7.1.3 학습 파라미터 설정

#### 7.1.4 에포크 10일 때의 결과

#### 7.1.5 에포크 200일 때의 결과

#### 7.1.6 에포크 600일 때의 결과

#### 7.1.7 에포크 1000일 때의 결과

#### 7.1.8 총평

#### 7.2 투자 시뮬레이션 결과 2 : SK하이닉스(000660)

#### 7.2.1 종목의 개요

#### 7.2.2 주식 데이터 전처리

#### 7.2.3 학습 파라미터 설정

#### 7.2.4 에포크 10일 때의 결과

#### 7.2.5 에포크 200일 때의 결과

#### 7.2.6 에포크 600일 때의 결과

#### 7.2.7 에포크 1000일 때의 결과

#### 7.2.8 총평

#### 7.3 투자 시뮬레이션 결과 3 : 현대차(005380)

#### 7.3.1 종목의 개요

#### 7.3.2 주식 데이터 전처리

#### 7.3.3 학습 파라미터 설정

#### 7.3.4 에포크 10일 때의 결과

#### 7.3.5 에포크 200일 때의 결과

#### 7.3.6 에포크 600일 때의 결과

#### 7.3.7 에포크 1000일 때의 결과

#### 7.3.8 총평

#### 7.4 투자 시뮬레이션 결과 4 : LG화학(051910)

#### 7.4.1 종목의 개요

#### 7.4.2 주식 데이터 전처리

#### 7.4.3 학습 파라미터 설정

#### 7.4.4 에포크 10일 때의 결과

#### 7.4.5 에포크 200일 때의 결과

#### 7.4.6 에포크 600일 때의 결과

#### 7.4.7 에포크 1000일 때의 결과

#### 7.4.8 총평

#### 7.5 투자 시뮬레이션 결과 5 : 네이버(035420)

#### 7.5.1 종목의 개요

#### 7.5.2 주식 데이터 전처리

#### 7.5.3 학습 파라미터 설정

#### 7.5.4 에포크 10일 때의 결과

#### 7.5.5 에포크 200일 때의 결과

#### 7.5.6 에포크 600일 때의 결과

#### 7.5.7 에포크 1000일 때의 결과

#### 7.5.8 총평

#### 7.6 투자 시뮬레이션 결과 6 : KT(030200)

#### 7.6.1 종목의 개요

#### 7.6.2 주식 데이터 전처리

#### 7.6.3 학습 파라미터 설정

#### 7.6.4 에포크 10일 때의 결과

#### 7.6.5 에포크 200일 때의 결과

#### 7.6.6 에포크 600일 때의 결과

#### 7.6.7 에포크 1000일 때의 결과

#### 7.6.8 총평

#### 7.7 투자 시뮬레이션 결과 정리 및 원숭이 투자와의 비교

#### 7.8 이번 장의 요점



#### 08장 모델 활용 : 학습된 정책 신경망 모델을 사용한 투자 시뮬레이션

#### 8.1 모델 학습과 모델 활용의 차이점

#### 8.1.1 시뮬레이션 과정 차이점

#### 8.1.2 소스코드의 차이점

#### 8.2 학습된 정책 신경망 모델을 사용한 투자 시뮬레이션

#### 8.2.1 학습된 모델 적용 1 : 삼성전자(005930)

#### 8.2.2 학습된 모델 적용 2 : SK하이닉스(000660)

#### 8.2.3 학습된 모델 적용 3 : 현대차(005380)

#### 8.2.4 학습된 모델 적용 4 : LG화학(051910)

#### 8.2.5 학습된 모델 적용 5 : NAVER(035420)

#### 8.2.6 학습된 모델 적용 6 : KT(030200)

#### 8.2.7 총평

#### 8.3 투자 시뮬레이션 결과 정리 및 원숭이 투자와의 비교

#### 8.4 이번 장의 요점









