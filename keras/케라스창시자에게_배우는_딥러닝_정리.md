## 케라스 창시자에게 배우는 딥러닝

### 1,2장

mnist 데이터셋은 넘파이 배열 형태로 케라스에 이미 포함되어 있음.불러 올때는, 

from keras.datasets import mnist

(train_images, train_labels) , (test_images, test_labels) = mnist.load_data()

로 불러온다.

모델은 간단히 

from keras import models

from keras import layers

net = models.Sequential()

net.add(layers.Dense(512, activation=’relu’, input_shape=(28 * 28,)))

net_add(layers.Dense(10, activation=’softmax’))

로 구현하고 

손실 함수, 옵티마이저, 모니터링 지표를 넣어 컴파일한다.

network.compile(optimizer=’rmsprop’,

​			loss = ‘categorical_crossentropy’,

​			metrics=[‘accuracy’])

train_images = train_images.reshape((60000,28 * 28))

train_images = train_images.astype(‘float32’) / 255

test_images = test_images.reshape((60000, 28 * 28))

test_images = test_images.astype(‘float32’) / 255

으로 데이터 형식을 바꿔준다.

from keras.utils import to_categorical

train_labels = to_categorical(train_labels)

test_labels = to_categorical(test_labels)

로 레이블을 바이너리 형식으로 변환한다.

그리고 net.fit(train_images, train_labels, epochs=5, batch_size = 128)

로 학습한다.

결과를 평가할 땐,

test_loss, test_acc = net.evaluate(test_images, test_labels)

해서 학습한 네트워크의 손실과 정확도를 평가할 수 있다.



1차원 배열을 벡터, 2차원 배열을 행렬, 3차원이상의 배열을 텐서라고 한다.

보는 법은 제일 뒤의 shape가 가장 안쪽의 배열을 구성하는 개수로 차례대로 바깥쪽의 배열을 구성하는 shape가 된다.

데이터 타입은 대개 float32, float64, int32등등을 사용한다. 

간혹 char을 사용하기도 한다.

딥러닝에서 사용하는 모든 데이터 텐서의 첫번 째 shape는 샘플 축이다. 그러므로 

batch = train_images[:128]

로 배치를 지정할 수 있고,

다음 배치는

batch = train_images[128:256] 이런식이 된다.

n 번째 배치는 

batch = train_images[128 * n : 128 * (n+1)]

으로 표현할 수 있다.

이미지 데이터 shape은 텐서플로의 경우, (batch_size, height, width, channel)로 구성된다.

theano는 (batch_size , channel, height, width)로 구성된다.

케라스는 백엔드(텐서플로, 씨아노) 를 어떤 걸로 쓰는지에 따라 다르다.

 ```python
def naive_relu(x):
	assert len(x.shape) ==2
	x = x.copy()  # 값 복사
	for i in range(x.shape[0]):
		for j in range(x.shape[1]):
			x[i , j] = max(x[i, j], 0)
	return x
 ```



브로드 캐스팅(중요하다)

브로드캐스팅은 2단계가 있다.

1. 큰 텐서의 차원에 맞도록 작은 텐서에 축이 추가된다.
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복된다.

브로드캐스팅의 예는 다음과 같다.

```python
import numpy as np

x = np.random.random((64, 3, 32, 10))
y = np.random.random((32, 10))
z = np.maximum(x, y)
```

\# 출력 텐서의 크기 ( 64, 3, 32, 10)

점곱 연산은 다음과 같이 나타낼 수 있다.

```python
import numpy as np

z = np.dot(x, y)
z = x * y
```

계산 하는 방식은

```python
def naive_vector_dot(x, y):
	assert len(x.shape) == 1
	assert len(y.shape) == 1
	assert x.shape[0] == y.shape[0]
	z = 0.
	for i in range(x.shape[0]):
		z+=x[i] * y[i]
	return z
```



두 벡터의 점곱은 스칼라가 되므로, 원소 개수가 같은 벡터끼리 점곱이 가능하다.

행렬 간의 점곱은

```python
def naive_vector_dot(x, y):
	assert len(x.shape) == 2
	assert len(y.shape) == 2
	assert x.shape[1] == y.shape[0]
	z = np.zeros((x.shape[0], y.shape[1]))
	for i in range(x.shape[0]):
		for j in range(y.shape[1]):
			row_x = x[i, :]
			column_y = y[:, j]
			z[i, j] = naive_vector_dot(row_x, column_y)
	return z
```



의 식으로 나타낼 수 있다.

affine 변환, 회전, 스케일링은 점곱을 통해 구현이 가능하다.



훈련 과정

1. 훈련 샘플 x와 타깃 y의 배치를 추출
2. x를 사용하여 네트워크를 실행(forward pass단계), 예측 y_pred를 구한다.
3. y_pred와 y_true의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산한다,
4. 배치에 대한 손실이 조금 감소되도록 네트워크의 모든 가중치를 업데이트 한다.

최적화 방법 중 SGD의 변종이 있는데,

모멘텀을 사용한 SGD, Adagrad, RMSProp등이 있다.

모멘텀은 SGD에 있는 2개의 문제점인 수렴 속도와 지역 최솟값을 해결한다.

지역 최솟값에 갇히지 않도록 과거의 가속도를 함께 고려하여 현재 단계의 최적화를 수행한다.

```
past_velocity = 0.

momentum = 0.1

while loss > 0.01:

​	w, loss, gradient = get_current_parameters()

​	velocity = momentum * past_velocity - learning_rate * gradient

​	w = w + momentum * velocity - learning_rate * gradient

​	past_velocity = velocity

​	update_parameter(w)
```



위는 단순한 구현의 예이다.



mnist 예제를 다시 살펴보면,

이미지를 reshape함수로 (60000, 784)의 크기로 변환하고,

float32타입으로 변환한다.

간단한 신경망을 구성하고, 마지막 레이어는 softmax로 확률값이 출력되도록 한다,

그리고 옵티마이저와 손실함수, 측정지표를 매개변수로 컴파일하고, 

에폭과 배치사이즈를 지정하여 image와 label을 인풋으로 학습한다,



학습은 훈련 데이터 샘플과 그에 상응하는 타깃이 주어졌을 때 손실 함수를 최소화 하는 모델 파라미터의 조합을 찾는 것을 의미한다. 

배치를 기준으로 손실을 구하고, 그에 상응하는 그래디언트로 학습한다. 

네트워크의 파라미터는 그래디언트의 반대 방향으로 조금씩 움직인다.

체인룰을 이용해 역전파를 쉽게 한다.

손실은 훈련하는 동안 최소화해야할 양이므로 해결하려는 문제의 성공을 측정하는 데 사용한다.

문제 해결 상황에 따라 다른 손실함수를 사용한다.

옵티마이저는 손실에 대한 그래디언트가 파라미터를 업데이트하는 정확한 방식을 정의한다.

 ### 3장

#### 3.1 신경망의 구조

네트워크를 구성하는 층
입력 데이터와 그에 상응하는 타깃
학습에 사용할 피드백 신호를 정의하는 손실 함수
학습 진행 방식을 결정하는 옵티마이저

#### 3.1.1 층: 딥러닝의 구성 단위

층 : 하나 이상의 텐서를 입력으로 받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈
완전 연결층(fully connected layer)과 밀집 층(dense layer)에 의해 처리되는 경우가 많다.
(samples, timestamps, features)의 3D 텐서는 주로 LSTM의 RNN으로 처리되는 경우가 많고, (samples, height, width, channels)와 같은 4D 텐서는 주로 CNN으로 처리 되는 경우가 많다.

 #### 3.1.2 모델: 층의 네트워크

모델 : 층을 순서대로 쌓은 비순환 유향 그래프
머신러닝 : 가능성 있는 공간을 사전에 정의하고 피드백 신호의 도움을 받아 입력 데이터에 대한 유용한 변환을 찾는 것
텐서 연산을 통해 모델의 가중치 텐서들이 적합한 가중치를 찾아가는 것이 목표.



#### 3.1.3 손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠

손실 함수 또는 목적함수 : 훈련하는 동안 최소화가 될 값
옵티마이저 : 손실 함수를 기반으로 네트워크가 어떻게 업데이트 될지 결정하는 방법
손실 함수를 제대로 선택해야만, 원하는 목적에 다다를 수 있는 모델이 완성될 수 있다. 
일반적으로, 2개의 클래스 분류 문제는 binary crossentropy, 다중 클래스 분류 문제에는 categorical crossentropy, 회귀 문제에는 평균제곱오차, 시퀀스 학습 문제에는 connection temporal classification등을 사용한다. 



#### 3.2 케라스 소개

https://keras.io
MIT 라이센스
2.7~3.6 파이썬과 호환가능



#### 3.2.1 케라스, 텐서플로, 씨아노, CNTK

백엔드 엔진으로 tensorflow, theano, CNTK 를 지원



#### 3.2.2 케라스를 사용한 개발: 빠르게 둘러보기

케라스에서 모델을 정의하는 방법 2가지
1. Sequential 클래스(층을 순서대로 쌓아올린 네트워크)
2. 함수형 API(완전히 임의의 구조를 만들 수 있는 비순환 유향그래프)
3. 

#### 3.3 딥러닝 컴퓨터 셋팅

윈도우와 CPU환경에서 케라스를 사용할 수 는 있지만 되도록 우분투와 GPU환경을 구축한 후 사용을 권장함.



#### 3.3.1 주피터 노트북: 딥러닝 실험을 위한 최적의 방법

주피터 노트북 : 긴 코드를 작게 쪼개 독립적으로 실행가능한 편집기
개인적인 경험으로, 튜토리얼을 익힐 때 로컬 컴퓨터에서 디버깅 속도가 빠르고 수정이 편리하기 때문에, 처음 접하는 도메인에서 기본 튜토리얼을 통해 실력을 향상시킬 때 도움을 많이 주는 툴이라고 생각합니다.



#### 3.3.2 케라스 시작하기: 두 가지 방법

1. aws EC2에서 주피터노트북으로 케라스를 사용할 수 있음
2. 로컬에서 환경 설정 후 사용



#### 3.3.3 클라우드에서 딥러닝 작업을 수행했을 때 장단점

딥러닝 파워 유저가 되기 위해서는 GPU는 필수



#### 3.3.4 어떤 GPU 카드가 딥러닝에 최적일까?

현재 최고 성능의 그래픽 카드는 Quadro rtx 6000이 압도적으로 우수한 성능을 보여주고, 
그 다음으로 rtx 2080 ti와 Titan V CEO edition이 나란히 2위의 성능을 보여주고 있음.
개인용 컴퓨터에서 장착 사용 가능한 가격대의 GPU는 rtx 2080 ti가 최고의 성능을 보여주고 있음.



#### 3.4 영화 리뷰 분류: 이진 분류 예제

#### 3.4.1 IMDB 데이터셋

IMDB 데이터셋 train data 2만 5000개, test data 2만 5000개로 나누어져 있음
50% 긍정, 50% 부정으로 구성

```python
from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
```

위의 방식으로 로드가 가능하다.

```python
train_data[0]
>>>[1, 14, 22, 16, ..., 178, 32]
train_labels[0]
>>>1
```

데이터는 단어 시퀀스가 인코딩 된 숫자 데이터가 리스트로 나열되어있고 레이블은 1과 0의 긍부정으로 구성되어 있다.
단어를 확인하고 싶다면, 디코딩하는 과정을 거쳐 검증하면 된다.



#### 3.4.2 데이터 준비

신경망에 숫자 리스트는 주입할 수 없다.
따라서 리스트를 텐서로 변환해야하는데 2가지 방법으로 할 수 있다.
1. 같은 길이가 되도록 리스트에 패딩을 추가하고 (samples, squence_length)크기의 정수 텐서로 변환한다. 그 다음 이 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용한다.
2. 리스트를 원-핫 인코딩하여 0과 1의 벡터로 변환한다.=>[3,5]의 위치는 1의 값, 나머지는 모두 0의 값을 가지는 n개 샘플차원의 벡터로 각각 변환하는 것. 그 다음 부동 소수 벡터 데이터를 다룰 수 있는 Dense층을 신경망의 첫 번째 층으로 사용한다.



#### 3.4.3 신경망 모델 만들기

입력데이터 : 벡터, 입력레이블 : 스칼라(0 또는 1)
이런 문제에는  Dense(16, activation='relu')을 쌓아올린 것
**중요 : 매개변수(16)은 은닉 유닛의 개수 -> 하나의 은닉 유닛은 층이 나타내는 표현 공간에서 하나의 차원이 된다.
16개의 은닉 유닛이 있다는 것은 가중치 행렬 w의 크기가 (input_dimension, 16)이라는 뜻이다.
입력 데이터와 w를 점곱하면 입력 데이터가 16차원으로 표현된 공간으로 투영된다. 표현 공간의 차원을 쉽게 생각하면, 학습할 때의 자유도 즉 파라미터라고 생각하면 된다. 데이터를 분류하는 작업이 쉬울 경우 비교적 적은 차원이 알맞고, 데이터가 크고 복잡하며 분류할 클래스가 많을 경우에는 많은 차원이 알맞다. 오버피팅에 유의하여 파라미터를 조정할 필요가 있다.
Dense를 쌓을 때 두가지를 유의할 것
1. 얼마나 많은 층을 쌓을 것인가?
2. 각 층에 얼마나 많은 은닉 유닛을 둘 것인가?
  loss function은 binary crossentropy가 적합하고, mean_squared_error도 사용가능하다.
  optimizer는 rmsprop을 사용하는데, 안정적이고 좋은 성능을 보이는 알고리즘이므로 Adam, NAG와 더불어 많이 사용하는 optimizer다.

#### 3.4.4 훈련 검증

model.fit() 함수는 History 객체를 반환한다.
train 과정에서 발생하는 정보를 딕셔너리형태 history 속성으로 갖고 있다.
validation 셋을 사용하기 위해서는, 다음과 같은 형태로 학습을 진행하면 된다.

```python
history = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_val,y_val))
history_dict = history.history
history_dict.keys()

>>>[u'acc', u'loss', u'val_acc', u'val_loss']
```

성능 비교를 위해 이부분은 꼭 기억할 것!
matplotlib 모듈로 시각화 하기 위해서,

```python
import matplotlib.pyplot as plt
history_dict = histroy.history
loss = history_dict['loss']
val_loss = history_dict['val_loss']
epochs = range(1, len(loss) +1)
plt.plot(epochs, loss, 'bo', label= 'Training loss') --- bo는 파란색 점을 의미
plt.plot(epochs, val_loss, 'b', label='Validation loss') ---- b는 파란색 실선을 의미
plt.title('Training & Val loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.show()
```



#### 3.4.5 훈련된 모델로 새로운 데이터에 대해 예측하기

모델을 훈련시킨 후 model.predict(x_test)로 확률을 예측할 수 있다.

```python
model.predict(x_test)
array([ [0.98006207]
		[0.99758697]
		[0.99975556]
		...,
		[0.82167041]
		[0.02885115]],dtype=flaot32)
```



이런 결과가 나올 경우, 어떤 샘플에 대해서는 0.98 이상의 높은 확률값으로 예측하고 있지만, 어떤 샘플에 대해서는 0.02처럼 확신이 부족한 예측을 보인다고 해석할 수 있다.



#### 3.5 뉴스 기사 분류: 다중 분류 문제

로이터 뉴스를 46개의 상호 배타적인 토픽으로 분류 하는 문제
정확히 말하면, single-label, multiclass classification이 되고, 데이터 포인트가 여러개의 클래스에 속할 수 있다면, multi-label, multiclass classification 문제가 된다.



#### 3.5.1 로이터 데이터셋

로이터 데이터셋
1. 1986년 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합
2. 텍스트 분류를 위해 사용되는 간단한 데이터셋
3. 46개의 토픽이 있음
4. 토픽에 따른 데이터 수의 불균형이 있음.
5. 8982개의 train sample과 2246개의 test sample이 있음
6. IMDB 리뷰와 같이 정수리스트(단어 인덱스), 토픽 인덱스(0~45)



#### 3.5.2 데이터 준비

```python
import numpy as np

def vectorize_sequences(sequences, dimension=10000):
	results = np.zeros((len(sequences), dimension))
	for i, sequence in enumerate(sequences):
		results[i,sequences] = 1.
	return results
    
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

```

원-핫 인코딩으로 벡터로 변환
레이블 데이터는 

```python
from keras.urils,.np_utils import to_categorical
one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```


케라스 내장함수를 이용하여 카테고리 분류가 가능하다.



#### 3.5.3 모델 구성

IMDB 영화 리뷰는 클래스가 2개 였으므로, 은닛 유닛의 개수를 16개로 지정했지만, 46개의 클래스를 분류하는 로이터 토픽 분류에서는 은닉 유닛의 개수가 적을 경우, feature를 누락하는 info bottleneck 현상을 일으킬 우려가 많으므로, 64개의 유닛을 사용해보자.

```python
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
```

* optimizer : rmsprop
* loss function : categorical_crossentropy



#### 3.5.4 훈련 검증

10000개 중 1000개의 데이터를 validation data로 사용하여 학습



#### 3.5.5 새로운 데이터에 대해 예측하기

```python
predictions = model.predict(x_test)

```

* prediction의 각 항목은 길이가 46인 벡터
  (46, )
* 벡터들의 원소의 합은 1
  np.sum(predictions[0])
  1.0
* 가장 큰 값이 예측 클래스 -> 가장 높은 확률의 클래스
  np.argmax(prediction[0])
  3



#### 3.5.6 레이블과 손실을 다루는 다른 방법

* ##### 정수 레이블을 사용할 때 

* loss function -> sparse_categorical_crossentropy

  

* ##### 범주형 인코딩이 된 레이블을 사용할 때 

* loss function -> categorical_crossentropy



#### 3.5.7 충분히 큰 중간층을 두어야 하는 이유

마지막 출력이 46개의 클래스분류(차원)이므로 그 중간 층의 은닉 유닛은 46개보다 많이 적어서는 안된다. 

-> 중요 개념 : low level feature map을 확장시킬 때 정보 손실이 많이 발생하기 때문.



#### 3.6 주택 가격 예측: 회귀 문제

개별적인 레이블이 아닌 연속적인 값을 예측하는 문제 : 회귀(regression)
주의할 점 : logistic regression은 회귀가 아닌 분류 알고리즘!



#### 3.6.1 보스턴 주택 가격 데이터셋

보스턴 주택 가격 데이터셋
1. 1970년 중반 보스턴 외곽 지역의 범죄율, 지방세율 등의 데이터가 주어졌을 때 주택 가격의 중간 값을 예측하는 문제에서 사용하는 데이터 셋
2. 데이터 포인트 개수는 총 506개, train sample은 404개 , test sample은 102개
3. sample의 feature들은 스케일이 다름 (0~1, 1~12, 1~100)
4. 13개의 feature
5. label 데이터는 주택의 중간 가격(천달러 단위) 



#### 3.6.2 데이터 준비

스케일이 다른 feature들을 Scaling하는 작업이 필요함.
참고 : 
Scaling : 서로 다른 단위의 데이터를 같은 단위로 만들어서 큰 숫자가 더 중요해보이는 왜곡을 막는 것
Standardization : 분포를 평균 0, 표준편차 1로 바꾸는것
Normalization : 변수를 0과 1사이로 바꾸는것

```python
mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std
```

* 절대 테스트 데이터에서 계산한 어떤 값도 사용해서는 안된다. 데이터 정규화처럼 간단한 작업도 !



#### 3.6.3 모델 구성

train 데이터가 적을수록 오버피팅이 잘 일어나기 때문에 작은 모델을 사용하는 것이 바람직
네트워크의 마지막 층은 활성화 함수가 없음 -> 선형 레이어(전형적인 스칼라 회귀를 위한 구성)
이 모델은 mse(mean squared error) loss function을 활용
평가 지표는 mae(mean absolute error)를 활용



#### 3.6.4 K-겹 검증을 사용한 훈련 검증

데이터 셋이 작으므로, 검증 데이터셋도 매우 작아짐
이럴 경우, K-fold cross-validation라고 불리는 방식으로 훈련을 진행함
K개의 분할(fold)로 데이터를 나누고 K-1개의 분할에서 훈련하고, 나머지 분할에서 평가하는 방법이다.
모델의 검증 점수는 K개의 검증 점수 평균