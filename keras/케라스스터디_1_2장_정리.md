mnist 데이터셋은 넘파이 배열 형태로 케라스에 이미 포함되어 있음.불러 올때는, 

from keras.datasets import mnist

(train_images, train_labels) , (test_images, test_labels) = mnist.load_data()

로 불러온다.

모델은 간단히 

from keras import models

from keras import layers

net = models.Sequential()

net.add(layers.Dense(512, activation=’relu’, input_shape=(28 * 28,)))

net_add(layers.Dense(10, activation=’softmax’))

로 구현하고 

손실 함수, 옵티마이저, 모니터링 지표를 넣어 컴파일한다.

network.compile(optimizer=’rmsprop’,

​			loss = ‘categorical_crossentropy’,

​			metrics=[‘accuracy’])

train_images = train_images.reshape((60000,28 * 28))

train_images = train_images.astype(‘float32’) / 255

test_images = test_images.reshape((60000, 28 * 28))

test_images = test_images.astype(‘float32’) / 255

으로 데이터 형식을 바꿔준다.

from keras.utils import to_categorical

train_labels = to_categorical(train_labels)

test_labels = to_categorical(test_labels)

로 레이블을 바이너리 형식으로 변환한다.

그리고 net.fit(train_images, train_labels, epochs=5, batch_size = 128)

로 학습한다.

결과를 평가할 땐,

test_loss, test_acc = net.evaluate(test_images, test_labels)

해서 학습한 네트워크의 손실과 정확도를 평가할 수 있다.



1차원 배열을 벡터, 2차원 배열을 행렬, 3차원이상의 배열을 텐서라고 한다.

보는 법은 제일 뒤의 shape가 가장 안쪽의 배열을 구성하는 개수로 차례대로 바깥쪽의 배열을 구성하는 shape가 된다.

데이터 타입은 대개 float32, float64, int32등등을 사용한다. 

간혹 char을 사용하기도 한다.

딥러닝에서 사용하는 모든 데이터 텐서의 첫번 째 shape는 샘플 축이다. 그러므로 

batch = train_images[:128]

로 배치를 지정할 수 있고,

다음 배치는

batch = train_images[128:256] 이런식이 된다.

n 번째 배치는 

batch = train_images[128 * n : 128 * (n+1)]

으로 표현할 수 있다.

이미지 데이터 shape은 텐서플로의 경우, (batch_size, height, width, channel)로 구성된다.

theano는 (batch_size , channel, height, width)로 구성된다.

케라스는 백엔드(텐서플로, 씨아노) 를 어떤 걸로 쓰는지에 따라 다르다.

 

def naive_relu(x):

​	assert len(x.shape) ==2

​	x = x.copy()  # 값 복사

​	for i in range(x.shape[0]):

​		for j in range(x.shape[1]):

​			x[i , j] = max(x[i, j], 0)

​	return x

브로드 캐스팅(중요하다)

브로드캐스팅은 2단계가 있다.

1. 큰 텐서의 차원에 맞도록 작은 텐서에 축이 추가된다.
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복된다.

브로드캐스팅의 예는 다음과 같다.

import numpy as np

x = np.random.random((64, 3, 32, 10))

y = np.random.random((32, 10))

z = np.maximum(x, y)

\# 출력 텐서의 크기 ( 64, 3, 32, 10)

점곱 연산은 다음과 같이 나타낼 수 있다.

import numpy as np

z = np.dot(x, y)

z = x * y

계산 하는 방식은

def naive_vector_dot(x, y):

​	assert len(x.shape) == 1

​	assert len(y.shape) == 1

​	assert x.shape[0] == y.shape[0]

​	z = 0.

​	for i in range(x.shape[0]):

​		z+=x[i] * y[i]

​	return z

두 벡터의 점곱은 스칼라가 되므로, 원소 개수가 같은 벡터끼리 점곱이 가능하다.

행렬 간의 점곱은

def naive_vector_dot(x, y):

​	assert len(x.shape) == 2

​	assert len(y.shape) == 2

​	assert x.shape[1] == y.shape[0]

​	z = np.zeros((x.shape[0], y.shape[1]))

​	for i in range(x.shape[0]):

​		for j in range(y.shape[1]):

​			row_x = x[i, :]

​			column_y = y[:, j]

​			z[i, j] = naive_vector_dot(row_x, column_y)

​	return z

의 식으로 나타낼 수 있다.

affine 변환, 회전, 스케일링은 점곱을 통해 구현이 가능하다.



훈련 과정

1. 훈련 샘플 x와 타깃 y의 배치를 추출
2. x를 사용하여 네트워크를 실행(forward pass단계), 예측 y_pred를 구한다.
3. y_pred와 y_true의 차이를 측정하여 이 배치에 대한 네트워크의 손실을 계산한다,
4. 배치에 대한 손실이 조금 감소되도록 네트워크의 모든 가중치를 업데이트 한다.

최적화 방법 중 SGD의 변종이 있는데,

모멘텀을 사용한 SGD, Adagrad, RMSProp등이 있다.

모멘텀은 SGD에 있는 2개의 문제점인 수렴 속도와 지역 최솟값을 해결한다.

지역 최솟값에 갇히지 않도록 과거의 가속도를 함께 고려하여 현재 단계의 최적화를 수행한다.

past_velocity = 0.

momentum = 0.1

while loss > 0.01:

​	w, loss, gradient = get_current_parameters()

​	velocity = momentum * past_velocity - learning_rate * gradient

​	w = w + momentum * velocity - learning_rate * gradient

​	past_velocity = velocity

​	update_parameter(w)

위는 단순한 구현의 예이다.



mnist 예제를 다시 살펴보면,

이미지를 reshape함수로 (60000, 784)의 크기로 변환하고,

float32타입으로 변환한다.

간단한 신경망을 구성하고, 마지막 레이어는 softmax로 확률값이 출력되도록 한다,

그리고 옵티마이저와 손실함수, 측정지표를 매개변수로 컴파일하고, 

에폭과 배치사이즈를 지정하여 image와 label을 인풋으로 학습한다,



학습은 훈련 데이터 샘플과 그에 상응하는 타깃이 주어졌을 때 손실 함수를 최소화 하는 모델 파라미터의 조합을 찾는 것을 의미한다. 

배치를 기준으로 손실을 구하고, 그에 상응하는 그래디언트로 학습한다. 

네트워크의 파라미터는 그래디언트의 반대 방향으로 조금씩 움직인다.

체인룰을 이용해 역전파를 쉽게 한다.

손실은 훈련하는 동안 최소화해야할 양이므로 해결하려는 문제의 성공을 측정하는 데 사용한다.

문제 해결 상황에 따라 다른 손실함수를 사용한다.

옵티마이저는 손실에 대한 그래디언트가 파라미터를 업데이트하는 정확한 방식을 정의한다.