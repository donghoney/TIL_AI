{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tensorflow_Learning_Tips_13011186","version":"0.3.2","provenance":[{"file_id":"17470VwWXaP90eobg_6OKVnWbSWpGhtqT","timestamp":1555635436488}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"h2uVVDK4u9EH","colab_type":"text"},"cell_type":"markdown","source":["## [실습1] Multinomial Classification 문제를 해결하기 위해 적절한  Learning Rate 를 찾아봅시다.\n","\n","\n","### 1) 직접 코드를 이해하여 주석을 달아 제출\n","- 주석을 달면서 이해가 안가는 부분 조교에게 질문\n","\n","### 2) Learning rate 를 찾아보세요\n","-  1.5 => 1e-10 => 0.1 순으로 변경해보자. \n","- 아래의 ?? 부분에 learning rate 값을 넣으면 된다.\n","  - optimizer = tf.train.GradientDescentOptimizer(learning_rate=???).minimize(cost)\n","\n"]},{"metadata":{"id":"WPaiH7_Oub-w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":10424},"outputId":"f13c6274-0375-4437-cd9b-cadcb311cbe7","executionInfo":{"status":"ok","timestamp":1555641408450,"user_tz":-540,"elapsed":2926,"user":{"displayName":"SOFT HI","photoUrl":"https://lh3.googleusercontent.com/-F3CCHlI0HDc/AAAAAAAAAAI/AAAAAAAAAAw/4rqV-_mvMdg/s64/photo.jpg","userId":"13903510717352952256"}}},"cell_type":"code","source":["\n","import tensorflow as tf\n","tf.set_random_seed(777)  # for reproducibility\n","\n","# 학습용 데이터 셋\n","x_data = [[1, 2, 1],\n","          [1, 3, 2],\n","          [1, 3, 4],\n","          [1, 5, 5],\n","          [1, 7, 5],\n","          [1, 2, 5],\n","          [1, 6, 6],\n","          [1, 7, 7]]\n","y_data = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]]\n","\n","# 평가용 데이터 셋\n","x_test = [[2, 1, 1],\n","          [3, 1, 2],\n","          [3, 3, 4]]\n","y_test = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1]]\n","\n","\n","\n","X = tf.placeholder(\"float\", [None, 3])\n","Y = tf.placeholder(\"float\", [None, 3])\n","\n","W = tf.Variable(tf.random_normal([3, 3]))\n","b = tf.Variable(tf.random_normal([3]))\n","\n","\n","hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)  #가설함수 : h(x)=WX+b\n","cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) #cross entropy 비용함수: \\sum{-(Y*log(h(x)))}\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost) # Gradient Descent Algorithm으로 Optimization\n","\n","prediction = tf.argmax(hypothesis, 1)  # argmax를 통해, 예측값을 ont-hot encoding\n","is_correct = tf.equal(prediction, tf.argmax(Y, 1)) # Y_pred과 Y가 일치하면 1, 아니면 0\n","accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) # is_corrent(1또는 0)의 평균을 accuracy로 구한다. \n","# accuracy = (TP + TN)  / (전체 데이타 수 = P + N)\n","\n","\n","with tf.Session() as sess:\n","\n","    sess.run(tf.global_variables_initializer())\n","\n","    for step in range(201):\n","        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data}) \n","        # sess.run을 통해 optimizer를 반복적으로 업데이트\n","        # cost_val : cross entropy 비용함수의 output\n","        # W : 해당 step에서의 가중치\n","        print(step, cost_val, W_val)\n","\n","    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n","    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","0 5.7320304 [[ 0.8026957  0.6786129 -1.2172831]\n"," [-0.3051686 -0.3032113  1.508257 ]\n"," [ 0.7572236 -0.7008909 -2.108204 ]]\n","1 5.4846697 [[ 0.7953078   0.6822878  -1.2135701 ]\n"," [-0.33240548 -0.2858785   1.518161  ]\n"," [ 0.72987473 -0.6822354  -2.0995104 ]]\n","2 5.238222 [[ 0.78794074  0.6859487  -1.2098639 ]\n"," [-0.3595857  -0.26857927  1.5280421 ]\n"," [ 0.70256126 -0.66360044 -2.090832  ]]\n","3 4.992939 [[ 0.7805989   0.6895929  -1.2061661 ]\n"," [-0.38669372 -0.25132117  1.537892  ]\n"," [ 0.6752939  -0.64499116 -2.0821738 ]]\n","4 4.7491636 [[ 0.7732879   0.6932169  -1.2024791 ]\n"," [-0.41370815 -0.23411389  1.5476991 ]\n"," [ 0.6480878  -0.6264145  -2.0735443 ]]\n","5 4.507373 [[ 0.76601493  0.69681674 -1.1988059 ]\n"," [-0.4405992  -0.21697012  1.5574464 ]\n"," [ 0.6209642  -0.60787964 -2.0649555 ]]\n","6 4.2682323 [[ 0.7587894   0.7003872  -1.1951509 ]\n"," [-0.46732482 -0.19990678  1.5671086 ]\n"," [ 0.5939538  -0.5893994  -2.0564253 ]]\n","7 4.032673 [[ 0.7516238   0.7039218  -1.1915199 ]\n"," [-0.49382535 -0.182947    1.5766493 ]\n"," [ 0.5671005  -0.57099175 -2.0479798 ]]\n","8 3.8019824 [[ 0.7445346   0.70741236 -1.1879213 ]\n"," [-0.5200166  -0.16612287  1.5860164 ]\n"," [ 0.5404673  -0.5526823  -2.0396562 ]]\n","9 3.5778916 [[ 0.7375436   0.71084815 -1.1843661 ]\n"," [-0.54578215 -0.1494795   1.5951385 ]\n"," [ 0.5141423  -0.5345078  -2.0315056 ]]\n","10 3.3626084 [[ 0.73067856  0.7142153  -1.1808683 ]\n"," [-0.57096696 -0.13308004  1.6039239 ]\n"," [ 0.48824427 -0.51652104 -2.0235944 ]]\n","11 3.1587389 [[ 0.7239734   0.71749586 -1.1774437 ]\n"," [-0.59537727 -0.11701142  1.6122656 ]\n"," [ 0.46292493 -0.4987963  -2.0159998 ]]\n","12 2.9690466 [[ 0.7174665   0.72066736 -1.1741083 ]\n"," [-0.6187912  -0.10138907  1.6200572 ]\n"," [ 0.43836302 -0.4814348  -2.0087993 ]]\n","13 2.7960968 [[ 0.71119726  0.7237023  -1.170874  ]\n"," [-0.6409815  -0.086359    1.6272174 ]\n"," [ 0.41475078 -0.46456847 -2.0020535 ]]\n","14 2.641934 [[ 0.70520234  0.7265689  -1.1677456 ]\n"," [-0.6617409  -0.07209591  1.6337137 ]\n"," [ 0.39227784 -0.44836092 -1.9957881 ]]\n","15 2.5079045 [[ 0.69951284  0.7292322  -1.1647193 ]\n"," [-0.6808979  -0.05879673  1.6395715 ]\n"," [ 0.3711207  -0.43300363 -1.9899882 ]]\n","16 2.3945847 [[ 0.69415367  0.7316566  -1.1617845 ]\n"," [-0.6983183  -0.0466677   1.6448629 ]\n"," [ 0.3514393  -0.41870463 -1.9846058 ]]\n","17 2.301664 [[ 0.6891429   0.73381    -1.1589271 ]\n"," [-0.71390444 -0.03590129  1.6496826 ]\n"," [ 0.33337036 -0.40566543 -1.979576  ]]\n","18 2.2278142 [[ 0.6844894   0.7356693  -1.1561329 ]\n"," [-0.7276053  -0.02664298  1.6541252 ]\n"," [ 0.31700882 -0.39404637 -1.9748335 ]]\n","19 2.1707225 [[ 0.68018985  0.73722595 -1.1533899 ]\n"," [-0.7394347  -0.01895803  1.6582698 ]\n"," [ 0.30238274 -0.38393113 -1.9703226 ]]\n","20 2.1274319 [[ 0.676227    0.7384881  -1.1506892 ]\n"," [-0.7494819  -0.01281567  1.6621746 ]\n"," [ 0.28943902 -0.37530848 -1.9660015 ]]\n","21 2.0948603 [[ 0.67257196  0.7394785  -1.1480247 ]\n"," [-0.7579011  -0.00810016  1.6658782 ]\n"," [ 0.27805212 -0.36808115 -1.961842  ]]\n","22 2.0702443 [[ 0.66918904  0.74022925 -1.1453925 ]\n"," [-0.76488507 -0.00464118  1.6694032 ]\n"," [ 0.26805064 -0.36209488 -1.9578267 ]]\n","23 2.0513625 [[ 0.66604084  0.7407757  -1.1427908 ]\n"," [-0.77063674 -0.00224758  1.6727613 ]\n"," [ 0.2592473  -0.3571716  -1.9539467 ]]\n","24 2.0365548 [[ 6.6309196e-01  7.4115229e-01 -1.1402185e+00]\n"," [-7.7534771e-01 -7.3271879e-04  1.6759574e+00]\n"," [ 2.5146130e-01 -3.5313472e-01 -1.9501976e+00]]\n","25 2.024638 [[ 6.6031092e-01  7.4139005e-01 -1.1376752e+00]\n"," [-7.7918744e-01  7.1427647e-05  1.6789930e+00]\n"," [ 2.4453065e-01 -3.4982374e-01 -1.9465779e+00]]\n","26 2.01479 [[ 6.5767086e-01  7.4151564e-01 -1.1351607e+00]\n"," [-7.8229994e-01  3.0882124e-04  1.6818681e+00]\n"," [ 2.3831646e-01 -3.4709999e-01 -1.9430875e+00]]\n","27 2.0064454 [[ 6.5514934e-01  7.4155140e-01 -1.1326749e+00]\n"," [-7.8480476e-01  9.9187688e-05  1.6845826e+00]\n"," [ 2.3270269e-01 -3.4484738e-01 -1.9397264e+00]]\n","28 1.999217 [[ 6.5272790e-01  7.4151564e-01 -1.1302178e+00]\n"," [-7.8679997e-01 -4.5969681e-04  1.6871367e+00]\n"," [ 2.2759369e-01 -3.4297046e-01 -1.9364942e+00]]\n","29 1.9928365 [[ 6.5039134e-01  7.4142331e-01 -1.1277889e+00]\n"," [-7.8836554e-01 -1.2888501e-03  1.6895314e+00]\n"," [ 2.2291116e-01 -3.4139159e-01 -1.9333906e+00]]\n","30 1.9871159 [[ 0.6481272   0.74128646 -1.1253879 ]\n"," [-0.7895667  -0.00232491  1.6917686 ]\n"," [ 0.21859111 -0.34004802 -1.9304142 ]]\n","31 1.9819216 [[ 0.6459253   0.7411148  -1.1230143 ]\n"," [-0.79045665 -0.00351724  1.6938509 ]\n"," [ 0.21458116 -0.33888918 -1.9275631 ]]\n","32 1.9771557 [[ 0.6437772   0.74091613 -1.1206676 ]\n"," [-0.791079   -0.00482552  1.6957815 ]\n"," [ 0.21083833 -0.33787447 -1.924835  ]]\n","33 1.9727461 [[ 0.641676    0.7406967  -1.1183469 ]\n"," [-0.79146963 -0.00621772  1.6975644 ]\n"," [ 0.20732722 -0.33697128 -1.922227  ]]\n","34 1.968637 [[ 0.63961595  0.7404616  -1.1160517 ]\n"," [-0.7916583  -0.00766845  1.6992037 ]\n"," [ 0.20401849 -0.33615345 -1.9197361 ]]\n","35 1.9647851 [[ 0.6375922   0.74021477 -1.1137811 ]\n"," [-0.79166967 -0.00915765  1.7007043 ]\n"," [ 0.20088771 -0.33540004 -1.9173588 ]]\n","36 1.9611552 [[ 0.6356007   0.7399594  -1.1115342 ]\n"," [-0.7915245  -0.01066954  1.7020711 ]\n"," [ 0.1979144  -0.33469436 -1.9150912 ]]\n","37 1.9577192 [[ 0.63363796  0.7396981  -1.1093103 ]\n"," [-0.7912404  -0.01219176  1.7033092 ]\n"," [ 0.19508134 -0.334023   -1.9129294 ]]\n","38 1.9544528 [[ 0.6317012   0.7394329  -1.1071082 ]\n"," [-0.79083204 -0.0137147   1.7044238 ]\n"," [ 0.19237387 -0.3333754  -1.9108696 ]]\n","39 1.9513357 [[ 0.6297878   0.7391653  -1.1049273 ]\n"," [-0.7903122  -0.01523096  1.7054201 ]\n"," [ 0.18977952 -0.3327431  -1.9089075 ]]\n","40 1.9483505 [[ 0.62789565  0.7388966  -1.1027665 ]\n"," [-0.7896917  -0.01673489  1.7063036 ]\n"," [ 0.18728752 -0.33211952 -1.9070392 ]]\n","41 1.945483 [[ 0.62602293  0.7386278  -1.1006249 ]\n"," [-0.78898007 -0.01822225  1.7070794 ]\n"," [ 0.1848886  -0.3314995  -1.9052603 ]]\n","42 1.9427185 [[ 0.624168    0.73835963 -1.0985018 ]\n"," [-0.7881856  -0.01968996  1.7077526 ]\n"," [ 0.18257466 -0.33087897 -1.9035668 ]]\n","43 1.9400463 [[ 0.62232935  0.73809266 -1.0963962 ]\n"," [-0.7873155  -0.02113582  1.7083284 ]\n"," [ 0.1803386  -0.33025494 -1.9019548 ]]\n","44 1.9374563 [[ 0.6205058   0.7378273  -1.0943073 ]\n"," [-0.7863761  -0.02255836  1.7088115 ]\n"," [ 0.1781742  -0.32962507 -1.9004202 ]]\n","45 1.9349389 [[ 0.6186962   0.73756385 -1.0922343 ]\n"," [-0.78537315 -0.02395668  1.7092068 ]\n"," [ 0.17607586 -0.3289877  -1.8989593 ]]\n","46 1.9324868 [[ 0.61689955  0.73730254 -1.0901763 ]\n"," [-0.7843117  -0.02533033  1.709519  ]\n"," [ 0.17403865 -0.32834163 -1.8975681 ]]\n","47 1.9300921 [[ 0.6151149   0.7370435  -1.0881327 ]\n"," [-0.78319633 -0.0266792   1.7097526 ]\n"," [ 0.17205812 -0.32768607 -1.8962431 ]]\n","48 1.9277487 [[ 0.6133415   0.73678684 -1.0861026 ]\n"," [-0.7820312  -0.02800346  1.7099117 ]\n"," [ 0.17013027 -0.32702056 -1.8949808 ]]\n","49 1.9254514 [[ 0.6115786   0.73653257 -1.0840853 ]\n"," [-0.78081995 -0.02930352  1.7100005 ]\n"," [ 0.16825145 -0.32634485 -1.8937777 ]]\n","50 1.9231946 [[ 0.6098255   0.7362806  -1.0820802 ]\n"," [-0.7795661  -0.03057991  1.710023  ]\n"," [ 0.16641836 -0.32565892 -1.8926306 ]]\n","51 1.9209743 [[ 0.6080816   0.73603106 -1.0800867 ]\n"," [-0.77827275 -0.03183328  1.7099831 ]\n"," [ 0.16462797 -0.32496288 -1.8915362 ]]\n","52 1.9187864 [[ 0.60634625  0.73578376 -1.0781041 ]\n"," [-0.7769428  -0.0330644   1.7098843 ]\n"," [ 0.16287751 -0.324257   -1.8904916 ]]\n","53 1.9166279 [[ 0.604619    0.73553866 -1.0761318 ]\n"," [-0.7755788  -0.03427403  1.7097299 ]\n"," [ 0.16116443 -0.32354158 -1.889494  ]]\n","54 1.9144952 [[ 0.60289943  0.7352957  -1.0741693 ]\n"," [-0.7741832  -0.03546304  1.7095233 ]\n"," [ 0.15948638 -0.32281703 -1.8885404 ]]\n","55 1.9123857 [[ 0.60118705  0.7350548  -1.0722159 ]\n"," [-0.77275836 -0.03663226  1.7092676 ]\n"," [ 0.15784119 -0.3220838  -1.8876284 ]]\n","56 1.9102969 [[ 0.5994814   0.7348158  -1.0702713 ]\n"," [-0.7713063  -0.03778255  1.7089658 ]\n"," [ 0.15622684 -0.32134235 -1.8867555 ]]\n","57 1.908227 [[ 0.59778214  0.7345786  -1.0683348 ]\n"," [-0.7698289  -0.03891475  1.7086207 ]\n"," [ 0.15464146 -0.32059318 -1.8859192 ]]\n","58 1.9061737 [[ 0.5960889   0.7343432  -1.0664061 ]\n"," [-0.7683281  -0.04002971  1.7082348 ]\n"," [ 0.15308331 -0.3198368  -1.8851174 ]]\n","59 1.9041356 [[ 0.59440136  0.73410934 -1.0644848 ]\n"," [-0.76680547 -0.04112822  1.7078106 ]\n"," [ 0.15155075 -0.3190737  -1.8843479 ]]\n","60 1.9021113 [[ 0.5927192   0.73387706 -1.0625705 ]\n"," [-0.76526266 -0.04221108  1.7073507 ]\n"," [ 0.1500423  -0.3183044  -1.8836087 ]]\n","61 1.9000994 [[ 0.59104216  0.7336462  -1.0606625 ]\n"," [-0.7637011  -0.04327905  1.7068571 ]\n"," [ 0.14855652 -0.31752935 -1.882898  ]]\n","62 1.898099 [[ 0.58936995  0.7334167  -1.0587608 ]\n"," [-0.76212215 -0.04433285  1.706332  ]\n"," [ 0.14709209 -0.31674907 -1.8822138 ]]\n","63 1.8961089 [[ 0.5877023   0.7331884  -1.0568649 ]\n"," [-0.76052713 -0.04537317  1.7057773 ]\n"," [ 0.1456478  -0.31596398 -1.8815546 ]]\n","64 1.8941283 [[ 0.58603895  0.7329613  -1.0549744 ]\n"," [-0.7589172  -0.04640068  1.705195  ]\n"," [ 0.14422247 -0.31517458 -1.8809187 ]]\n","65 1.8921559 [[ 0.58437973  0.7327353  -1.0530891 ]\n"," [-0.7572935  -0.047416    1.7045866 ]\n"," [ 0.14281504 -0.31438124 -1.8803047 ]]\n","66 1.890192 [[ 0.58272445  0.7325102  -1.0512089 ]\n"," [-0.75565714 -0.04841974  1.703954  ]\n"," [ 0.1414245  -0.31358442 -1.879711  ]]\n","67 1.8882353 [[ 0.58107287  0.73228604 -1.0493331 ]\n"," [-0.754009   -0.04941244  1.7032986 ]\n"," [ 0.1400499  -0.31278446 -1.8791363 ]]\n","68 1.886286 [[ 0.57942486  0.73206276 -1.0474617 ]\n"," [-0.7523501  -0.05039465  1.7026219 ]\n"," [ 0.13869037 -0.31198177 -1.8785795 ]]\n","69 1.8843429 [[ 0.5777802   0.73184025 -1.0455946 ]\n"," [-0.7506813  -0.05136687  1.7019253 ]\n"," [ 0.13734506 -0.31117666 -1.8780392 ]]\n","70 1.8824056 [[ 0.57613873  0.7316184  -1.0437313 ]\n"," [-0.7490033  -0.05232956  1.70121   ]\n"," [ 0.13601321 -0.3103695  -1.8775146 ]]\n","71 1.8804743 [[ 0.5745003   0.7313972  -1.0418718 ]\n"," [-0.74731696 -0.05328316  1.7004772 ]\n"," [ 0.13469408 -0.30956057 -1.8770044 ]]\n","72 1.8785484 [[ 0.5728649   0.7311766  -1.0400157 ]\n"," [-0.74562293 -0.0542281   1.6997281 ]\n"," [ 0.13338701 -0.30875018 -1.8765076 ]]\n","73 1.8766278 [[ 0.5712322   0.73095655 -1.038163  ]\n"," [-0.74392194 -0.05516477  1.6989638 ]\n"," [ 0.13209136 -0.3079386  -1.8760235 ]]\n","74 1.874712 [[ 0.5696022   0.730737   -1.0363134 ]\n"," [-0.74221456 -0.05609351  1.6981852 ]\n"," [ 0.1308065  -0.3071261  -1.8755512 ]]\n","75 1.872801 [[ 0.56797475  0.7305178  -1.0344669 ]\n"," [-0.7405014  -0.0570147   1.6973933 ]\n"," [ 0.12953192 -0.30631295 -1.8750898 ]]\n","76 1.8708944 [[ 0.56634974  0.73029906 -1.0326232 ]\n"," [-0.738783   -0.05792863  1.6965889 ]\n"," [ 0.12826706 -0.30549932 -1.8746386 ]]\n","77 1.8689923 [[ 0.5647271   0.73008066 -1.0307821 ]\n"," [-0.7370599  -0.0588356   1.6957728 ]\n"," [ 0.12701145 -0.30468544 -1.8741968 ]]\n","78 1.8670944 [[ 0.5631068   0.7298626  -1.0289437 ]\n"," [-0.73533255 -0.05973591  1.6949457 ]\n"," [ 0.12576462 -0.30387154 -1.8737638 ]]\n","79 1.8652009 [[ 0.5614886   0.7296447  -1.0271076 ]\n"," [-0.7336014  -0.06062981  1.6941085 ]\n"," [ 0.12452616 -0.30305782 -1.873339  ]]\n","80 1.8633114 [[ 0.55987257  0.7294271  -1.0252739 ]\n"," [-0.7318669  -0.06151754  1.6932617 ]\n"," [ 0.12329565 -0.30224442 -1.872922  ]]\n","81 1.8614255 [[ 0.55825853  0.72920966 -1.0234424 ]\n"," [-0.7301294  -0.06239934  1.692406  ]\n"," [ 0.12207273 -0.3014315  -1.8725119 ]]\n","82 1.8595438 [[ 0.5566464   0.7289924  -1.021613  ]\n"," [-0.7283893  -0.06327541  1.691542  ]\n"," [ 0.12085703 -0.30061924 -1.8721085 ]]\n","83 1.8576657 [[ 0.5550362   0.72877526 -1.0197856 ]\n"," [-0.72664696 -0.06414596  1.6906703 ]\n"," [ 0.11964824 -0.2998078  -1.8717111 ]]\n","84 1.8557913 [[ 0.5534278   0.72855824 -1.0179602 ]\n"," [-0.7249027  -0.06501117  1.6897912 ]\n"," [ 0.11844605 -0.29899728 -1.8713194 ]]\n","85 1.8539208 [[ 0.5518212   0.72834134 -1.0161366 ]\n"," [-0.72315675 -0.06587121  1.6889054 ]\n"," [ 0.11725017 -0.29818782 -1.8709329 ]]\n","86 1.8520539 [[ 0.55021626  0.7281245  -1.0143149 ]\n"," [-0.7214095  -0.06672624  1.6880131 ]\n"," [ 0.11606034 -0.29737955 -1.8705513 ]]\n","87 1.8501909 [[ 0.548613    0.72790766 -1.0124948 ]\n"," [-0.7196612  -0.06757642  1.687115  ]\n"," [ 0.1148763  -0.29657257 -1.8701743 ]]\n","88 1.8483311 [[ 0.5470114   0.7276909  -1.0106764 ]\n"," [-0.71791196 -0.06842189  1.6862112 ]\n"," [ 0.11369782 -0.29576698 -1.8698014 ]]\n","89 1.8464749 [[ 0.5454113   0.7274741  -1.0088595 ]\n"," [-0.71616215 -0.06926277  1.6853024 ]\n"," [ 0.11252468 -0.29496288 -1.8694323 ]]\n","90 1.8446224 [[ 0.54381275  0.72725725 -1.0070442 ]\n"," [-0.714412   -0.07009918  1.6843886 ]\n"," [ 0.11135668 -0.29416037 -1.8690668 ]]\n","91 1.8427731 [[ 0.5422157   0.7270404  -1.0052303 ]\n"," [-0.7126616  -0.07093126  1.6834704 ]\n"," [ 0.11019363 -0.29335952 -1.8687047 ]]\n","92 1.8409274 [[ 0.54062015  0.7268235  -1.0034178 ]\n"," [-0.7109113  -0.07175908  1.6825478 ]\n"," [ 0.10903535 -0.2925604  -1.8683455 ]]\n","93 1.8390853 [[ 0.539026    0.72660655 -1.0016067 ]\n"," [-0.7091611  -0.07258277  1.6816213 ]\n"," [ 0.10788167 -0.2917631  -1.8679891 ]]\n","94 1.8372464 [[ 0.53743327  0.72638947 -0.99979687]\n"," [-0.7074113  -0.0734024   1.6806911 ]\n"," [ 0.10673244 -0.29096767 -1.8676353 ]]\n","95 1.8354111 [[ 0.5358419   0.72617227 -0.9979883 ]\n"," [-0.705662   -0.07421806  1.6797575 ]\n"," [ 0.10558752 -0.29017422 -1.8672838 ]]\n","96 1.8335791 [[ 0.5342518   0.725955   -0.99618095]\n"," [-0.70391333 -0.07502984  1.6788206 ]\n"," [ 0.10444676 -0.28938276 -1.8669345 ]]\n","97 1.8317506 [[ 0.5326631   0.7257376  -0.9943748 ]\n"," [-0.7021655  -0.07583781  1.6778808 ]\n"," [ 0.10331005 -0.28859338 -1.8665872 ]]\n","98 1.8299253 [[ 0.53107566  0.72552    -0.99256986]\n"," [-0.70041853 -0.07664202  1.676938  ]\n"," [ 0.10217726 -0.28780612 -1.8662417 ]]\n","99 1.8281035 [[ 0.5294895   0.72530234 -0.990766  ]\n"," [-0.69867265 -0.07744256  1.6759927 ]\n"," [ 0.10104828 -0.28702104 -1.8658978 ]]\n","100 1.8262851 [[ 0.5279046   0.7250845  -0.98896325]\n"," [-0.6969279  -0.07823947  1.6750449 ]\n"," [ 0.09992302 -0.28623816 -1.8655554 ]]\n","101 1.82447 [[ 0.526321    0.72486645 -0.9871616 ]\n"," [-0.6951844  -0.07903282  1.6740948 ]\n"," [ 0.09880138 -0.28545755 -1.8652143 ]]\n","102 1.8226584 [[ 0.52473855  0.72464824 -0.9853609 ]\n"," [-0.6934423  -0.07982264  1.6731424 ]\n"," [ 0.09768327 -0.28467923 -1.8648746 ]]\n","103 1.8208501 [[ 0.5231573   0.72442985 -0.98356134]\n"," [-0.6917016  -0.08060901  1.672188  ]\n"," [ 0.09656861 -0.28390324 -1.8645359 ]]\n","104 1.8190451 [[ 0.52157724  0.7242113  -0.9817627 ]\n"," [-0.68996245 -0.08139195  1.6712319 ]\n"," [ 0.09545731 -0.2831296  -1.8641982 ]]\n","105 1.8172435 [[ 0.5199984   0.7239925  -0.9799651 ]\n"," [-0.6882249  -0.08217151  1.6702739 ]\n"," [ 0.09434931 -0.28235838 -1.8638614 ]]\n","106 1.8154452 [[ 0.5184207   0.72377354 -0.9781684 ]\n"," [-0.68648905 -0.08294774  1.6693143 ]\n"," [ 0.09324455 -0.2815896  -1.8635255 ]]\n","107 1.8136501 [[ 0.51684415  0.7235544  -0.9763727 ]\n"," [-0.6847549  -0.08372065  1.6683531 ]\n"," [ 0.09214294 -0.28082326 -1.8631903 ]]\n","108 1.8118584 [[ 0.51526874  0.72333497 -0.9745779 ]\n"," [-0.6830226  -0.0844903   1.6673905 ]\n"," [ 0.09104445 -0.28005943 -1.8628557 ]]\n","109 1.8100703 [[ 0.51369447  0.7231153  -0.97278404]\n"," [-0.6812922  -0.0852567   1.6664264 ]\n"," [ 0.089949   -0.2792981  -1.8625215 ]]\n","110 1.8082851 [[ 0.5121213   0.7228955  -0.9709911 ]\n"," [-0.6795637  -0.08601988  1.6654612 ]\n"," [ 0.08885657 -0.2785393  -1.8621879 ]]\n","111 1.8065035 [[ 0.5105493   0.72267544 -0.96919894]\n"," [-0.6778372  -0.08677988  1.6644948 ]\n"," [ 0.08776709 -0.27778307 -1.8618547 ]]\n","112 1.8047253 [[ 0.50897837  0.72245514 -0.9674077 ]\n"," [-0.6761128  -0.08753672  1.6635271 ]\n"," [ 0.08668052 -0.2770294  -1.8615217 ]]\n","113 1.8029501 [[ 0.50740856  0.7222346  -0.9656173 ]\n"," [-0.67439044 -0.08829042  1.6625584 ]\n"," [ 0.08559683 -0.27627832 -1.8611891 ]]\n","114 1.8011785 [[ 0.5058398   0.7220138  -0.9638278 ]\n"," [-0.67267025 -0.08904101  1.6615888 ]\n"," [ 0.08451597 -0.27552986 -1.8608568 ]]\n","115 1.7994099 [[ 0.5042722   0.7217927  -0.9620391 ]\n"," [-0.6709522  -0.0897885   1.6606183 ]\n"," [ 0.0834379  -0.27478403 -1.8605245 ]]\n","116 1.7976447 [[ 0.5027057   0.7215714  -0.9602512 ]\n"," [-0.6692364  -0.09053292  1.6596469 ]\n"," [ 0.08236259 -0.27404085 -1.8601924 ]]\n","117 1.7958828 [[ 0.50114024  0.7213498  -0.95846415]\n"," [-0.66752285 -0.09127428  1.6586747 ]\n"," [ 0.08129002 -0.27330032 -1.8598604 ]]\n","118 1.7941246 [[ 0.49957585  0.7211279  -0.9566779 ]\n"," [-0.6658116  -0.09201258  1.6577017 ]\n"," [ 0.08022015 -0.27256247 -1.8595284 ]]\n","119 1.792369 [[ 0.49801254  0.7209058  -0.95489246]\n"," [-0.6641027  -0.09274786  1.656728  ]\n"," [ 0.07915295 -0.2718273  -1.8591964 ]]\n","120 1.7906172 [[ 0.4964503   0.7206834  -0.95310783]\n"," [-0.6623961  -0.09348013  1.6557537 ]\n"," [ 0.0780884  -0.27109483 -1.8588643 ]]\n","121 1.7888687 [[ 0.4948891   0.7204607  -0.951324  ]\n"," [-0.66069186 -0.09420938  1.6547788 ]\n"," [ 0.07702649 -0.27036506 -1.8585322 ]]\n","122 1.7871232 [[ 0.493329    0.72023773 -0.9495409 ]\n"," [-0.6589901  -0.09493564  1.6538033 ]\n"," [ 0.07596718 -0.269638   -1.8582    ]]\n","123 1.7853808 [[ 0.49176994  0.7200145  -0.9477586 ]\n"," [-0.6572907  -0.09565892  1.6528273 ]\n"," [ 0.07491045 -0.26891366 -1.8578676 ]]\n","124 1.7836419 [[ 0.49021193  0.719791   -0.9459771 ]\n"," [-0.6555938  -0.09637924  1.6518507 ]\n"," [ 0.07385629 -0.26819205 -1.857535  ]]\n","125 1.7819064 [[ 0.48865497  0.7195672  -0.94419634]\n"," [-0.6538994  -0.09709658  1.6508737 ]\n"," [ 0.07280469 -0.2674732  -1.8572023 ]]\n","126 1.7801739 [[ 0.48709908  0.71934307 -0.94241637]\n"," [-0.65220743 -0.09781098  1.6498961 ]\n"," [ 0.07175561 -0.26675707 -1.8568693 ]]\n","127 1.7784449 [[ 0.48554423  0.71911865 -0.9406371 ]\n"," [-0.650518   -0.09852243  1.6489182 ]\n"," [ 0.07070905 -0.26604372 -1.8565361 ]]\n","128 1.7767192 [[ 0.48399043  0.71889395 -0.9388586 ]\n"," [-0.6488311  -0.09923095  1.6479398 ]\n"," [ 0.06966499 -0.26533315 -1.8562027 ]]\n","129 1.7749966 [[ 0.48243767  0.71866894 -0.9370809 ]\n"," [-0.6471468  -0.09993652  1.6469611 ]\n"," [ 0.06862342 -0.2646253  -1.8558689 ]]\n","130 1.7732773 [[ 0.48088598  0.71844363 -0.9353039 ]\n"," [-0.6454651  -0.10063917  1.645982  ]\n"," [ 0.06758432 -0.26392025 -1.8555349 ]]\n","131 1.771561 [[ 0.47933534  0.718218   -0.9335277 ]\n"," [-0.6437859  -0.10133891  1.6450026 ]\n"," [ 0.06654769 -0.263218   -1.8552005 ]]\n","132 1.7698482 [[ 0.47778574  0.7179921  -0.9317522 ]\n"," [-0.64210933 -0.10203573  1.6440228 ]\n"," [ 0.06551351 -0.2625185  -1.8548658 ]]\n","133 1.7681388 [[ 0.47623718  0.7177659  -0.9299775 ]\n"," [-0.6404354  -0.10272964  1.6430428 ]\n"," [ 0.06448177 -0.26182178 -1.8545308 ]]\n","134 1.7664323 [[ 0.47468966  0.7175394  -0.92820346]\n"," [-0.6387641  -0.10342064  1.6420624 ]\n"," [ 0.06345247 -0.26112786 -1.8541955 ]]\n","135 1.7647293 [[ 0.4731432   0.71731263 -0.92643017]\n"," [-0.6370954  -0.10410875  1.6410818 ]\n"," [ 0.06242559 -0.26043674 -1.8538597 ]]\n","136 1.7630293 [[ 0.47159776  0.71708554 -0.92465764]\n"," [-0.6354294  -0.10479397  1.640101  ]\n"," [ 0.06140112 -0.2597484  -1.8535235 ]]\n","137 1.7613325 [[ 0.47005337  0.7168581  -0.92288584]\n"," [-0.633766   -0.10547629  1.6391199 ]\n"," [ 0.06037906 -0.25906286 -1.853187  ]]\n","138 1.7596389 [[ 0.46851003  0.71663034 -0.92111474]\n"," [-0.6321053  -0.10615573  1.6381385 ]\n"," [ 0.0593594  -0.25838012 -1.8528501 ]]\n","139 1.7579486 [[ 0.46696773  0.7164023  -0.9193444 ]\n"," [-0.63044727 -0.10683228  1.6371571 ]\n"," [ 0.05834213 -0.25770018 -1.8525127 ]]\n","140 1.756262 [[ 0.46542647  0.71617395 -0.9175748 ]\n"," [-0.6287919  -0.10750595  1.6361754 ]\n"," [ 0.05732724 -0.25702307 -1.852175  ]]\n","141 1.7545779 [[ 0.46388626  0.71594524 -0.91580594]\n"," [-0.6271393  -0.10817674  1.6351936 ]\n"," [ 0.05631474 -0.25634876 -1.8518368 ]]\n","142 1.7528973 [[ 0.4623471   0.71571624 -0.91403776]\n"," [-0.62548935 -0.10884465  1.6342115 ]\n"," [ 0.05530461 -0.25567725 -1.8514981 ]]\n","143 1.75122 [[ 0.46080896  0.71548694 -0.9122703 ]\n"," [-0.6238421  -0.10950968  1.6332294 ]\n"," [ 0.05429684 -0.25500855 -1.8511591 ]]\n","144 1.7495457 [[ 0.45927188  0.7152573  -0.91050357]\n"," [-0.6221976  -0.11017184  1.6322471 ]\n"," [ 0.05329144 -0.25434265 -1.8508196 ]]\n","145 1.7478746 [[ 0.45773587  0.71502733 -0.9087376 ]\n"," [-0.6205558  -0.11083113  1.6312646 ]\n"," [ 0.05228839 -0.25367957 -1.8504796 ]]\n","146 1.7462064 [[ 0.4562009   0.714797   -0.90697235]\n"," [-0.61891675 -0.11148756  1.6302819 ]\n"," [ 0.05128769 -0.2530193  -1.8501391 ]]\n","147 1.7445419 [[ 0.45466697  0.7145664  -0.9052078 ]\n"," [-0.6172804  -0.11214112  1.6292992 ]\n"," [ 0.05028933 -0.25236183 -1.8497982 ]]\n","148 1.7428802 [[ 0.4531341   0.71433544 -0.903444  ]\n"," [-0.61564684 -0.11279181  1.6283163 ]\n"," [ 0.04929332 -0.2517072  -1.8494568 ]]\n","149 1.7412218 [[ 0.45160225  0.7141042  -0.9016809 ]\n"," [-0.614016   -0.11343963  1.6273333 ]\n"," [ 0.04829964 -0.25105536 -1.8491149 ]]\n","150 1.7395668 [[ 0.45007145  0.71387255 -0.8999185 ]\n"," [-0.61238796 -0.11408459  1.6263502 ]\n"," [ 0.0473083  -0.25040632 -1.8487725 ]]\n","151 1.7379146 [[ 0.4485417   0.71364063 -0.8981568 ]\n"," [-0.61076266 -0.11472669  1.6253669 ]\n"," [ 0.04631929 -0.2497601  -1.8484297 ]]\n","152 1.7362657 [[ 0.44701302  0.71340835 -0.89639586]\n"," [-0.6091401  -0.11536594  1.6243837 ]\n"," [ 0.04533261 -0.2491167  -1.8480864 ]]\n","153 1.73462 [[ 0.44548538  0.7131758  -0.8946356 ]\n"," [-0.60752034 -0.11600232  1.6234003 ]\n"," [ 0.04434824 -0.2484761  -1.8477426 ]]\n","154 1.7329772 [[ 0.4439588   0.71294284 -0.8928761 ]\n"," [-0.6059033  -0.11663585  1.6224169 ]\n"," [ 0.0433662  -0.24783832 -1.8473983 ]]\n","155 1.7313379 [[ 0.44243324  0.7127096  -0.89111733]\n"," [-0.6042891  -0.11726653  1.6214333 ]\n"," [ 0.04238648 -0.24720334 -1.8470535 ]]\n","156 1.7297015 [[ 0.44090876  0.712476   -0.8893593 ]\n"," [-0.60267764 -0.11789435  1.6204497 ]\n"," [ 0.04140906 -0.24657117 -1.8467083 ]]\n","157 1.7280681 [[ 0.43938532  0.7122421  -0.887602  ]\n"," [-0.601069   -0.11851932  1.619466  ]\n"," [ 0.04043396 -0.2459418  -1.8463626 ]]\n","158 1.726438 [[ 0.43786293  0.7120079  -0.88584536]\n"," [-0.5994631  -0.11914144  1.6184822 ]\n"," [ 0.03946116 -0.24531524 -1.8460164 ]]\n","159 1.7248111 [[ 0.4363416   0.7117733  -0.88408947]\n"," [-0.59786004 -0.11976071  1.6174984 ]\n"," [ 0.03849066 -0.24469149 -1.8456696 ]]\n","160 1.7231871 [[ 0.43482134  0.7115384  -0.8823343 ]\n"," [-0.5962597  -0.12037713  1.6165146 ]\n"," [ 0.03752247 -0.24407054 -1.8453224 ]]\n","161 1.7215663 [[ 0.43330213  0.7113031  -0.8805798 ]\n"," [-0.5946622  -0.1209907   1.6155306 ]\n"," [ 0.03655657 -0.2434524  -1.8449746 ]]\n","162 1.7199488 [[ 0.43178397  0.7110675  -0.8788261 ]\n"," [-0.59306747 -0.12160143  1.6145467 ]\n"," [ 0.03559296 -0.24283706 -1.8446264 ]]\n","163 1.718334 [[ 0.4302669   0.7108316  -0.87707305]\n"," [-0.59147555 -0.1222093   1.6135626 ]\n"," [ 0.03463165 -0.2422245  -1.8442776 ]]\n","164 1.7167227 [[ 0.42875084  0.7105953  -0.87532073]\n"," [-0.5898864  -0.12281434  1.6125785 ]\n"," [ 0.03367262 -0.24161474 -1.8439283 ]]\n","165 1.7151141 [[ 0.42723587  0.7103587  -0.87356913]\n"," [-0.5883001  -0.12341654  1.6115944 ]\n"," [ 0.03271588 -0.24100779 -1.8435786 ]]\n","166 1.7135088 [[ 0.42572194  0.71012175 -0.87181824]\n"," [-0.5867166  -0.12401589  1.6106102 ]\n"," [ 0.03176142 -0.24040362 -1.8432283 ]]\n","167 1.7119064 [[ 0.4242091   0.70988446 -0.87006813]\n"," [-0.5851359  -0.12461241  1.609626  ]\n"," [ 0.03080924 -0.23980226 -1.8428775 ]]\n","168 1.7103075 [[ 0.4226973   0.7096468  -0.86831874]\n"," [-0.58355796 -0.12520608  1.6086419 ]\n"," [ 0.02985934 -0.23920368 -1.8425262 ]]\n","169 1.7087111 [[ 0.42118657  0.7094089  -0.86657006]\n"," [-0.58198285 -0.12579691  1.6076576 ]\n"," [ 0.02891171 -0.23860787 -1.8421744 ]]\n","170 1.707118 [[ 0.4196769   0.7091706  -0.8648221 ]\n"," [-0.58041054 -0.12638491  1.6066732 ]\n"," [ 0.02796636 -0.23801486 -1.841822  ]]\n","171 1.705528 [[ 0.4181683   0.7089319  -0.86307484]\n"," [-0.57884103 -0.12697008  1.6056889 ]\n"," [ 0.02702327 -0.23742463 -1.8414692 ]]\n","172 1.703941 [[ 0.41666076  0.7086929  -0.8613283 ]\n"," [-0.5772744  -0.1275524   1.6047046 ]\n"," [ 0.02608244 -0.23683718 -1.8411158 ]]\n","173 1.7023569 [[ 0.41515428  0.7084536  -0.8595825 ]\n"," [-0.57571054 -0.1281319   1.6037203 ]\n"," [ 0.02514389 -0.2362525  -1.8407619 ]]\n","174 1.7007757 [[ 0.41364887  0.7082139  -0.8578374 ]\n"," [-0.5741495  -0.12870856  1.6027359 ]\n"," [ 0.02420759 -0.2356706  -1.8404075 ]]\n","175 1.6991981 [[ 0.41214454  0.7079739  -0.85609305]\n"," [-0.57259125 -0.12928239  1.6017514 ]\n"," [ 0.02327355 -0.23509146 -1.8400526 ]]\n","176 1.6976229 [[ 0.41064128  0.7077335  -0.85434943]\n"," [-0.5710358  -0.12985338  1.600767  ]\n"," [ 0.02234177 -0.2345151  -1.8396972 ]]\n","177 1.6960511 [[ 0.4091391   0.7074928  -0.85260653]\n"," [-0.5694832  -0.13042156  1.5997826 ]\n"," [ 0.02141224 -0.23394151 -1.8393413 ]]\n","178 1.6944823 [[ 0.40763795  0.7072518  -0.85086435]\n"," [-0.56793344 -0.13098691  1.5987982 ]\n"," [ 0.02048497 -0.23337068 -1.8389848 ]]\n","179 1.6929163 [[ 0.40613788  0.7070104  -0.8491229 ]\n"," [-0.56638646 -0.13154945  1.5978137 ]\n"," [ 0.01955994 -0.23280261 -1.8386279 ]]\n","180 1.6913534 [[ 0.4046389   0.70676863 -0.8473821 ]\n"," [-0.5648423  -0.13210915  1.5968293 ]\n"," [ 0.01863715 -0.23223731 -1.8382704 ]]\n","181 1.6897936 [[ 0.40314096  0.7065265  -0.84564215]\n"," [-0.5633009  -0.13266604  1.5958449 ]\n"," [ 0.01771661 -0.23167476 -1.8379124 ]]\n","182 1.6882362 [[ 0.4016441   0.7062841  -0.8439029 ]\n"," [-0.5617624  -0.1332201   1.5948604 ]\n"," [ 0.01679831 -0.23111495 -1.837554  ]]\n","183 1.6866825 [[ 0.40014833  0.70604134 -0.84216434]\n"," [-0.5602267  -0.13377136  1.593876  ]\n"," [ 0.01588224 -0.2305579  -1.8371949 ]]\n","184 1.6851313 [[ 0.39865363  0.7057982  -0.8404265 ]\n"," [-0.55869377 -0.13431978  1.5928916 ]\n"," [ 0.01496841 -0.2300036  -1.8368354 ]]\n","185 1.6835835 [[ 0.39716002  0.7055547  -0.83868945]\n"," [-0.55716366 -0.1348654   1.5919071 ]\n"," [ 0.01405681 -0.22945203 -1.8364754 ]]\n","186 1.6820381 [[ 0.3956675   0.70531094 -0.8369531 ]\n"," [-0.55563635 -0.13540821  1.5909227 ]\n"," [ 0.01314744 -0.2289032  -1.8361149 ]]\n","187 1.6804959 [[ 0.39417604  0.7050668  -0.8352175 ]\n"," [-0.5541119  -0.1359482   1.5899383 ]\n"," [ 0.0122403  -0.2283571  -1.8357538 ]]\n","188 1.6789567 [[ 0.39268565  0.7048223  -0.83348256]\n"," [-0.55259025 -0.13648538  1.5889539 ]\n"," [ 0.01133537 -0.22781375 -1.8353922 ]]\n","189 1.6774205 [[ 0.39119634  0.70457745 -0.8317484 ]\n"," [-0.5510714  -0.13701975  1.5879694 ]\n"," [ 0.01043267 -0.22727312 -1.8350302 ]]\n","190 1.6758871 [[ 0.3897081   0.70433223 -0.830015  ]\n"," [-0.54955536 -0.13755132  1.586985  ]\n"," [ 0.00953219 -0.22673522 -1.8346676 ]]\n","191 1.6743566 [[ 0.38822094  0.70408666 -0.8282823 ]\n"," [-0.5480421  -0.13808009  1.5860006 ]\n"," [ 0.00863391 -0.22620004 -1.8343045 ]]\n","192 1.672829 [[ 0.38673487  0.7038408  -0.8265503 ]\n"," [-0.5465317  -0.13860606  1.5850161 ]\n"," [ 0.00773785 -0.22566758 -1.8339409 ]]\n","193 1.6713042 [[ 0.38524988  0.70359457 -0.8248191 ]\n"," [-0.5450241  -0.13912922  1.5840317 ]\n"," [ 0.00684399 -0.22513783 -1.8335768 ]]\n","194 1.6697826 [[ 0.38376597  0.703348   -0.8230886 ]\n"," [-0.5435193  -0.1396496   1.5830473 ]\n"," [ 0.00595235 -0.22461079 -1.8332121 ]]\n","195 1.6682637 [[ 0.38228312  0.70310104 -0.8213588 ]\n"," [-0.54201734 -0.14016718  1.5820628 ]\n"," [ 0.0050629  -0.22408646 -1.832847  ]]\n","196 1.6667476 [[ 0.38080138  0.70285374 -0.8196298 ]\n"," [-0.54051816 -0.14068197  1.5810784 ]\n"," [ 0.00417565 -0.22356485 -1.8324814 ]]\n","197 1.6652346 [[ 0.3793207   0.70260614 -0.8179015 ]\n"," [-0.5390218  -0.14119397  1.580094  ]\n"," [ 0.0032906  -0.22304593 -1.8321153 ]]\n","198 1.6637244 [[ 0.37784111  0.7023582  -0.8161739 ]\n"," [-0.5375282  -0.14170319  1.5791097 ]\n"," [ 0.00240773 -0.22252971 -1.8317486 ]]\n","199 1.6622169 [[ 3.7636262e-01  7.0210987e-01 -8.1444710e-01]\n"," [-5.3603745e-01 -1.4220962e-01  1.5781254e+00]\n"," [ 1.5270615e-03 -2.2201619e-01 -1.8313814e+00]]\n","200 1.6607125 [[ 3.7488520e-01  7.0186120e-01 -8.1272101e-01]\n"," [-5.3454947e-01 -1.4271328e-01  1.5771410e+00]\n"," [ 6.4857677e-04 -2.2150534e-01 -1.8310138e+00]]\n","Prediction: [0 0 1]\n","Accuracy:  0.0\n"],"name":"stdout"}]},{"metadata":{"id":"ddxGZ_2vwYIv","colab_type":"text"},"cell_type":"markdown","source":["## [실습2] 입력 데이터의 정규화의 유무에 따른 모델 학습 성공 유무 확인하기\n","\n","### 1) 코드에 주석을 달아 제출하세요\n","\n","### 2) 데이터 정규화 유무에 따른 모델 학습의 결과를 서술하여 제출하세요.\n","\n","- 입력데이터의 feature 별 값의 스케일 차가 크면, 학습이 이루어지지 않고 발산하기 쉽다. <br> [??] 부분에 xy = min_max_scaler(xy)  코드 넣어, 안정적으로 모델이 학습되는 것을 확인한다."]},{"metadata":{"id":"PJRlDNlowYzX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":17187},"outputId":"888fb618-866f-4f3e-8d58-51ff2d35a71d","executionInfo":{"status":"ok","timestamp":1555641409846,"user_tz":-540,"elapsed":4300,"user":{"displayName":"SOFT HI","photoUrl":"https://lh3.googleusercontent.com/-F3CCHlI0HDc/AAAAAAAAAAI/AAAAAAAAAAw/4rqV-_mvMdg/s64/photo.jpg","userId":"13903510717352952256"}}},"cell_type":"code","source":["\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","tf.set_random_seed(777)  \n","\n","xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n","               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n","               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n","               [816, 820.958984, 1008100, 815.48999, 819.23999],\n","               [819.359985, 823, 1188100, 818.469971, 818.97998],\n","               [819, 823, 1198100, 816, 820.450012],\n","               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n","               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n","\n","\n","# very important. It does not work without it.\n","scaler = MinMaxScaler().fit(xy) # sklearn.preprocessing.MinMaxScaler에 xy를 0,1 범위의 데이터로 정규화한다.\n","xy = scaler.transform(xy) \n","\n","\n","\n","\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","\n","\n","X = tf.placeholder(tf.float32, shape=[None, 4])\n","Y = tf.placeholder(tf.float32, shape=[None, 1])\n","\n","W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n","b = tf.Variable(tf.random_normal([1]), name='bias')\n","\n","hypothesis = tf.matmul(X, W) + b # h(x)= WX+b\n","cost = tf.reduce_mean(tf.square(hypothesis - Y)) # meanSquareError= \\sum{(h(x)-Y)^2}\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5) #learning rate는 10 ** -5 \n","train = optimizer.minimize(cost)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","for step in range(101):\n","    cost_val, hy_val, _ = sess.run(\n","        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data}) \n","    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val) # cost 와 h(x)를 출력한다.\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["0 Cost:  0.49931023 \n","Prediction:\n"," [[1.344498  ]\n"," [2.1943436 ]\n"," [1.4994034 ]\n"," [0.74916124]\n"," [1.1013528 ]\n"," [0.9656155 ]\n"," [0.4695969 ]\n"," [0.6890419 ]]\n","1 Cost:  0.49927858 \n","Prediction:\n"," [[1.3444668 ]\n"," [2.1943111 ]\n"," [1.4993767 ]\n"," [0.74914104]\n"," [1.1013287 ]\n"," [0.96559215]\n"," [0.46958092]\n"," [0.6890255 ]]\n","2 Cost:  0.499247 \n","Prediction:\n"," [[1.3444356 ]\n"," [2.1942787 ]\n"," [1.4993501 ]\n"," [0.7491209 ]\n"," [1.1013048 ]\n"," [0.9655688 ]\n"," [0.46956494]\n"," [0.6890091 ]]\n","3 Cost:  0.4992153 \n","Prediction:\n"," [[1.3444043 ]\n"," [2.1942463 ]\n"," [1.4993232 ]\n"," [0.7491007 ]\n"," [1.1012806 ]\n"," [0.9655455 ]\n"," [0.46954894]\n"," [0.6889926 ]]\n","4 Cost:  0.4991837 \n","Prediction:\n"," [[1.3443732 ]\n"," [2.1942139 ]\n"," [1.4992967 ]\n"," [0.74908054]\n"," [1.1012566 ]\n"," [0.9655221 ]\n"," [0.46953297]\n"," [0.68897617]]\n","5 Cost:  0.4991521 \n","Prediction:\n"," [[1.344342 ]\n"," [2.1941814]\n"," [1.49927  ]\n"," [0.7490604]\n"," [1.1012325]\n"," [0.9654988]\n"," [0.469517 ]\n"," [0.6889598]]\n","6 Cost:  0.49912056 \n","Prediction:\n"," [[1.3443108 ]\n"," [2.1941493 ]\n"," [1.4992434 ]\n"," [0.7490402 ]\n"," [1.1012084 ]\n"," [0.9654755 ]\n"," [0.46950096]\n"," [0.68894327]]\n","7 Cost:  0.4990889 \n","Prediction:\n"," [[1.3442795]\n"," [2.1941168]\n"," [1.4992167]\n"," [0.74902  ]\n"," [1.1011842]\n"," [0.9654522]\n"," [0.469485 ]\n"," [0.6889269]]\n","8 Cost:  0.4990573 \n","Prediction:\n"," [[1.3442484 ]\n"," [2.1940844 ]\n"," [1.49919   ]\n"," [0.74899983]\n"," [1.1011603 ]\n"," [0.9654288 ]\n"," [0.46946904]\n"," [0.6889104 ]]\n","9 Cost:  0.49902558 \n","Prediction:\n"," [[1.3442172 ]\n"," [2.1940517 ]\n"," [1.4991633 ]\n"," [0.7489797 ]\n"," [1.1011362 ]\n"," [0.9654056 ]\n"," [0.46945304]\n"," [0.688894  ]]\n","10 Cost:  0.49899405 \n","Prediction:\n"," [[1.344186  ]\n"," [2.1940193 ]\n"," [1.4991368 ]\n"," [0.7489595 ]\n"," [1.1011122 ]\n"," [0.96538216]\n"," [0.46943706]\n"," [0.6888776 ]]\n","11 Cost:  0.49896234 \n","Prediction:\n"," [[1.3441547 ]\n"," [2.193987  ]\n"," [1.49911   ]\n"," [0.74893934]\n"," [1.101088  ]\n"," [0.9653589 ]\n"," [0.4694211 ]\n"," [0.68886113]]\n","12 Cost:  0.49893072 \n","Prediction:\n"," [[1.3441236 ]\n"," [2.1939545 ]\n"," [1.4990834 ]\n"," [0.74891907]\n"," [1.101064  ]\n"," [0.9653355 ]\n"," [0.46940514]\n"," [0.6888446 ]]\n","13 Cost:  0.49889916 \n","Prediction:\n"," [[1.3440924 ]\n"," [2.193922  ]\n"," [1.4990567 ]\n"," [0.748899  ]\n"," [1.10104   ]\n"," [0.96531224]\n"," [0.46938914]\n"," [0.6888283 ]]\n","14 Cost:  0.4988675 \n","Prediction:\n"," [[1.3440611 ]\n"," [2.1938896 ]\n"," [1.49903   ]\n"," [0.7488787 ]\n"," [1.1010159 ]\n"," [0.9652888 ]\n"," [0.46937314]\n"," [0.6888118 ]]\n","15 Cost:  0.49883592 \n","Prediction:\n"," [[1.3440299 ]\n"," [2.1938572 ]\n"," [1.4990034 ]\n"," [0.74885863]\n"," [1.1009917 ]\n"," [0.9652655 ]\n"," [0.4693572 ]\n"," [0.6887953 ]]\n","16 Cost:  0.49880427 \n","Prediction:\n"," [[1.3439988]\n"," [2.1938248]\n"," [1.4989766]\n"," [0.7488384]\n"," [1.1009678]\n"," [0.9652422]\n"," [0.4693412]\n"," [0.688779 ]]\n","17 Cost:  0.4987727 \n","Prediction:\n"," [[1.3439676 ]\n"," [2.1937923 ]\n"," [1.4989501 ]\n"," [0.7488183 ]\n"," [1.1009437 ]\n"," [0.9652189 ]\n"," [0.46932518]\n"," [0.6887625 ]]\n","18 Cost:  0.498741 \n","Prediction:\n"," [[1.3439363 ]\n"," [2.1937597 ]\n"," [1.4989234 ]\n"," [0.7487981 ]\n"," [1.1009196 ]\n"," [0.96519566]\n"," [0.46930924]\n"," [0.68874604]]\n","19 Cost:  0.49870944 \n","Prediction:\n"," [[1.3439052 ]\n"," [2.1937273 ]\n"," [1.4988968 ]\n"," [0.7487779 ]\n"," [1.1008956 ]\n"," [0.96517223]\n"," [0.46929324]\n"," [0.68872964]]\n","20 Cost:  0.4986779 \n","Prediction:\n"," [[1.3438741 ]\n"," [2.193695  ]\n"," [1.4988701 ]\n"," [0.7487577 ]\n"," [1.1008716 ]\n"," [0.9651489 ]\n"," [0.46927726]\n"," [0.6887132 ]]\n","21 Cost:  0.49864632 \n","Prediction:\n"," [[1.3438427 ]\n"," [2.1936626 ]\n"," [1.4988434 ]\n"," [0.74873763]\n"," [1.1008475 ]\n"," [0.9651256 ]\n"," [0.4692613 ]\n"," [0.68869674]]\n","22 Cost:  0.49861458 \n","Prediction:\n"," [[1.3438115 ]\n"," [2.19363   ]\n"," [1.4988167 ]\n"," [0.74871737]\n"," [1.1008234 ]\n"," [0.9651023 ]\n"," [0.46924534]\n"," [0.68868035]]\n","23 Cost:  0.4985831 \n","Prediction:\n"," [[1.3437804 ]\n"," [2.1935978 ]\n"," [1.4987901 ]\n"," [0.7486973 ]\n"," [1.1007994 ]\n"," [0.96507895]\n"," [0.46922934]\n"," [0.6886639 ]]\n","24 Cost:  0.4985516 \n","Prediction:\n"," [[1.3437493 ]\n"," [2.1935656 ]\n"," [1.4987636 ]\n"," [0.7486771 ]\n"," [1.1007754 ]\n"," [0.9650557 ]\n"," [0.4692134 ]\n"," [0.68864745]]\n","25 Cost:  0.49852002 \n","Prediction:\n"," [[1.343718  ]\n"," [2.1935332 ]\n"," [1.4987369 ]\n"," [0.7486569 ]\n"," [1.1007514 ]\n"," [0.9650323 ]\n"," [0.4691974 ]\n"," [0.68863106]]\n","26 Cost:  0.49848828 \n","Prediction:\n"," [[1.3436867 ]\n"," [2.1935005 ]\n"," [1.49871   ]\n"," [0.7486367 ]\n"," [1.1007272 ]\n"," [0.96500903]\n"," [0.46918142]\n"," [0.6886146 ]]\n","27 Cost:  0.49845675 \n","Prediction:\n"," [[1.3436556 ]\n"," [2.193468  ]\n"," [1.4986836 ]\n"," [0.74861664]\n"," [1.1007031 ]\n"," [0.96498567]\n"," [0.46916544]\n"," [0.68859816]]\n","28 Cost:  0.49842522 \n","Prediction:\n"," [[1.3436245 ]\n"," [2.193436  ]\n"," [1.4986569 ]\n"," [0.7485964 ]\n"," [1.100679  ]\n"," [0.96496236]\n"," [0.4691495 ]\n"," [0.68858176]]\n","29 Cost:  0.49839357 \n","Prediction:\n"," [[1.3435932]\n"," [2.1934032]\n"," [1.4986303]\n"," [0.7485763]\n"," [1.1006551]\n"," [0.9649391]\n"," [0.4691335]\n"," [0.6885653]]\n","30 Cost:  0.49836195 \n","Prediction:\n"," [[1.343562  ]\n"," [2.1933708 ]\n"," [1.4986036 ]\n"," [0.7485561 ]\n"," [1.100631  ]\n"," [0.96491575]\n"," [0.46911752]\n"," [0.68854886]]\n","31 Cost:  0.4983304 \n","Prediction:\n"," [[1.3435309 ]\n"," [2.1933384 ]\n"," [1.498577  ]\n"," [0.74853593]\n"," [1.1006069 ]\n"," [0.96489245]\n"," [0.46910158]\n"," [0.6885325 ]]\n","32 Cost:  0.4982988 \n","Prediction:\n"," [[1.3434998]\n"," [2.193306 ]\n"," [1.4985503]\n"," [0.7485157]\n"," [1.100583 ]\n"," [0.9648691]\n"," [0.4690856]\n"," [0.688516 ]]\n","33 Cost:  0.4982672 \n","Prediction:\n"," [[1.3434684 ]\n"," [2.1932735 ]\n"," [1.4985236 ]\n"," [0.74849564]\n"," [1.1005588 ]\n"," [0.9648458 ]\n"," [0.4690696 ]\n"," [0.6884996 ]]\n","34 Cost:  0.4982356 \n","Prediction:\n"," [[1.3434372 ]\n"," [2.1932411 ]\n"," [1.498497  ]\n"," [0.74847543]\n"," [1.1005347 ]\n"," [0.9648225 ]\n"," [0.46905366]\n"," [0.68848324]]\n","35 Cost:  0.49820405 \n","Prediction:\n"," [[1.3434061 ]\n"," [2.1932087 ]\n"," [1.4984703 ]\n"," [0.7484553 ]\n"," [1.1005107 ]\n"," [0.96479917]\n"," [0.4690377 ]\n"," [0.6884667 ]]\n","36 Cost:  0.4981725 \n","Prediction:\n"," [[1.343375  ]\n"," [2.1931763 ]\n"," [1.4984437 ]\n"," [0.7484351 ]\n"," [1.1004866 ]\n"," [0.9647758 ]\n"," [0.4690217 ]\n"," [0.68845034]]\n","37 Cost:  0.49814102 \n","Prediction:\n"," [[1.3433437 ]\n"," [2.193144  ]\n"," [1.498417  ]\n"," [0.748415  ]\n"," [1.1004627 ]\n"," [0.96475255]\n"," [0.46900573]\n"," [0.68843395]]\n","38 Cost:  0.49810934 \n","Prediction:\n"," [[1.3433125 ]\n"," [2.1931114 ]\n"," [1.4983904 ]\n"," [0.7483948 ]\n"," [1.1004386 ]\n"," [0.9647292 ]\n"," [0.46898976]\n"," [0.6884175 ]]\n","39 Cost:  0.49807778 \n","Prediction:\n"," [[1.3432814 ]\n"," [2.193079  ]\n"," [1.4983637 ]\n"," [0.74837464]\n"," [1.1004145 ]\n"," [0.9647059 ]\n"," [0.46897385]\n"," [0.68840104]]\n","40 Cost:  0.49804622 \n","Prediction:\n"," [[1.3432503 ]\n"," [2.1930466 ]\n"," [1.4983371 ]\n"," [0.74835443]\n"," [1.1003906 ]\n"," [0.9646826 ]\n"," [0.4689578 ]\n"," [0.68838465]]\n","41 Cost:  0.49801463 \n","Prediction:\n"," [[1.3432189 ]\n"," [2.1930141 ]\n"," [1.4983104 ]\n"," [0.74833435]\n"," [1.1003664 ]\n"," [0.9646593 ]\n"," [0.46894187]\n"," [0.6883682 ]]\n","42 Cost:  0.49798313 \n","Prediction:\n"," [[1.3431877 ]\n"," [2.192982  ]\n"," [1.4982837 ]\n"," [0.7483142 ]\n"," [1.1003424 ]\n"," [0.96463597]\n"," [0.4689259 ]\n"," [0.6883518 ]]\n","43 Cost:  0.49795157 \n","Prediction:\n"," [[1.3431566 ]\n"," [2.1929495 ]\n"," [1.498257  ]\n"," [0.748294  ]\n"," [1.1003183 ]\n"," [0.9646126 ]\n"," [0.46890995]\n"," [0.68833536]]\n","44 Cost:  0.49792004 \n","Prediction:\n"," [[1.3431255 ]\n"," [2.192917  ]\n"," [1.4982306 ]\n"," [0.74827385]\n"," [1.1002942 ]\n"," [0.96458936]\n"," [0.46889398]\n"," [0.6883189 ]]\n","45 Cost:  0.49788848 \n","Prediction:\n"," [[1.3430942]\n"," [2.1928847]\n"," [1.4982039]\n"," [0.7482537]\n"," [1.1002703]\n"," [0.964566 ]\n"," [0.468878 ]\n"," [0.6883025]]\n","46 Cost:  0.49785683 \n","Prediction:\n"," [[1.343063  ]\n"," [2.192852  ]\n"," [1.4981773 ]\n"," [0.74823356]\n"," [1.1002462 ]\n"," [0.9645427 ]\n"," [0.46886203]\n"," [0.6882861 ]]\n","47 Cost:  0.4978254 \n","Prediction:\n"," [[1.3430319 ]\n"," [2.1928198 ]\n"," [1.4981507 ]\n"," [0.7482134 ]\n"," [1.1002222 ]\n"," [0.9645194 ]\n"," [0.46884608]\n"," [0.6882697 ]]\n","48 Cost:  0.49779385 \n","Prediction:\n"," [[1.3430008 ]\n"," [2.1927874 ]\n"," [1.498124  ]\n"," [0.74819326]\n"," [1.1001981 ]\n"," [0.96449614]\n"," [0.46883008]\n"," [0.6882533 ]]\n","49 Cost:  0.49776214 \n","Prediction:\n"," [[1.3429695 ]\n"," [2.1927547 ]\n"," [1.4980973 ]\n"," [0.74817306]\n"," [1.1001741 ]\n"," [0.9644727 ]\n"," [0.46881416]\n"," [0.68823683]]\n","50 Cost:  0.49773067 \n","Prediction:\n"," [[1.3429383 ]\n"," [2.1927226 ]\n"," [1.4980707 ]\n"," [0.7481529 ]\n"," [1.10015   ]\n"," [0.96444947]\n"," [0.4687982 ]\n"," [0.6882204 ]]\n","51 Cost:  0.4976992 \n","Prediction:\n"," [[1.3429072 ]\n"," [2.1926901 ]\n"," [1.4980441 ]\n"," [0.74813277]\n"," [1.100126  ]\n"," [0.9644262 ]\n"," [0.46878222]\n"," [0.688204  ]]\n","52 Cost:  0.4976676 \n","Prediction:\n"," [[1.3428761 ]\n"," [2.1926577 ]\n"," [1.4980174 ]\n"," [0.7481126 ]\n"," [1.100102  ]\n"," [0.96440285]\n"," [0.46876624]\n"," [0.6881876 ]]\n","53 Cost:  0.49763602 \n","Prediction:\n"," [[1.3428447 ]\n"," [2.1926253 ]\n"," [1.4979907 ]\n"," [0.7480925 ]\n"," [1.1000779 ]\n"," [0.96437955]\n"," [0.4687503 ]\n"," [0.68817115]]\n","54 Cost:  0.4976045 \n","Prediction:\n"," [[1.3428135 ]\n"," [2.1925929 ]\n"," [1.4979641 ]\n"," [0.7480723 ]\n"," [1.1000538 ]\n"," [0.96435624]\n"," [0.46873435]\n"," [0.68815476]]\n","55 Cost:  0.49757293 \n","Prediction:\n"," [[1.3427824 ]\n"," [2.1925604 ]\n"," [1.4979374 ]\n"," [0.7480522 ]\n"," [1.1000297 ]\n"," [0.96433294]\n"," [0.46871838]\n"," [0.68813837]]\n","56 Cost:  0.49754137 \n","Prediction:\n"," [[1.3427513 ]\n"," [2.192528  ]\n"," [1.4979109 ]\n"," [0.74803203]\n"," [1.1000056 ]\n"," [0.96430963]\n"," [0.46870238]\n"," [0.688122  ]]\n","57 Cost:  0.49750984 \n","Prediction:\n"," [[1.34272   ]\n"," [2.1924956 ]\n"," [1.4978842 ]\n"," [0.7480118 ]\n"," [1.0999817 ]\n"," [0.9642863 ]\n"," [0.46868643]\n"," [0.68810546]]\n","58 Cost:  0.4974783 \n","Prediction:\n"," [[1.3426888 ]\n"," [2.1924632 ]\n"," [1.4978576 ]\n"," [0.7479917 ]\n"," [1.0999576 ]\n"," [0.96426296]\n"," [0.46867052]\n"," [0.6880891 ]]\n","59 Cost:  0.49744678 \n","Prediction:\n"," [[1.3426577 ]\n"," [2.1924307 ]\n"," [1.4978309 ]\n"," [0.74797153]\n"," [1.0999336 ]\n"," [0.9642397 ]\n"," [0.46865457]\n"," [0.6880727 ]]\n","60 Cost:  0.49741533 \n","Prediction:\n"," [[1.3426266 ]\n"," [2.1923985 ]\n"," [1.4978043 ]\n"," [0.7479514 ]\n"," [1.0999095 ]\n"," [0.9642165 ]\n"," [0.46863857]\n"," [0.68805623]]\n","61 Cost:  0.4973837 \n","Prediction:\n"," [[1.3425953 ]\n"," [2.192366  ]\n"," [1.4977777 ]\n"," [0.74793124]\n"," [1.0998856 ]\n"," [0.9641931 ]\n"," [0.4686226 ]\n"," [0.68803984]]\n","62 Cost:  0.49735215 \n","Prediction:\n"," [[1.3425641 ]\n"," [2.1923335 ]\n"," [1.497751  ]\n"," [0.7479111 ]\n"," [1.0998614 ]\n"," [0.96416986]\n"," [0.46860665]\n"," [0.68802345]]\n","63 Cost:  0.49732074 \n","Prediction:\n"," [[1.342533  ]\n"," [2.1923013 ]\n"," [1.4977244 ]\n"," [0.74789095]\n"," [1.0998375 ]\n"," [0.9641465 ]\n"," [0.46859068]\n"," [0.688007  ]]\n","64 Cost:  0.4972893 \n","Prediction:\n"," [[1.3425019 ]\n"," [2.192269  ]\n"," [1.4976977 ]\n"," [0.7478708 ]\n"," [1.0998135 ]\n"," [0.96412325]\n"," [0.46857473]\n"," [0.6879906 ]]\n","65 Cost:  0.49725768 \n","Prediction:\n"," [[1.3424706]\n"," [2.1922364]\n"," [1.4976711]\n"," [0.7478507]\n"," [1.0997894]\n"," [0.9640999]\n"," [0.4685588]\n"," [0.6879742]]\n","66 Cost:  0.49722612 \n","Prediction:\n"," [[1.3424394 ]\n"," [2.192204  ]\n"," [1.4976444 ]\n"," [0.7478305 ]\n"," [1.0997653 ]\n"," [0.9640766 ]\n"," [0.46854284]\n"," [0.68795776]]\n","67 Cost:  0.49719462 \n","Prediction:\n"," [[1.3424083 ]\n"," [2.1921716 ]\n"," [1.4976178 ]\n"," [0.7478104 ]\n"," [1.0997413 ]\n"," [0.9640533 ]\n"," [0.46852684]\n"," [0.6879414 ]]\n","68 Cost:  0.49716318 \n","Prediction:\n"," [[1.3423772 ]\n"," [2.1921394 ]\n"," [1.4975913 ]\n"," [0.7477902 ]\n"," [1.0997173 ]\n"," [0.96402997]\n"," [0.46851093]\n"," [0.687925  ]]\n","69 Cost:  0.49713165 \n","Prediction:\n"," [[1.342346  ]\n"," [2.192107  ]\n"," [1.4975646 ]\n"," [0.74777013]\n"," [1.0996932 ]\n"," [0.96400666]\n"," [0.46849498]\n"," [0.68790853]]\n","70 Cost:  0.49710017 \n","Prediction:\n"," [[1.3423147 ]\n"," [2.1920745 ]\n"," [1.4975381 ]\n"," [0.7477499 ]\n"," [1.0996692 ]\n"," [0.9639834 ]\n"," [0.468479  ]\n"," [0.68789214]]\n","71 Cost:  0.4970686 \n","Prediction:\n"," [[1.3422836 ]\n"," [2.192042  ]\n"," [1.4975114 ]\n"," [0.74772984]\n"," [1.0996451 ]\n"," [0.9639601 ]\n"," [0.46846306]\n"," [0.68787575]]\n","72 Cost:  0.49703708 \n","Prediction:\n"," [[1.3422525 ]\n"," [2.1920097 ]\n"," [1.4974847 ]\n"," [0.74770963]\n"," [1.099621  ]\n"," [0.9639368 ]\n"," [0.4684471 ]\n"," [0.6878593 ]]\n","73 Cost:  0.49700564 \n","Prediction:\n"," [[1.3422213 ]\n"," [2.1919773 ]\n"," [1.4974582 ]\n"," [0.74768955]\n"," [1.0995971 ]\n"," [0.96391356]\n"," [0.46843114]\n"," [0.6878429 ]]\n","74 Cost:  0.49697408 \n","Prediction:\n"," [[1.34219  ]\n"," [2.1919448]\n"," [1.4974315]\n"," [0.7476694]\n"," [1.099573 ]\n"," [0.9638902]\n"," [0.4684152]\n"," [0.6878265]]\n","75 Cost:  0.49694255 \n","Prediction:\n"," [[1.3421589 ]\n"," [2.1919124 ]\n"," [1.4974048 ]\n"," [0.74764925]\n"," [1.099549  ]\n"," [0.96386695]\n"," [0.46839926]\n"," [0.68781006]]\n","76 Cost:  0.49691102 \n","Prediction:\n"," [[1.3421278 ]\n"," [2.19188   ]\n"," [1.4973781 ]\n"," [0.74762905]\n"," [1.099525  ]\n"," [0.9638436 ]\n"," [0.46838334]\n"," [0.6877936 ]]\n","77 Cost:  0.49687952 \n","Prediction:\n"," [[1.3420966 ]\n"," [2.1918476 ]\n"," [1.4973515 ]\n"," [0.7476089 ]\n"," [1.0995009 ]\n"," [0.96382034]\n"," [0.46836737]\n"," [0.6877773 ]]\n","78 Cost:  0.49684802 \n","Prediction:\n"," [[1.3420653 ]\n"," [2.1918151 ]\n"," [1.497325  ]\n"," [0.7475889 ]\n"," [1.0994768 ]\n"," [0.96379703]\n"," [0.46835142]\n"," [0.6877609 ]]\n","79 Cost:  0.49681652 \n","Prediction:\n"," [[1.3420342 ]\n"," [2.1917827 ]\n"," [1.4972982 ]\n"," [0.74756867]\n"," [1.0994529 ]\n"," [0.9637737 ]\n"," [0.46833545]\n"," [0.68774456]]\n","80 Cost:  0.49678504 \n","Prediction:\n"," [[1.3420031 ]\n"," [2.1917503 ]\n"," [1.4972717 ]\n"," [0.7475486 ]\n"," [1.0994289 ]\n"," [0.9637504 ]\n"," [0.46831948]\n"," [0.6877281 ]]\n","81 Cost:  0.4967535 \n","Prediction:\n"," [[1.3419719 ]\n"," [2.1917179 ]\n"," [1.4972451 ]\n"," [0.7475284 ]\n"," [1.0994048 ]\n"," [0.9637271 ]\n"," [0.46830356]\n"," [0.68771166]]\n","82 Cost:  0.4967221 \n","Prediction:\n"," [[1.3419406 ]\n"," [2.1916857 ]\n"," [1.4972184 ]\n"," [0.7475083 ]\n"," [1.0993807 ]\n"," [0.96370393]\n"," [0.46828765]\n"," [0.6876953 ]]\n","83 Cost:  0.49669063 \n","Prediction:\n"," [[1.3419096 ]\n"," [2.1916533 ]\n"," [1.4971919 ]\n"," [0.7474881 ]\n"," [1.0993568 ]\n"," [0.96368057]\n"," [0.46827164]\n"," [0.6876789 ]]\n","84 Cost:  0.49665916 \n","Prediction:\n"," [[1.3418785 ]\n"," [2.1916208 ]\n"," [1.4971653 ]\n"," [0.747468  ]\n"," [1.0993328 ]\n"," [0.96365726]\n"," [0.4682557 ]\n"," [0.6876624 ]]\n","85 Cost:  0.49662763 \n","Prediction:\n"," [[1.3418473 ]\n"," [2.1915884 ]\n"," [1.4971386 ]\n"," [0.7474478 ]\n"," [1.0993087 ]\n"," [0.96363395]\n"," [0.4682398 ]\n"," [0.6876461 ]]\n","86 Cost:  0.49659616 \n","Prediction:\n"," [[1.3418161 ]\n"," [2.191556  ]\n"," [1.497112  ]\n"," [0.7474277 ]\n"," [1.0992848 ]\n"," [0.9636107 ]\n"," [0.46822384]\n"," [0.68762964]]\n","87 Cost:  0.49656475 \n","Prediction:\n"," [[1.341785  ]\n"," [2.1915238 ]\n"," [1.4970853 ]\n"," [0.74740756]\n"," [1.0992607 ]\n"," [0.9635874 ]\n"," [0.46820787]\n"," [0.68761325]]\n","88 Cost:  0.49653324 \n","Prediction:\n"," [[1.3417538 ]\n"," [2.1914914 ]\n"," [1.4970587 ]\n"," [0.74738747]\n"," [1.0992366 ]\n"," [0.96356416]\n"," [0.46819195]\n"," [0.6875969 ]]\n","89 Cost:  0.49650165 \n","Prediction:\n"," [[1.3417226 ]\n"," [2.1914587 ]\n"," [1.497032  ]\n"," [0.74736726]\n"," [1.0992126 ]\n"," [0.9635408 ]\n"," [0.468176  ]\n"," [0.68758047]]\n","90 Cost:  0.4964703 \n","Prediction:\n"," [[1.3416914 ]\n"," [2.1914268 ]\n"," [1.4970055 ]\n"," [0.7473472 ]\n"," [1.0991884 ]\n"," [0.96351755]\n"," [0.46816003]\n"," [0.687564  ]]\n","91 Cost:  0.4964388 \n","Prediction:\n"," [[1.3416603 ]\n"," [2.1913943 ]\n"," [1.4969788 ]\n"," [0.74732697]\n"," [1.0991646 ]\n"," [0.9634942 ]\n"," [0.46814412]\n"," [0.6875477 ]]\n","92 Cost:  0.49640733 \n","Prediction:\n"," [[1.3416291 ]\n"," [2.191362  ]\n"," [1.4969522 ]\n"," [0.74730694]\n"," [1.0991405 ]\n"," [0.96347094]\n"," [0.46812817]\n"," [0.68753123]]\n","93 Cost:  0.49637583 \n","Prediction:\n"," [[1.3415979 ]\n"," [2.1913295 ]\n"," [1.4969255 ]\n"," [0.74728674]\n"," [1.0991164 ]\n"," [0.96344775]\n"," [0.4681122 ]\n"," [0.68751484]]\n","94 Cost:  0.49634445 \n","Prediction:\n"," [[1.3415669 ]\n"," [2.191297  ]\n"," [1.496899  ]\n"," [0.7472667 ]\n"," [1.0990925 ]\n"," [0.96342444]\n"," [0.4680963 ]\n"," [0.6874985 ]]\n","95 Cost:  0.49631307 \n","Prediction:\n"," [[1.3415359 ]\n"," [2.1912646 ]\n"," [1.4968727 ]\n"," [0.74724656]\n"," [1.0990686 ]\n"," [0.96340126]\n"," [0.4680804 ]\n"," [0.68748206]]\n","96 Cost:  0.49628174 \n","Prediction:\n"," [[1.3415048 ]\n"," [2.1912327 ]\n"," [1.496846  ]\n"," [0.74722654]\n"," [1.0990447 ]\n"," [0.963378  ]\n"," [0.4680645 ]\n"," [0.6874656 ]]\n","97 Cost:  0.49625033 \n","Prediction:\n"," [[1.3414738 ]\n"," [2.1912003 ]\n"," [1.4968195 ]\n"," [0.74720645]\n"," [1.0990207 ]\n"," [0.96335477]\n"," [0.46804857]\n"," [0.6874493 ]]\n","98 Cost:  0.49621892 \n","Prediction:\n"," [[1.3414428 ]\n"," [2.1911678 ]\n"," [1.4967929 ]\n"," [0.74718636]\n"," [1.0989968 ]\n"," [0.9633316 ]\n"," [0.46803263]\n"," [0.6874329 ]]\n","99 Cost:  0.49618757 \n","Prediction:\n"," [[1.3414117 ]\n"," [2.1911356 ]\n"," [1.4967663 ]\n"," [0.7471663 ]\n"," [1.0989728 ]\n"," [0.96330833]\n"," [0.46801674]\n"," [0.68741655]]\n","100 Cost:  0.49615628 \n","Prediction:\n"," [[1.3413806 ]\n"," [2.1911035 ]\n"," [1.4967399 ]\n"," [0.74714625]\n"," [1.098949  ]\n"," [0.9632851 ]\n"," [0.46800086]\n"," [0.68740016]]\n"],"name":"stdout"}]},{"metadata":{"id":"SkM5_iKgxYB4","colab_type":"text"},"cell_type":"markdown","source":["## [실습3] mnist 데이터 셋으로 손글씨 데이터 분류하기\n","\n","\n","- 입력데이터는 28x28 영상 데이터를 784차원의 1차원 백터로 사용\n","- 출력데이터는 숫자 정답\n","\n","### 1) 손글씨 데이터 정확도 올리기? 어떻게?\n","- parameters인 num_epochs와 batch_size를 변경해보자!!\n","  - num_epochs = 50, batch_size = 100\n","\n","### 2) 손글씨 데이터 정확도 올리기? 아이디어 있으면 고고!\n","\n","\n"]},{"metadata":{"id":"0KZqjBvcxYcp","colab_type":"code","outputId":"dfb36983-bdd2-4276-b826-ad60924fe4fa","executionInfo":{"status":"ok","timestamp":1555642481468,"user_tz":-540,"elapsed":80853,"user":{"displayName":"SOFT HI","photoUrl":"https://lh3.googleusercontent.com/-F3CCHlI0HDc/AAAAAAAAAAI/AAAAAAAAAAw/4rqV-_mvMdg/s64/photo.jpg","userId":"13903510717352952256"}},"colab":{"base_uri":"https://localhost:8080/","height":2122}},"cell_type":"code","source":["'''\n","AdamOptimizer(0.001) -> nan\n","RMSPropOptimizer(0.001) -> nan\n","RMSPropOptimizer(0.0001) -> nan\n","\n","질문.\n","Adam과 RMSProp은 learning_rate가 안정적으로 줄어들어도 어느 순간부터 cost가 nan으로 출력되며 학습이 도중에 제대로 이루어지지않습니다.\n","GradientDescent는 learning_rate = 0.05 에서 안정적으로 학습이 됩니다.\n","반면 다른 Optimizer들은 더 낮은 learningrate(=0.001 or 0.0001)에서도 학습이 제대로 이루어지지 않는지 궁금합니다.\n","'''\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import random\n","\n","tf.set_random_seed(777)  \n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n","\n","\n","nb_classes = 10\n","\n","# MNIST data image of shape 28 * 28 = 784\n","X = tf.placeholder(tf.float32, [None, 784])\n","\n","# 0 - 9 digits recognition = 10 classes\n","Y = tf.placeholder(tf.float32, [None, nb_classes])\n","\n","W = tf.Variable(tf.random_normal([784, nb_classes]))\n","b = tf.Variable(tf.random_normal([nb_classes]))\n","\n","\n","\n","# Hypothesis (using softmax)\n","hypothesis = tf.nn.softmax(tf.matmul(X, W) + b) \n","cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n","train = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(cost)\n","\n","\n","\n","# Test model\n","is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n","\n","# parameters\n","num_epochs = 100\n","batch_size = 100\n","num_iterations = int(mnist.train.num_examples / batch_size) # iteration 횟수 지정 examples/batch_size\n","\n","\n","# 배치란?\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    print('variables intializer complete!')\n","    # Training cycle\n","    for epoch in range(num_epochs):\n","        avg_cost = 0\n","\n","        for i in range(num_iterations):\n","            batch_xs, batch_ys = mnist.train.next_batch(batch_size) # 배치사이즈만큼 xs,ys에 저장\n","            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n","            avg_cost += cost_val / num_iterations # sum(배치 cost 평균 / 배치 횟수) = epoch당 cost 평균\n","\n","        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n","\n","    print(\"Learning finished\")\n","\n","    # Test the model using test sets\n","    print(\n","        \"Accuracy: \",\n","        accuracy.eval(\n","            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n","        ),\n","    )\n","\n","    # Get one and predict\n","    r = random.randint(0, mnist.test.num_examples - 1)\n","    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n","    print(\n","        \"Prediction: \",\n","        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n","    )\n","\n","    plt.imshow(\n","        mnist.test.images[r : r + 1].reshape(28, 28),\n","        cmap=\"Greys\", \n","        interpolation=\"nearest\", # nearest interploation\n","    )\n","    plt.show()\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Extracting MNIST_data/train-images-idx3-ubyte.gz\n","Extracting MNIST_data/train-labels-idx1-ubyte.gz\n","Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n","Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n","variables intializer complete!\n","Epoch: 0001, Cost: 3.632310482\n","Epoch: 0002, Cost: 1.567251844\n","Epoch: 0003, Cost: 1.188099505\n","Epoch: 0004, Cost: 1.013124018\n","Epoch: 0005, Cost: 0.908733001\n","Epoch: 0006, Cost: 0.836826925\n","Epoch: 0007, Cost: 0.784094668\n","Epoch: 0008, Cost: 0.742479029\n","Epoch: 0009, Cost: 0.708860960\n","Epoch: 0010, Cost: 0.680765827\n","Epoch: 0011, Cost: 0.656887213\n","Epoch: 0012, Cost: 0.635973536\n","Epoch: 0013, Cost: 0.617944387\n","Epoch: 0014, Cost: 0.601943201\n","Epoch: 0015, Cost: 0.586977734\n","Epoch: 0016, Cost: 0.573781239\n","Epoch: 0017, Cost: 0.562178803\n","Epoch: 0018, Cost: 0.551028942\n","Epoch: 0019, Cost: 0.540923092\n","Epoch: 0020, Cost: 0.531837326\n","Epoch: 0021, Cost: 0.523049246\n","Epoch: 0022, Cost: 0.515014651\n","Epoch: 0023, Cost: 0.507548864\n","Epoch: 0024, Cost: 0.500380666\n","Epoch: 0025, Cost: 0.493660823\n","Epoch: 0026, Cost: 0.487726012\n","Epoch: 0027, Cost: 0.481895704\n","Epoch: 0028, Cost: 0.476215121\n","Epoch: 0029, Cost: 0.470778357\n","Epoch: 0030, Cost: 0.465936476\n","Epoch: 0031, Cost: 0.460942360\n","Epoch: 0032, Cost: 0.456601595\n","Epoch: 0033, Cost: 0.452098239\n","Epoch: 0034, Cost: 0.447947274\n","Epoch: 0035, Cost: 0.443888483\n","Epoch: 0036, Cost: 0.440086919\n","Epoch: 0037, Cost: 0.436301277\n","Epoch: 0038, Cost: 0.432934334\n","Epoch: 0039, Cost: 0.429199499\n","Epoch: 0040, Cost: 0.425957390\n","Epoch: 0041, Cost: 0.422945829\n","Epoch: 0042, Cost: 0.419914876\n","Epoch: 0043, Cost: 0.416887808\n","Epoch: 0044, Cost: 0.413890646\n","Epoch: 0045, Cost: 0.411106912\n","Epoch: 0046, Cost: 0.408613135\n","Epoch: 0047, Cost: 0.405857182\n","Epoch: 0048, Cost: 0.403326817\n","Epoch: 0049, Cost: 0.400921016\n","Epoch: 0050, Cost: 0.398703745\n","Epoch: 0051, Cost: 0.396275018\n","Epoch: 0052, Cost: 0.393997022\n","Epoch: 0053, Cost: 0.391799245\n","Epoch: 0054, Cost: 0.389531576\n","Epoch: 0055, Cost: 0.387694446\n","Epoch: 0056, Cost: 0.385296505\n","Epoch: 0057, Cost: 0.383596284\n","Epoch: 0058, Cost: 0.381539539\n","Epoch: 0059, Cost: 0.379622334\n","Epoch: 0060, Cost: 0.377778756\n","Epoch: 0061, Cost: 0.376029341\n","Epoch: 0062, Cost: 0.374107845\n","Epoch: 0063, Cost: 0.372587256\n","Epoch: 0064, Cost: 0.370710549\n","Epoch: 0065, Cost: 0.369138543\n","Epoch: 0066, Cost: 0.367663884\n","Epoch: 0067, Cost: 0.366032379\n","Epoch: 0068, Cost: 0.364517275\n","Epoch: 0069, Cost: 0.363090833\n","Epoch: 0070, Cost: 0.361488374\n","Epoch: 0071, Cost: 0.360217642\n","Epoch: 0072, Cost: 0.358521960\n","Epoch: 0073, Cost: 0.357300029\n","Epoch: 0074, Cost: 0.356014644\n","Epoch: 0075, Cost: 0.354703320\n","Epoch: 0076, Cost: 0.353215735\n","Epoch: 0077, Cost: 0.351989861\n","Epoch: 0078, Cost: 0.350781485\n","Epoch: 0079, Cost: 0.349527196\n","Epoch: 0080, Cost: 0.348266906\n","Epoch: 0081, Cost: 0.347026459\n","Epoch: 0082, Cost: 0.345954090\n","Epoch: 0083, Cost: 0.344618342\n","Epoch: 0084, Cost: 0.343596961\n","Epoch: 0085, Cost: 0.342501580\n","Epoch: 0086, Cost: 0.341507355\n","Epoch: 0087, Cost: 0.340384237\n","Epoch: 0088, Cost: 0.339292208\n","Epoch: 0089, Cost: 0.338284070\n","Epoch: 0090, Cost: 0.337387133\n","Epoch: 0091, Cost: 0.336287959\n","Epoch: 0092, Cost: 0.335263284\n","Epoch: 0093, Cost: 0.334354405\n","Epoch: 0094, Cost: 0.333350306\n","Epoch: 0095, Cost: 0.332489305\n","Epoch: 0096, Cost: 0.331528912\n","Epoch: 0097, Cost: 0.330669628\n","Epoch: 0098, Cost: 0.329697516\n","Epoch: 0099, Cost: 0.328815490\n","Epoch: 0100, Cost: 0.328065478\n","Learning finished\n","Accuracy:  0.9104\n","Label:  [6]\n","Prediction:  [6]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADXlJREFUeJzt3X+oXPWZx/HPo22Q2BDMZjYGY/Z2\ny2VBjJuW4bKiLF3cFqvBWARtkOUulE3ACnshqMEKq39oRGxLkCVwq5ekS9d0IRUjym5sKGhgLY7i\nan5sjCu3NCH3ZmIKSRCpJs/+MSflqne+M845Z865fd4vuNyZ88yZ82SST86c850zX3N3AYjnkqob\nAFANwg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKgvDXNjy5cv95GRkWFuEghlenpap06dsn4e\nmyv8ZnazpG2SLpX0tLs/nnr8yMiIWq1Wnk0CSGg2m30/duC3/WZ2qaR/lfQdSddI2mBm1wz6fACG\nK88x/5ik99z9fXf/g6RdktYX0xaAsuUJ/1WSfjfn/rFs2aeY2UYza5lZq91u59gcgCKVfrbf3Sfd\nvenuzUajUfbmAPQpT/iPS7p6zv1V2TIAC0Ce8L8uadTMvmpmiyR9T9KeYtoCULaBh/rc/RMzu1fS\nf6kz1Dfl7gcL6wxAqXKN87v7S5JeKqgXAEPEx3uBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF\n+IGgCD8QFOEHgiL8QFCEHwhqqF/djeGbnJxM1p966qlkfWxsLNf6ixcvTtZRHfb8QFCEHwiK8ANB\nEX4gKMIPBEX4gaAIPxAU4/x/As6fP9+1duTIkeS6hw4dStYPHz6crE9MTCTra9asSdZRHfb8QFCE\nHwiK8ANBEX4gKMIPBEX4gaAIPxBUrnF+M5uWdFbSeUmfuHuziKbwabOzs8n6E0880bW2bdu2XNtu\nNtN/patXr871/KhOER/y+Tt3P1XA8wAYIt72A0HlDb9L2mtmb5jZxiIaAjAced/23+jux83szyW9\nbGb/6+6vzH1A9p/CRonjQ6BOcu353f149vukpOckfe7bHt190t2b7t5sNBp5NgegQAOH38wuN7Ml\nF29L+rakA0U1BqBced72r5D0nJldfJ5/d/f/LKQrAKUbOPzu/r6kvy6wF3QxMzOTrOcdy0+55557\nkvWlS5eWtm2Ui6E+ICjCDwRF+IGgCD8QFOEHgiL8QFB8dfcCcPDgwcq2vW7dusq2jXKx5weCIvxA\nUIQfCIrwA0ERfiAowg8ERfiBoBjnHwJ3T9b37t2brPe6rDaP+++/P1lfsmRJadtGtdjzA0ERfiAo\nwg8ERfiBoAg/EBThB4Ii/EBQjPPXwK233lrac2/atClZ37p1a2nbRr2x5weCIvxAUIQfCIrwA0ER\nfiAowg8ERfiBoHqO85vZlKR1kk66+7XZsmWSfiFpRNK0pDvd/ffltVlvH330UbL+4osvJuu9rvfv\nZdmyZV1rExMTuZ4bf7r62fPvkHTzZ5ZtkbTP3Ucl7cvuA1hAeobf3V+RdPozi9dL2pnd3inp9oL7\nAlCyQY/5V7j7iez2jKQVBfUDYEhyn/DzzgFr14NWM9toZi0za7Xb7bybA1CQQcM/a2YrJSn7fbLb\nA9190t2b7t5sNBoDbg5A0QYN/x5J49ntcUnPF9MOgGHpGX4ze1bSf0v6KzM7Zmbfl/S4pG+Z2VFJ\nf5/dB7CA9Bznd/cNXUo3FdxLrX344Ydda6+++mpy3bvuuitZN7Nk/corr0zWx8fHu9ZGR0eT6yIu\nPuEHBEX4gaAIPxAU4QeCIvxAUIQfCIqv7i7Ak08+Werz33RTelT10UcfLXX7Zfnggw9yrf/0008n\n6wcOHOha63UZ9XXXXZes97pUetGiRcl6HbDnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfv0+LF\ni7vWel1yG9lrr73WtXbDDTck1+11qXMevcb5d+3alawfOXIkWd+yJf2F1nW41Jo9PxAU4QeCIvxA\nUIQfCIrwA0ERfiAowg8ExTh/n9asWdO1dujQoVzP/cgjjyTrDz30UK7nz+OBBx5I1vN8l8GFCxeS\n9UsuKW/flHfbO3bsSNZ7Xc+/ffv2ZH0Y2PMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFA9x/nNbErS\nOkkn3f3abNnDkv5JUjt72IPu/lJZTQ7Du+++m6zPzMx0reW97rzXFN5lSn1+QZLOnTuXrOf5s/ca\nSy/zev6yt93rda2Dfvb8OyTdPM/yn7j72uxnQQcfiKhn+N39FUmnh9ALgCHKc8x/r5m9bWZTZnZF\nYR0BGIpBw79d0tckrZV0QtKPuj3QzDaaWcvMWu12u9vDAAzZQOF391l3P+/uFyT9VNJY4rGT7t50\n92aj0Ri0TwAFGyj8ZrZyzt3vSuo+HSqAWupnqO9ZSd+UtNzMjkn6F0nfNLO1klzStKRNJfYIoAQ9\nw+/uG+ZZ/EwJvVTqzJkzyfrHH39c2rbPnj2brOeZx/6FF15I1nt9F0GZY+0L2erVq5P1u+++e0id\nDI5P+AFBEX4gKMIPBEX4gaAIPxAU4QeC4qu7M1u3bk3We13amsfYWNcPSKKmJiYmkvWlS5cOqZPB\nsecHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAY58/s3r07WS9ziu4qLeRpssvc9ubNm5P1yy67rMh2\nKsGeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpy/T/v37+9am5ycTK47NTWVrB89enSgnoqwkKfJ\nXrJkSbJ+2223da25e3Ld66+/Plm/4447kvWFgD0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTVc5zf\nzK6W9DNJKyS5pEl332ZmyyT9QtKIpGlJd7r778trtVqp72G/7777kuuOj48n67OzswP1VIR9+/Yl\n6zt37kzWV61alaw/9thjXWu9xtp7jfP3uqZ+dHQ0WY+unz3/J5I2u/s1kv5G0g/M7BpJWyTtc/dR\nSfuy+wAWiJ7hd/cT7v5mdvuspMOSrpK0XtLF3cJOSbeX1SSA4n2hY34zG5H0dUm/kbTC3U9kpRl1\nDgsALBB9h9/MviJpt6QJdz8zt+adg7d5D+DMbKOZtcys1W63czULoDh9hd/MvqxO8H/u7r/MFs+a\n2cqsvlLSyfnWdfdJd2+6e7PRaBTRM4AC9Ay/dU65PiPpsLv/eE5pj6SLp7HHJT1ffHsAymJ9DLfc\nKOlVSe9Iuvh9xw+qc9z/H5JWS/qtOkN9p1PP1Ww2vdVq5e0ZQBfNZlOtVquv67B7jvO7+35J3Z7s\npi/SGID64BN+QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiB\noAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4\ngaB6ht/MrjazX5vZITM7aGb/nC1/2MyOm9lb2c8t5bcLoChf6uMxn0ja7O5vmtkSSW+Y2ctZ7Sfu\n/mR57QEoS8/wu/sJSSey22fN7LCkq8puDEC5vtAxv5mNSPq6pN9ki+41s7fNbMrMruiyzkYza5lZ\nq91u52oWQHH6Dr+ZfUXSbkkT7n5G0nZJX5O0Vp13Bj+abz13n3T3prs3G41GAS0DKEJf4TezL6sT\n/J+7+y8lyd1n3f28u1+Q9FNJY+W1CaBo/ZztN0nPSDrs7j+es3zlnId9V9KB4tsDUJZ+zvbfIOkf\nJL1jZm9lyx6UtMHM1kpySdOSNpXSIYBS9HO2f78km6f0UvHtABgWPuEHBEX4gaAIPxAU4QeCIvxA\nUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iytx9eBsza0v67ZxFyyWdGloDX0xde6tr\nXxK9DarI3v7C3fv6vryhhv9zGzdruXuzsgYS6tpbXfuS6G1QVfXG234gKMIPBFV1+Ccr3n5KXXur\na18SvQ2qkt4qPeYHUJ2q9/wAKlJJ+M3sZjM7YmbvmdmWKnroxsymzeydbObhVsW9TJnZSTM7MGfZ\nMjN72cyOZr/nnSatot5qMXNzYmbpSl+7us14PfS3/WZ2qaR3JX1L0jFJr0va4O6HhtpIF2Y2Lanp\n7pWPCZvZ30o6J+ln7n5ttuwJSafd/fHsP84r3P2BmvT2sKRzVc/cnE0os3LuzNKSbpf0j6rwtUv0\ndacqeN2q2POPSXrP3d939z9I2iVpfQV91J67vyLp9GcWr5e0M7u9U51/PEPXpbdacPcT7v5mdvus\npIszS1f62iX6qkQV4b9K0u/m3D+mek357ZL2mtkbZrax6mbmsSKbNl2SZiStqLKZefScuXmYPjOz\ndG1eu0FmvC4aJ/w+70Z3/4ak70j6Qfb2tpa8c8xWp+GavmZuHpZ5Zpb+oypfu0FnvC5aFeE/Lunq\nOfdXZctqwd2PZ79PSnpO9Zt9ePbiJKnZ75MV9/NHdZq5eb6ZpVWD165OM15XEf7XJY2a2VfNbJGk\n70naU0Efn2Nml2cnYmRml0v6tuo3+/AeSePZ7XFJz1fYy6fUZebmbjNLq+LXrnYzXrv70H8k3aLO\nGf//k/TDKnro0tdfSvqf7Odg1b1Jeladt4Efq3Nu5PuS/kzSPklHJf1K0rIa9fZvkt6R9LY6QVtZ\nUW83qvOW/m1Jb2U/t1T92iX6quR14xN+QFCc8AOCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENT/\nAxbANXBCoAG3AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"70O6-2MtOnY5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}