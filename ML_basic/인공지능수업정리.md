## 인공지능 _ 최유경 교수님

### 강의 4. 

* 인공지능을 위한 응용수학 : 선형대수, 확률통계, 수치해석

**https://www.deeplearningbook.org/**

위 링크에 들어가면 딥러닝을 위한 수학( 선형대수, 확률통계, 수치해석)을 공부할수있다.

* 인공지능 분야에서는 상황을 판단하는 하나의 방법으로 정답이 될 확률이 가장 높은 것을 정답으로 채택하는 방법을 자주 사용한다.
* 확률

$$
확률 \ = \frac{어떤\ 사건이\ 발생할\ 수\ 있는\ 경우의\ 가짓\ 수}{모든 \ 경우의\ 가짓수}
$$

```
예) 주사위의 1이 나올 경우의 수를 구하시오.
주사위를 던져 나오는 모든 경우의 수 : 1,2,3,4,5,6 총 6가지
주사위 1이 나오 경우의 수 : 1가지
주사위를 던졌을 때 1이 나올 확률 = 1/6
```

* 확률변수(Random Variable)
  * 정의역이 표본공간, 공역이 실수 전체의 집합인 '함수'
  * 그러나 변수의 역할을 하므로 확률 변수라고 한다.
  * 한 개의 동전을 두 번 던지는 시행에서 앞면이 나오는 횟수를 확률변수 $$X$$ 라고 하자. 
  * $$X=f(S)$$ $$x$$ 는 동전 두 개를 던져 앞면이 나오는 모든 경우의 수 $$(S_i)$$ 를 구하는 '함수' 이다.
* 이산확률변수(Discrete Random Variable)
  * 확률변수 X가 가지는 값이 유한개이거나 자연수와 같이 셀 수 있을 때, X를 이산확률변수라고 한다.
* 연속확률변수(Continuous Random Variable)
  * 확률변수 X가 가지는 값이 무한개여서 셀 수 없을 때, X를 연속확률변수라고 한다.
  * ex) 키, 몸무게, 넓이
  * 어떤 사건의 연속확률변수가 x일 때, 그에 대한 확률 P는 연속확률분포 $$f(x)$$ 를 지정한 $$X$$ 의 구간 안에서 적분한 것과 같다. 

$$
P(a\le X\le b) = \int_{a}^{b} f(x)\, dx
$$



* 확률 분포(Probability Distribution)

  * 정의역이 확률 변수, 공역이 확률의 집합인 '함수'
  * 앞면을 H, 뒷면을 T라 하면 표본 공간 S는 S={HH, HT, TH, TT} 이므로, 이산확률변수 X가 취할 수 있는 값은 0,1,2 이고, 각 값을 취할 확률은 1/4, 1/2, 1/4 이다.
  * 즉, P(X=0) = 1/4, P(X=1) = 1/2, P(X=2) = 1/4
  * $$P=g(X)$$ $$P$$ 는 동전 두 개 중 앞면이 $$x_i$$ 개 나올 확률 분포 함수이다.
  * Histogram 표현 := 확률분포표

* 확률함수

  * 확률 질량 함수(Probability Mass Function)
    * 이산 확률 분포의 확률 함수
  * 확률 밀도 함수(Probability Density Function)
    * 연속 확률 분포의 확률 함수

* 결합확률과 조건부확률 (Joint Probability & Conditional Probability)

  * 결합확률의 정의
    * 사건 A와 사건 B가 서로 독립된 사건일 때, 두 사건의 결합 확률은 다음과 같다.

  $$
  P(A\cap B)=P(A,B)=P(A)P(B)
  $$

  

  * 조건부확률의 정의
    * 사건 B가 일어났을 때, 사건 A가 일어날 조건부 확률을 다음과 같다.

  $$
  P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(A,B)}{P(B)}
  $$

  * 문제 1.

  ```
  하나의 주사위를 두 번 던진다고 가정하자.
  사건 A는 주사위를 두번 던져서 나온 숫자의 합이 8이상일 사건이고, 사건 B는 첫번째 주사위를 던졌을 때 나오는 숫자가 5가 되는 사건 이라고 할때, 결합확률P(A,B)와 조건부확률 P(A|B)를 구하시오.
  ```

  * 정답

  ```
  결합확률 P(A,B)를 구해보자. P(A,B)는 첫 주사위를 던졌을 때 5가 나오고, 두 번째 주사위를 던졌을 때, 3이상의 수가 나올 확률을 의미한다. 1/6 * 4/6 = 1/9 가 된다. 다음은 조건부 확률 P(A|B)를 구해보자. P(B)는 첫번째 주사위를 던졌을 때, 5가 나오는 사건이므로 확률은 1/6이 된다. 조건부 확률 정의 P(A|B) = P(A,B)/P(B) 에 따르면, 1/9 / 1/6 = 2/3 이 되어 P(A|B)의 확률은 2/3이 된다. 
  ```

  * 문제2. 

  ```
  백만 명에 다섯 명 꼴로 어떤 질병에 걸린다고 가정하자. 이 질병을 진단하는데 최신 AI 기술을 사용하면, 99.99%의 정밀도로 그 사람이 질병에 걸렸는지 안 걸렸는지를 판정할 수 있다. (0.01%확률로 잘못된 판정을 할 수 있음) 최수정씨는 시험삼아 최신 AI 검사를 받았다. 진단 결과가 양성으로 나왔다면 실제로 최수정씨가 이 질병에 걸렸을 확률은 얼마일까요?
  
  ```

  * 정답(맞나?)

  ```
  사건 A: 질병에 걸릴 사건
  사건 B: 진단 결과가 양성으로 나올 사건
  P(A,B) = P(A)P(B) = 0.00005 * 0.9999
  P(A|B) = P(A,B)/P(B) = 0.00005 * 0.9999 / 0.9999 = 0.00005
  ```

* 사슬법칙(Chain rule)

  * 조건부확률과 결합확률의 관계를 확장하면 복수의 사건 X1,X2,...,XN 에 대한 조건부 확률을 다음처럼 쓸 수있다. 이를 사슬법칙(chain rule)이라고 한다.

  $$
  P(X_1,X_2) = P(X_1)P(X_2|X_1) \\
  P(X_1,X_2,X_3) = P(X_3|X_1,X_2)P(X_1,X_2) \\
  =P(X_1)P(X_2|X_1)P(X_3|X_1,X_2) \\
  P(X_1,X_2,X_3,X_4) = P(X_4|X_1,X_2,X_3)P(X_1,X_2,X_3) \\
  =P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)P(X_4|X_1,X_2,X_3)
  $$



* 기댓값(Expectation Value)

  * 나올 것이라고 예상하는 값
  * X가 확률변수이고 확률 P(X)인 사건이 벌어질 때, 예상할 수 있는 결괏값이 기대 값임
  * 모든 이산확률변수 X에 대한 기댓 값 E(X)는 다음과 같다. (이 때, 확률은 P(X))

  $$
  E(X) = \sum P(X) \cdot X
  $$

* 기댓값의 속성(Properties of Expectation Value)

  * $$E(k) = k $$ (상수의 기댓값은 상수가 된다.)
  * $$E(kX) = kE(X)$$ (확률변수를 상수 배하면, 기댓값도 상수 배가 된다)
  * $$E(X+Y) = E(X) + E(Y)$$ (확률변수의 합의 기댓값은 각 기댓값의 합과 같다)
  * $$X$$ 와 $$Y$$ 가 서로 독립일 때 $$E(XY) = E(X)E(Y)$$ (독립적인 확률변수의 곱에 대한 기댓값은 각 기댓값의 곱과 같다)

```
예시) 1등 상금이 1억원인 복권이 1장, 2등 상금이 100만원이 복권이 10장, 3등 상금이 1만원이 복권이 1000장 있다고 가정합시다. 이 복권의 총 판매량은 백만장입니다. 이 복권을 1장 샀을 때 기대할 수 있는 당첨 금액은 얼마일까요?
```

```
 Answer)
이때의 확률 변수를 X, 그에 대한 확률을 P(X)라고 할때, 이산확률분포는 다음 표와 같습니다.
기댓값 E는 모든 확률변수 X와 그에 대한 발생확률 P(X)를 곱한 다음, 모두를 더한 것과 같습니다. 따라서 다음과 같이 풀수 있습니다.
𝐸 = (100,000,000*1)/1,000,000 + (1,000,000*10)/1,000,000 + (10,000*1000)/1,000,000 + 0
= 100+10+10 = 120
여기서 말하는 기댓값 E=120의 의미는 복권 1장을 샀을 때 예상되는 당첨 금액이 120원이라는 뜻입니다.
```



* Mean :=Expectation Value

  * 평균의 정의(Definition of Mean)

    * 평균은 수학적으로 기댓값과 같은 의미이다.
    * 즉, '과거 6개월 간의 매출 평균이 다음달의 예상 매출액이 된다'는 확률의 관점에서 달리 표현하자면 '6개의 확률변수가 각각 같은 확률(1/6)로 발생하므로 다음 한달 동안의 매출에 대한 기댓값은 각 월의 매출에 1/6을 곱한 것을 모두 더한 합계와 같다'라고 하는 것

    $$
    N개의\ 확률변수가\ 각각 x_1,x_2,x_3,...,x_n이라는\ 값을\ 가질\ 때\ 평균\ 값은\ 다음과\ 같다. \\
    \bar{x} = \sum_{k=1}^n\frac{1}{n}\cdot x_k = \frac{1}{n}\sum_{k=1}^n x_k
    $$

```
Question) 그런데 과연 앞의 방식대로 평균값만 구하면 다음 달의 매출을 올바르게 예상할 수 있는 것인가?
```

```
Question) 그래서 단순히 평균값을 구하는 방법 외에도 평균값과 실 데이터가 얼마나 차이(Deviation, 편차)가 나는지에 대해 생각해 봅시다.
```



* 편차 vs 분산(Deviation vs Variance)

  * 편차의 문제
    * 매달 얼마만큼의 매출액이 고객별로 흩어져 있는지 분석했으나, 편차의 합계를 구해보면 0이 되어버린다.
    * (+) 방향과 (-) 방향으로 흩어진 매출의 차이가 상쇄되기 때문이다.
  * 따라서, 편차의 합으로는 매출의 흩어진 정도를 확인할 수 없다.

* 분산(Definition of Variance)

  * 어떻게 하면 상쇄되는 영향력을 없앨 수 있을까?
  * (+), (-) 부호를 없애기 위해 편차의 제곱을 이용하기로 한다.

  $$
  N개의\ 확률변수가\ 각각\ x_1,x_2,x_3,...,x_n이라는\ 값을\ 가질\ 때\ 평균\ 값이\ \bar{x} 일\ 때\ 분산은\ 다음과\ 같다. \\
  \sigma^2=\frac{1}{n}\sum_{k=1}^n (x_k-\bar{x})^2
  $$

* 분산 vs 표준편차 (Variance vs Standard Deviation)
  * 분산의 문제
    * 제곱한 값을 사용하기 때문에 단위를 표현하기가 애매해 짐
    * 따라서, 단위의 물리적 의미를 찾기 위해 분산의 제곱근인 표준편차(standard deviation)를 사용하기로 약속함

$$
N개의\ 확률변수가\ 각각\ x_1,x_2,x_3,...,x_n\ 이라는\ 값을\ 가질\ 때\ 평균\ 값이\ \bar{x}일\ 때\ 표준편차는\ 다음과\ 같다. \\
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n}\sum_{k=1}^n (x_k-\bar{x})^2}
$$

```
Question) 세 명의 고객 중에서 전체 매출의 월간 동향에 반응하며 트렌드에 민감한 구매 성향을 보이는 고객은 누구일까?
```

```
Answer) 세명의 고객 각각이 월 매출과 어느 정도의 상관관계(Covariance)를 가졌는가를 고민하면 된다.
```

$$
두\ 가지\ 데이터에\ 대한\ n조의\ 확률변수\ (X,Y)={(x1,y1),(x2,y2),...,(xn,yn)}이\ 있다고\ 가정한다.\\ \ x의\ 평균이\ \mu_x 이고\ Y의\ 평균이\ \mu_y 라고\ 할\ 때\ 공분산\ Cov(X,Y)\ 는\ 다음과\ 같다.
$$

$$
Cov(X,Y) = \frac{1}{n}\sum_{k=1}^n(x_k-\mu_x)(y_k-\mu_y)
$$

* 공분산(Covariance)
  * 공분산 > 0 : 증가하는 관계에 있다.
  * 공분산 < 0 : 감소하는 관계에 있다.
  * 공분산 = 0 : 상관관계가 없으며, 서로 독립이다.
* 공분산 vs 상관계수(Covariance vs Correlation Coefficient)
  * 공분산의 문제
    * 제곱한 값을 사용하기 때문에 단위를 표현하기가 애매해 짐
    * 따라서 단위의 물리적 의미를 찾기 위해 공분산을 각각의 표준편차로 나누어 단위를 없애버린 값 상관계수(correlation coefficient)를 사용하기로 함.
    * 이 과정에서 상관계수는 -1 ~ 1 사이 값을 가지게 되고, 이러한 과정을 정규화라고 한다.

$$
확률변수\ X와\ Y의\ 분산이\ 양수이고,\ 각각의\ 표준편차가\ \sigma_X,\sigma_Y,\\ 공분산이\ \sigma_{XY}\ 라고\ 할\ 때의\ 상관계수는\ 다음과\ 같다.\ (이때,\ -1\le \rho \le 1 )\\
\rho = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}
$$

* 상관계수(Correlation Coefficient)

  * 실제로 $$\rho_{RA}\ \rho_{RP}$$ 를 계산해 봅시다.

  * | 고객명 | 1월        | 2월     | 3월     | 4월        | 5월        | 6월        | 소계     |
    | ------ | ---------- | ------- | ------- | ---------- | ---------- | ---------- | -------- |
    | 백소연 | 5,000원    | 5,000원 | 5,000원 | 5,000원    | 5,000원    | 5,000원    | 30,000원 |
    | 이민준 | 10,,000원  | 3,000원 | 1,000원 | 1,000원    | 15,000원   | 0원        | 30,000원 |
    | 이용진 | 3,000원    | 7,000원 | 2.000원 | 8.000원    | 4.000원    | 6.000원    | 30,000원 |
    | ...    | ...        | ...     | ...     | ...        | ...        | ...        | ...      |
    | 월매출 | 2천5백만원 | 4천만원 | 2천만원 | 5천5백만원 | 3천5백만원 | 4천5백만원 | 2억2천만 |



* Maximum Likelihood Estimation(MAP)

  * Maximum Likelihood Estimation(최대우도추정)이란?
    * 파라마터($$\theta​$$)에 대한 가능도함수 $$\mathcal{L}(\theta) ​$$ 를 최대화 할 수 있는 $$\theta ​$$ 값을 구하는 것
    * 즉, 1차 미분 값이 0이 되는 $$\theta$$ 를 구하는 것!

  $$
  \frac{dL(\theta)}{d\theta}=0
  $$
  * 예1) 예를 들어, 어떤 동전을 던져서 나오는 결과가 확률 변수$$X$$라고 한다면, 이 변수는 앞($$\uparrow$$)과 뒤($$\downarrow$$) 의 두 값을 가질 수 있다. 동전을 던져 앞이 나올 확률이

  * $$
    Pr(X=\uparrow)=\theta
    $$

    로 주어지는 경우, 동전을 세 번 던져 앞, 뒤, 앞이 나왔을 때의 $$\theta$$ 의 가능도는 
    $$
    \mathcal{L}(\theta|\uparrow\downarrow\uparrow)=\theta\ \cdot\ (1-\theta)\ \cdot\ \theta = \theta^2(1-\theta)
    $$
    가 된다. 가능도 함수를 적분하면
    $$
    \int_{0}^{1} \mathcal{L}(\theta|\uparrow\downarrow\uparrow)\, d\theta = \int_{0}^{1} \theta^2(1-\theta)\, d\theta = 1/12
    $$
    이므로, 가능도는 확률 분포가 아님을 알 수 있다.

* Maximum Likelihood Estimation의 문제점?

  * 가능도수의 차수가 고차원 방정식(ex) 100차) 이라 `미분`이 상당히 복잡!!

*  **로그** 가능도함수 도입

  * $$log_eL(\theta)$$
  * 로그를 사용하면, 곱셈을 덧셈으로 바꿀 수 있고
  * 로그를 사용하면, 100차 방정식을 1차방정식으로 모양을 바꾸어 미분 가능

  $$
  \frac{d}{d\theta}log_eL(\theta)=0
  $$

  

* 실제로 과거의 데이터로부터 미래를 예측할 때 이러한 방법을 사용함

$$
L(\theta_1,\theta_2,....,\theta_m)을\ 최대로\ 하는\ \theta_1,\theta_2,...,\theta_m은\ 다음\ 방정식을\ 만족한다. \\
\frac{d}{d\theta_1}L(\theta_1,\theta_2,....,\theta_m)=0 \\
\frac{d}{d\theta_2}L(\theta_1,\theta_2,....,\theta_m)=0 \\
... \\
\frac{d}{d\theta_m}L(\theta_1,\theta_2,....,\theta_m)=0 \\
$$



### 강의 5.

### $수\ 치\ 해\ 석\  :\ =\ 수\ 치\ 계\ 산\ $ 

* 수치해석이란? 
  * 수치적으로 근사값을 구하는 방법을 연구하는 분야
  * 선형방정식의 해, 보간법, 미분방정식, 적분방정식, 고유치문제, 유한요소법, 최적화 이론
* 딥러닝을 위한 최적화
  * 딥러닝 알고리즘에는 최적화가 필수
  * 최적화란 x값을 바꿔가면서 f(x)의 값을 최대/최소화 하는 것
  * 대부분 최소값 f(x) 를 찾음
  * 최대화는 -f(x)를 수행하면 됨
  * 일반적으로 f(x)는 Objective function(목적함수), criterion(판정기준)이라고 불림
  * 최소값을 찾는 f(x)의 경우 Cost function(비용함수), Loss function(손실함수), Error function(오차함수)

* 딥러닝을 위한 수치계산

  * 수학 공식을 해석적으로 유도해서 해결하지 않음

  * 반복적인 과정을 통해서 정답의 추정값을 계속 갱신하여 문제를 품

    예) 최소, 최대를 구하는 최적화 문제

  * 수치계산을 위한 몇가지 주의사항

    * 알고리즘은 '실수' 함수로 구성되기도 한다.
    * 실수는 컴퓨터로 표현하는데 '한계'를 가지고 있다.
    * 입력값의 작은 변화가 출력값의 큰 변화를 가져올 문제가 됨
    * 입력의 반올림 오차가 증폭되어서 출력의 차이가 커짐

  * Conditioning(조건화)

    * 입력의 작은 변화에 대해 함수가 얼마나 급하게 변하는지를 뜻하는 용어(진동)

  * Condition Number(조건수)

    * 가장 큰 고윳값과 가장 작은 고윳값의 크기(절대값)의 비
    * 이 비가 크면, 역행렬 계산은 입력의 오차에 특히나 민감함

    $$
    Condition\ Number = max_{i,j}	\left| \frac{\lambda_i}{\lambda_j} \right|
    $$

* Roadmap

  * Iterative Optimization
  * Rounding error, underflow, overflow

* 일차 미분의 가장 단순한 형태는 수식에 따라 임의의 시작점으로부터 수렴할 때까지 x를 변화시키는 것
  $$
  X_{k+1} = X_k - \lambda f'(X_k)
  $$

* Iterative Optimization : Curvature

  이차 미분의 가장 단순한 형태는 
  $$
  X_{k+1} = X_k-\frac{f'(X_k)}{f''(X_k)}
  $$
  최적화 기법 : LM 공부

* Approximate Optimization

  * global minimum(전역 최소점)
  * local minimum(국소 최소점)

![ì°ë´ë ¤ì¤ëììì¤ìê¸¸ìì°¾ê¸°(Optimizer)ìë°ë¬ê³ë³´ SGD Momentum NAG Adagrad RMSProp AdaDelta Adam Nadam ì¤íê³ì°í´ììì§ì¸í,â¨ ìê¹ë´ë ¤ì¤ëê´ì±ë°©í¥ëê°ì ì¼ë¨ê´ì±ë°©í¥ë¨¼ì ìì§ì´ê³ ...](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)

* Critical Points
  * $$f'(x) = 0$$ 인 점
  * Local minimum point(극소점), Local maximum point(극대점), saddle point(안장점 : 극소도 극대도 아닌 점)

* 다변수에서는 모든 편미분이 0인 지점을 찾아야한다.
* 그 식이 Jacobian(야코비언) 행렬과 Hessian(헤시안) 행렬이다



### 강의 6.

* 작지만 시장이 개척되어야만 기술이 발전한다

  * Uber : 10억 비용의 자율주행차량을 제작해서 시장을 개척하고자 한다.

* 머신러닝  : 데이터(과일들) + 정답(요리) => 함수(레시피)를 기계가 학습하는 방법

* 가설 설정 : $$H(x) = Wx+b​$$  

* 비용 함수 (Cost function) : $$H(x) - y$$ 
  $$
  cost = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
  $$

$$
cost(W,b) = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
$$

* 목표 : $$minimize\ cost(W,b)$$  
* cost를 최소화하기 위해 사용하는 방법?  => Optimization Algorithm을 사용한다
* ![ì°ë´ë ¤ì¤ëììì¤ìê¸¸ìì°¾ê¸°(Optimizer)ìë°ë¬ê³ë³´ SGD Momentum NAG Adagrad RMSProp AdaDelta Adam Nadam ì¤íê³ì°í´ììì§ì¸í,â¨ ìê¹ë´ë ¤ì¤ëê´ì±ë°©í¥ëê°ì ì¼ë¨ê´ì±ë°©í¥ë¨¼ì ìì§ì´ê³ ...](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)

* Gradient descent algorithm(경사 하강 알고리즘)

  * cost function 최소화

  * 많은 최소화 문제에 사용한다

  * 주어진 $$cost(W,b)$$ 에서 $$cost$$를 최소화하는 $$W,b$$ 를 찾는 것

  * $$cost(w_1,w_2,w_3...,b)$$ 도 가능하다

  * Gradient Descent Algorithm

  * $$
    W :=W - \alpha\frac{\partial}{\partial W}cost(W)
    $$

    

* Convex Function(Global minimum을 보장하는지 결정짓는 함수)

* $$
  cost(W,b) = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
  $$

* ![ê´ë ¨ ì´ë¯¸ì§](https://lh5.googleusercontent.com/DLRjNnXFnpUMKd2FjxX3fTCjLIjd_NRFFxfZB4jzPZmTbSiDWXQ-D9JNgcHrBOTTCGtcWtmNmPEpY3MX0BbOjoMgSg0CubWHnzp-mf7FBWqftlMQewNKu6iOsuF9_DgUcnD3AyEC)

  ![convexì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](https://undergroundmathematics.org/glossary/convex-shape/images/convex.png)



* Linear Regression(기출하신다고함)
  * Hypothesis
    * $$H(X) = WX$$
  * Cost function
    * $$cost(W) = \frac{1}{m}\sum{(WX-y)^2}$$
  * Gradient descent algorithm
    * $$W :=W - \alpha \frac{\}{}$$




### 강의 7.

* 어떻게 linear regression을 사용해서 이진 분류를 하는가

* 오늘의 목표 : 이진 분류(binary classification)

  * example) Spam Detection, Facebook feed, Credit Card Fraudulent Transaction detection

* Binary Label Encoding -> '0' or '1'

  * Spam Detection : Spam(1) or Ham(0)
  * Facebook feed : show(1) or hide(0)
  * Credit Card Fraudulent Transaction detection: legitimate(1)/ fraud(0)

* 이진 분류의 예

  * Positive or Negative
  * Buy or Sell
  * Pass or Fail

* Linear Regression이 이진 분류에서 단점은?

  * 단점 1 : 선형으로는 0과 1을 제대로 가르지 못한다. 특정 값을 안으로 집어 넣는다 -> Bound 되지 않는다.

* 이진 분류에서 Linear Regression을 사용하면?

  * 우리는 Y를 0과 1 두가지 데이터로 알고 있다.
  * $$H(x) = Wx+b$$ 
  * ( + ) : 이 Hypothesis는 간단하고 사용하기 쉽다.
  *  ( - ) : 이 hypothesis는 1보다 크거나 0보다 작은 값을 줄 수 있다.너무 크고 너무 작은 값이 나온다. 0과 1사이의 값이 안나온다.

* Logistic Hypothesis

  * $$H(x) = Wx+b$$ 의 식을 어떻게 0과 1사이로 바운드 시킬까?

  * 0 <= H(x) <= 1 

  * Sigmoid : 두 방향에서 커브가 있는 함수다.

  * $$Logistic\  function := sigmoid\ function$$

  * Sigmoid 함수 덕분에 H(x)가 Bound 되었다.
    $$
    H(X) = \frac{1}{1+e^{-(W^TX)}}
    $$

    $$
    0<=H(x)<=1
    $$

    

* Cost function

  * 이제 Cost function 에 적용해보자.

  * linear regression cost function에 적용하니, local minimum에 빠진다.

  * $$
    H(x) = Wx + b \Rightarrow Convex function
    $$

    $$
    H(X) = \frac{1}{1+e^{-W^TX}} \Rightarrow Non-Convex function
    $$

    ```
    설계 팁 : 
    정답에 가까워 질수록 Cost function 값은 작고
    정답에서 멀어질 수록 Cost function 값은 크게 ! 
    설계하면 된다.
    ```

* Cost function for logistic regression

  * Logistic regression function

  * $$
    H(X) = \frac{1}{1+e^{-W^TX}}
    $$

  * 

  $$
  cost(W) = \frac{1}{m}\sum \ \ \ \ \ c(H(x),y)
  $$

  * y=1이 true일 때와 y=0이 true일 때의 함수

  $$
  c(H(x),y) = \begin{cases}
  -log(H(x)) & :y=1 \\
  -log(1-H(x)) & :y=0
  \end{cases}
  $$

  * 하나의 equation으로 잘 표현하면
    $$
    C(H(x),y)=-ylog(H(x))-(1-y)log(1-H(x))
    $$

* Gradient decent algorith

$$
W:=W-\alpha\frac{\partial}{\partial W}cost(W)
$$

```python
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-y)* tf.log(1-hypothesis)))

# Minimize
a = tf.Variable(0.1) # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```

* Binary Classification을 위한 Logistic regression 
  1. $$H_L(x) = WX​$$
  2. $$z = H_L(x), \ g(z)​$$
  3. $$g(z) = \frac{1}{1+e^{-2}}​$$
  4. $$H_R(x) = g(H_L(x))$$ 



* Multi-Class classification을 Binary classification으로 어떻게 할까?

* Multinomial classification(여러 개의 클래스!)
* A or not / B or not / C or not 으로 나눈다~
* Softmax?
  * Multiclass classification의 Hypothesis를 효율적으로 [0,1]로 제한하는 방법을 알아보자.
* Softmax function
  * 모든 값이 0~1사이
  * 전체 합이 1(확률 정규화!!)
  * Probability와 비슷하다.
* Cross-entropy