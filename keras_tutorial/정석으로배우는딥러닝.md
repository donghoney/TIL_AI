##  신경망

신경망이란?

신경망은 여러 인공지능 분야에서 쓰이는 알고리즘 중 하나인데 신경망의 큰 특징은 '인간은 뇌 구조를 모방한다'는 것입니다.

인간의 뇌는 뉴런(neuron) 이라는 신경세포로 구성되고 '신경망'이라는 이름도 본래 이 뉴런에서 나온 단어입니다. 

신경망을 뉴런의 네트워크라고 부르는 이유는 인간의 뇌가 뉴런이 네트워크를 이루고 있는 형태로 만들어져 있기 때문입니다.

인간의 대뇌피질에는 약 140억개의 뉴런이 거대한 그물망과 같은 네트워크를 형성하고 있습니다.

뉴런끼리 정보를 전달하므로 인간은 사물을 인식하고 정보를 처리할 수 있는 것입니다.



### 뉴런의 정보 전달 방식

뉴런과 뉴런 사이에는 전기 신호를 통해 정보가 전달됩니다.

뉴런이 다른 뉴런으로부터 입력을 받으면 자신 안에서 전기를 더한 후 전체 전기량이 어떤 임계값을 넘으면 다음 뉴런으로 전기신호를 보냅니다. 이 전기신호가 뉴런의 네트워크 안에서 흐르고 있는데 각 뉴런 사이의 결합의 강도가 다르므로 어떤 뉴런이 얼마만큼의 전기신호를 받는지에 따라 네트워크 전체에서 전기신호가 전달되는 방식이 달라집니다. 



### 단순한 모델화 

* 두 개의 뉴런 중 어느 것이 어느 정도의 전기신호를 받는가
* 임곗값은 어느정도로 설정해야 하는가
* 임곗값을 넘었을 때 어느 정도의 전기신호를 보내는가

두 개의 뉴런이 얼마만큼의 전기신호를 받는지를 각각 
$$
x_1 ,\ x_2
$$
이라는 변수로 표현했을 때, 이 두개의 뉴런은 정보의 입구가 되는 부분이므로 이 두 개의 뉴런에는 임곗값이 없으며 다음 뉴런에게 그대로 전기신호를 전달합니다. 그러나 각 뉴런 사이에서 결합의 강도가 모두 다르므로 실제로 전기신호가 전달되는 양도 모두 다릅니다. 

이 결합의 강도를 각각 
$$
w_1, \ w_2
$$
로 표현합니다.

그렇다면 두개의 뉴런으로부터 전달되는 전기신호의 총량은 다음과 같습니다.
$$
w_1x_1 \ + \ w_2x_2
$$
 이  $$w_1, \ w_2 $$ 를 네트워크의 가중치라고 부릅니다.

전기 신호를 받은 뉴런이 다음 뉴런으로 신호를 보낼지(발화할지) 보내지 않을지는 해당 뉴런이 받은 전기량이 임곗값을 넘었는지 넘지 않았는지에 의해 정해집니다. 따라서 이 임곗값을 $$\theta$$라고 했을 때 조건식 $$w_1x_1 \ + \ w_2x_2\ >= \theta \ $$ 를 만족하면 해당 뉴런이 발화하고 조건식을 만족하지 않으면 발화하지 않습니다. 뉴런이 발화할 때 다음 뉴런으로 전기량이 얼마나 전해지는지에 관한 정보는 네트워크의 웨이트가 가지고 있으므로 뉴런의 발화에 관해서는 +1(발화했다)과 0(발화하지 않았다)만을 생각하면 됩니다.

따라서 이전 뉴런으로부터 받는 전기신호량(= 출력)을  y라고 하면 이 y는 다음과 같은 식으로 나타낼 수 있습니다.
$$
y = 
\begin{cases}
1 & (w_1x_1 \ + \ w_2x_2\ge \ \theta)\\
0 & (w_1x_1 \ + \ w_2x_2 \ge \ \theta)
\end{cases}
$$
 이렇게 단순한 모델을 만들어 봤습니다. 네트워크의 웨이트 $$w_1, \ w_2$$와 임곗값 $$\theta$$ 를 알맞게 설정한다면 입력 $$x_1, \ x_2$$ 에 대한 출력  y 값이 실제 뇌 안에서 전파되는 전기 신호량과 같아질 것입니다. 신경망이 아무리 복잡해져도 기본적으로 위에 나온 내용을 응용하면 됩니다.



### 논리회로

인간의 뇌는 정보를 아날로그(=연속된 값) 방식으로 처리합니다. 따라서 뇌 안에 있는 신경 회로는 아날로그 회로라고 말할 수 있습니다. 기계를 구성하는 전자회로에는 아날로그 회로와 디지털 회로가 있습니다. 디지털 회로를 사용하면 자연계의 정보를 디지털(0과 1) 방식으로 처리하므로 아날로그 방식으로 신호를 처리할 때보다 정보를 빠르게 처리할 수 있습니다.



디지털 회로에서는 0과 1신호의 입력을 제어하기 위해 논리 게이트라는 회로를 사용합니다. 

1. AND 게이트(논리곱)
2. OR 게이트(논리합)
3. NOT 게이트(논리부정)



이 세가지를 조합해서 모든 입출력 패턴을 만듭니다. 신경망으로 정보를 처리한다는 것은 인간 대신 기계가 정보를 처리한다는 것이므로 이러한 논리 게이트를 구축할 수 있는지 여부가 가장 중요합니다. 

1. AND 게이트



   ![AND ANSI.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/6/64/AND_ANSI.svg/100px-AND_ANSI.svg.png)

   | **INPUT** | INPUT | OUTPUT  |
   | --------- | ----- | ------- |
   | A         | B     | A AND B |
   | 0         | 0     | 0       |
   | 0         | 1     | 0       |
   | 1         | 0     | 0       |
   | 1         | 1     | 1       |

2. OR 게이트

   ![OR ANSI Labelled.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/OR_ANSI_Labelled.svg/120px-OR_ANSI_Labelled.svg.png)

   | **INPUT** | INPUT | OUTPUT |
   | --------- | ----- | ------ |
   | A         | B     | A OR B |
   | 0         | 0     | 0      |
   | 0         | 1     | 1      |
   | 1         | 0     | 1      |
   | 1         | 1     | 1      |

3.  NOT 게이트

![NOT SYM.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/NOT_SYM.svg/200px-NOT_SYM.svg.png)

| **INPUT** | **OUTPUT** |
| --------- | ---------- |
| A         | NOT A      |
| 0         | 1          |
| 1         | 0          |



###  구현

벡터를 사용해서 퍼셉트론의 식을 표현하면 이해하기 쉽다는 장점이 있습니다. 그 뿐만 아니라 구현할 때도 벡터와 배열을 대응시키면 직관적으로 구현할 수 있다는 장점도 있습니다. 간단한 예를 살펴보겠습니다.

* 입력뉴런 2개
* 두 종류의 정규분포를 따르는 데이터를 분류
* 데이터 평균값 0
* 발화하는 데이터는 평균값이 5이며 각각 10개의 데이터가 있다고 가정



```python
import numpy as np

rng = np.random.RandomState(123)

d = 2  # 데이터의 차원
N = 10  # 각 패턴마다의 데이터 수
mean = 5  # 뉴런이 발화하는 데이터의 평균 값

x1 = rng.randn(N, d) + np.array([0, 0])
x2 = rng.rand(N, d) + np.array([mean, mean])

x = np.concatenate((x1, x2), axis=0)
w = np.zeros(d)
b = 0


def y(x):
    return step(np.dot(w, x) + b)


def step(x):
    return 1 * (x > 0)


def t(i):
    if i < N:
        return 0
    else:
        return 1


while True:
    classified = True
    for i in range(N * 2):
        delta_w = (t(i) - y(x[i])) * x[i]
        delta_b = (t(i) - y(x[i]))
        w += delta_w
        b += delta_b
        classified *= all(delta_w == 0) * (delta_b == 0)
    if classified:
        break

print('w :',w) # w : [2.14037745 1.2763927]
print('b :',b) # b : -9

```



20개의 데이터 중 하나라도 $$\triangle w \ \ne \  0$$또는 $$\triangle b \ne \ 0$$이면 이 행에서 classified가 0이 되고 또 다시 학습을 반복합니다.



이 프로그램을 전체 실행하면   $$ w : [2.14037745 1.2763927] , \ b : -9$$ 라는 결과가 구해집니다.

$$x \ = \ (x_1x_2)^T$$라고 하면
$$
2.14037745x_1 \ + \ 1.2763927x_2 \ - \ 9 \ = \ 0
$$
위의 식이 바로 뉴런이 발화하는지 않는지를 판별하는 경계선이라는 것을 알 수 있습니다.



##### 따라서 예를 들어 (0, 0)이면 뉴런이 발화하지 않고 (5, 5)이면 뉴런이 발화할 것이라고 판단할 수 있습니다.



### 로지스틱 회귀

계단함수와 시그모이드 함수

단순 퍼셉트론에서는 뉴런이 발화하는지 , 발화하지 않는지를 계단함수를 사용해서 판별하므로 뉴런에서 나오는 출력이 0과 1 두가지 값 뿐이었습니다. 물론 이것만 가지고도 데이터는 분류할 수 있지만 현실에서 발생하는 문제에 단순 퍼셉트론을 적용하려고 하면 적합하지 않은 경우도 생깁니다.

 예를 들어, 스팸 메일을 분류하는 일을 생각해 보겠습니다. 스팸이 아닌 메일이 스팸 메일함에 들어 있어 읽지 못했던 경험이 있을 것 입니다. 신경망은 이제까지 주어진 데이터 안에서 스팸 메일을 골라내기 때문에 이제까지 스팸이었던 메일과 내용이 비슷한 메일(그러나 스팸이 아닌 메일)을 스팸이라고 판정해버립니다. 이런 현상은 신경망이 학습을 하는 방법의 특성상 피할 수 없는 문제 입니다.

그래서 '아슬아슬'하게 스팸으로 판정된 메일은 수신함에 그대로 남겨둬서 메일을 못 읽고 지나치는 일이 없도록 할 수 있습니다. 

그러나 단순 퍼셉트론으로는 이 '아슬아슬'한 정도를 수치화 하는 것이 불가능합니다. 출력값이 0이나 1뿐이므로 뉴런이 '아슬아슬'하게 발화하기 직전의 상태인 데이터도 그리고 전혀 발화하지 않는 데이터도 똑같이 0에 포함돼버립니다.

이 문제를 해결하려면 출력값이 0이나 1이 아니라 출력값이 0에서 1 사이에 존재하는 확률이어야 합니다.

그런 함수가 바로 시그모이스 함수입니다.
$$
\sigma(x) \ = \ \frac{1}{1+e^{-x}}
$$


계단함수는 0과 1사이의 값을 출력하지 못하지만,

시그모이드 함수는 0과 1사이의 값을 출력할 수 있습니다.

계단 함수 대신 시그모이드 함수를 사용한 모델을 '로지스틱 회귀'라고 합니다.다시 말하면 뉴런의 출력식 
$$
y = f(w^Tx+b)
$$
에서 사용되는 함수인 $$f(.)$$부분은 계단함수여도 되고 시그모이드 함수여도 (또는 다른 함수여도) 됩니다. 뉴런의 선형결합을 계산한 후에 이들 함수처럼 비선형변환을 하는 함수를 모두 '활성화함수(activation function)'라고 합니다

시그모이드 함수가 사용되는 이유는 이 함수의 수학적 특징에 있습니다. 시그모이드 함수를 미분해보면 다음과 같은 식이 됩니다.
$$
\sigma'(x) = \sigma(x)(1-\sigma(x))
$$
이 미분식을 보면 시그모이드 함수 자신이 또다시 미분식에 나타난다는 것을 알 수 있습니다. 이론상으로도 , 그리고 구현할 때도 이 특징은 매우 도움이 됩니다.



### 모델화

#### 우도함수와 교차 엔트로피 오차함수

로지스틱 회귀는 단순 퍼셉트론과는 달리 확률적인 분류 모델이므로 접근하는 방법도 다릅니다. 어떤 입력  x 에 대해 뉴런이 발화할지 여부를 나타내는 확률변수를 C라고 하겠습니다. 다시 말하면 C는 뉴런이 발화할 경우 C = 1이 되고 발화하지 않을 경우 C = 0 이 되는 확률변수입니다. 

단순 퍼셉트론과 똑같은 모델을 생각해보면 뉴런이 발화할 확률은 다음과 같고
$$
p(C=1|x)=\sigma(w^Tx+b)
$$
확률을 모두 더한 것이 1이므로 반대로 뉴런이 발화하지 않을 확률은 다음과 같습니다.
$$
p(C=0|x)=1-p(C=1|x)
$$
C는 0이나 1밖에는 값을 가질 수 없으므로 $$y:=\sigma(w^Tx+b)$$라고 두면 위의 두 식을 다음과 같이 정리할 수 있습니다.
$$
p(C=t|x)=y^t(1-y)^{1-t}
$$
이 때 $$t\in\{0,1\}$$입니다. 따라서 N개의 입력 데이터 $$x_n(n=1,2,...,N)$$과 이 입력 데이터와 쌍을 이루는 출력 데이터 t_n이 주어졌을 때 네트워크의 파라미터인 웨이트 w와 바이어스 b를 최우추정하기 위한 '우도함수(likelihood function)'는 $$y_n :=\sigma(w^Tx_n+b)$$ 를 사용해 다음과 같이 나타낼 수 있습니다. 
$$
\begin{matrix}L(w,b) &=& \prod_{n=1}^Np(C=t_n|x_n) \\
&=& \ \prod_{n=1}^Ny_n^{t_n}(1-y_n)^{1-t_n}
\end{matrix}
$$
이 우도함수값이 최대가 되도록(최우추정) 파라미터를 수정해가며 찾으면 네트워크가 학습을 잘 한 것입니다.

이처럼 함수가 최대나 최소가 되는 상태를 구하는 문제를 '최적화 문제(optimization problem)'라고 합니다. 함수의 최대화는 부호를 반전시키면 최소화가 되므로 일반적으로 함수를 '최적화한다'고 말할 때는 함수를 최소로 만드는 파라미터를 구하는 것을 의미합니다.



함수가 최대나 최소라는 단어를 들으면 '미분을 떠올리게 됩니다. 일단 파라미터의 편미분(경사도)를 구하는 것부터 시작합니다. 따라서 우도함수의 최대화를 생각할 떄도 우도함수를 각 파라미터로 편미분하면 되는데 곱의 형태는 편미분을 계산하기 매우 까다롭습니다. 쉽게 계산할 수 있도록 식에 로그를 취해서 식 전체를 덧셈 형태로 변형합니다. 그리고 일반적인 최적화 문제와 모양을 맞추기 위해 부호를 바꾸면 다음과 같은 식이 됩니다.
$$
\begin{matrix}E(w,b) &:=& -logL(w,b) \\
&=& -\sum_{n=1}^N\{t_nlogy_n + (1-t_n)log(1-y_n)\}
\end{matrix}
$$
 이 식과 같은 형태인 함수를 '교차 엔트로피 오차 함수(cross-entrophy error function)'라고 합니다 이 함수를 최소화 하는 것이 본래의 우도 함수를 최적화 하는 것이므로 최적의 상태에서 오차가 어느정도 있는지 나타내는 식이라고 말할 수 있습니다. 일반적으로 이 함수 E를 '오차함수'또는 '손실 함수'라고 합니다.



### 경사하강법

교차 엔트로피 오차 함수에서 파라미터는 w,b 이므로 'w,b로 편미분 해서 0이 되는 값'을 구해야 하는데, 이 값을 해석적으로 식을 풀어서 구하기는 어렵습니다. 그래서 그렇게 하는 대신 반복학습을 통해 파라미터를 순차적으로 갱신하는 방법으로 구하겠습니다. 이런 접근법의 대표적인 기법은 경사하강법(gradient descent)입니다.
$$
w^{(k+1)}=w^{(k)}-\eta\frac{\delta E(w,b)}{\delta w}
$$

$$
b^{(k+1)}=b^{(k)}-\eta\frac{\delta E(w,b)}{\delta b}
$$

이 식에서 $$\eta$$는 학습률 이라는 하이퍼 파라미터이며 모델의 파라미터가 수렴되는 정도를 조절합니다.

일반적으로는 0.1이나 0.01과 같은 적당히 작은 값을 사용합니다.

이제 각 파라미터에 대한 경사를 구해 보겠습니다.
$$
E_n := -\{t_nlogy_n\ + \ (1-t_n)log(1-y_n)\}
$$
식을 위와 같이 두면 웨이트 w의 경사는 다음과 같이 구할 수 있습니다.
$$
\begin{matrix}
\frac{\delta E(w,b)}{\delta w} &=& \sum_{n=1}^N\frac{\delta E_n}{\delta y_n}\frac{\delta y_n}{\delta w}\\
&=& -\sum_{n=1}^N\left(\frac{t_n}{y_n}-\frac{1-t_n}{1-y_n}\right)\frac{\delta y_n}{\delta w}\\
&=& -\sum_{n=1}^N\left(\frac{t_n}{y_n}-\frac{1-t_n}{1-y_n}\right)y_n(1-y_n)x_n\\
&=& -\sum_{n=1}^N(t_n(1-y_n)-y_n(1-t_n))x_n \\
&=& -\sum_{n=1}^N(t_n-y_n)x_n
\end{matrix}
$$


위 2번째와 3번째 식에서는 시그모이드 함수의 미분식 $$\sigma'(x)=\sigma(x)(1-\sigma(x))$$를 이용했습니다. 시그모이드 함수를 사용하면 마지막 식이 깔끔하게 정리된 상태로 도출됩니다. 바이어스 b도 같은 방법으로 계산하면 다음 식을 얻을 수 있습니다.





### 기법

* 단순 퍼셉트론
  * 
* 로지스틱 회귀
* 다중 클래스 로지스틱 회귀
* 다층 퍼셉트론

