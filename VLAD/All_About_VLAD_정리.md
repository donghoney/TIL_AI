## All About VLAD

원문 링크 : https://www.robots.ox.ac.uk/~vgg/publications/2013/arandjelovic13/arandjelovic13.pdf

### Abstract

본 논문의 목적은 질의 이미지가 주어지면 대규모 객체 인스턴스 검색이다. 이러한 시스템의 시작점은 기능 감지 및 설명입니다 (예 : SIFT 사용). 그러나 이 논문의 초점은 스토리지 요구 사항으로 인해 매우 컴팩트 한 이미지 디스크립터가 필요하며 실행시 직접 SIFT 디스크립터에 대한 정보를 액세스 할 수없는 매우 대규모 검색에 대한 것입니다. Jegou '등이 소개 한 최첨단 컴팩트 디스크립터 인 VLAD부터 시작합니다. [8] 이러한 목적을 위해 세 가지 새로운 기여를한다. 첫째, 정규화 방법을 간단하게 변경하면 검색 성능이 크게 향상된다는 것을 알 수있다. 둘째, 어휘 적응은 초기 어휘 학습 후 이미지가 데이터 세트에 추가 될 때 발생하는 문제를 실질적으로 완화 할 수 있음을 보여줍니다. 이 두 가지 방법은 중급 (20k-D ~ 30k-D) 및 소형 (128-D) 디스크립터 모두에 대해 여기에서 조사한 모든 벤치 마크에 대해 최첨단 기술을 새롭게 설정합니다. 우리의 세 번째 기여는 다중 공간 VLAD 표현 인 MultiVLAD로, 이미지의 작은 부분에 대해서만 확장되는 객체를 검색하고 지역화 할 수 있습니다. 원래 이미지 SIFT 설명자를 사용하지 않아도됩니다.



### 1. Instroduction

대규모 특정 개체 검색 영역에서는 지난 10 년 동안 성능이 꾸준히 향상되었습니다. 시각적 단어 (BoW) 공식 [16]의 백을 처음 도입 한 이래로, 디스크립터 [1, 15, 19], 양자화 손실 감소 [6,14,18], 향상된 리콜 [1, 3, 4]. 

​	그러나,이 분야에서 가장 중요한 공헌 중 하나는 J'egou et al.에 의한 지역 집계 기술자 벡터 (Vector of Locally Aggregated Descriptors, VLAD)의 도입이다. [8]. 이 이미지 디스크립터는 매우 큰 이미지 데이터 세트 (예를 들면, 10 억 개의 이미지)에 대한 모든 디스크립터가 주 메모리에 여전히 맞을 수 있도록 (따라서 값 비싼 하드 디스크 액세스를 피하도록) 매우 낮은 차원 (예 : 이미지 당 16 바이트)으로 설계되었다. 그것의 소개는 이미지 디스크립터의 메모리 풋 프린트와 검색 성능 사이의 절충에 대한 새로운 연구 테마를 열었습니다. 평균 정밀도로 측정. 

​	우리는 섹션 2.1에서 VLAD를 검토하지만, 여기서는 시각적 단어 인코딩과 같은 VLAD가 SIFT와 같은 로컬 불변 기술자를 벡터 양자화하여 시작한다는 점을 언급합니다. 클러스터에 할당 된 SIFT 수보다는 클러스터 센터의 차이를 기록하여 BoW 이미지 설명 자와 다릅니다. 그것은 in-plane rotation invariance와 같은 원래의 SIFT 서술자의 불가사의 중 일부를 계승하며 이미지 스케일링 및 클리핑과 같은 다른 변형에 어느 정도 관대합니다. 표준 BoW 접근법과의 또 다른 차이점은 VLAD 검색 시스템이 일반적으로 원래 로컬 설명 자의 사용을 배제한다는 것입니다. 이들은 BoW 시스템에서 공간 검증 및 재평가를 위해 사용되지만, 매우 큰 이미지 데이터 세트의 경우 단일 기계의 메모리에 너무 많은 스토리지를 보유해야합니다. VLAD는 이전의 피셔 (Fisher) 벡터들 [11]과 정신이 비슷하다. 클러스터 중심에 할당 된 SIFT의 분포 양상을 기록하기 때문이다. 

​	예상대로, 논문은 원래 VLAD 공식을 개선하는 방법을 연구하고있다 [2, 5]. 이 백서는 또한 VLAD의 성능을 향상시키기위한 것입니다. 우리는 세 가지 기여를 낸다.

1. 표준화(Intra-normalization) : VLAD 벡터의 몇 가지 큰 구성 요소가 VLAD간에 계산 된 유사성을 역으로 지배 할 수있는 버스트 니스 문제 [7]를 해결하는 VLAD에 대한 새로운 표준화 기법을 제안한다. 새로운 정규화는 간단하며 항상 검색 성능을 향상시킵니다. 

2.  다중 VLAD(Multi-VLAD) : 이미지에 대해 여러 VLAD를 기록 할 때 얻을 수있는 이점을 연구하고 작은 개체 (이미지의 작은 부분 만 다루거나 쿼리에서 상당한 규모 변경이있는 개체)에 대한 검색 성능이 개선되었음을 보여줍니다 영상). 또한 객체 인스턴스에 해당하는 윈도우가 VLAD 타일링보다 더 미세한 해상도로 추정되는 sub-VLAD의 위치 인식 방법을 제안한다. 

3. 어휘 적응(Vocabulary adaption) : 우리는 하나의 데이터 세트 A에 대해 훈련 된 어휘가 다른 데이터 세트 B를 나타 내기 위해 사용되고 어휘력이 B에 대해 훈련 된 어휘를 사용하는 것보다 성능이 떨어지는 어휘 감도의 문제를 조사합니다. 이미지 데이터베이스의 모든 로컬 설명자를 저장하거나 다시 계산할 필요없이 어휘 적응을 통해 VLAD 설명자를 향상시키는 간단한 방법입니다. 

   ​	처음 두 가지 기여는 VLAD 성능 향상을 목표로합니다. 첫 번째 검색은 일반적으로 검색을 향상시키고 두 번째는 부분적으로 중요한 결함을 극복합니다. VLAD는 BoW 방식과 비교하여 규모의 변화에 대한 불변성이 떨어집니다. 세 번째 기여는 예를 들어 이미지 데이터베이스가 시간이 지남에 따라 증가하고 원래의 어휘가 추가 이미지를 잘 표현할 수없는 실제 애플리케이션에서 발생하는 문제를 해결합니다. 

   ​	3 ~ 5 장에서는 Oxford Buildings 5k 및 Holidays 이미지 데이터 세트 벤치 마크를 실행 예제로 사용하여 이러한 각 방법을 자세히 설명하고 이전 VLAD 공식에 비해 성능이 향상되었음을 보여줍니다. 이 방법은 6 장의 대규모 검색 (Oxford 105k 및 Flickr 1M)에 대한 최신 기술과 결합되어 비교됩니다.



### 2. VLAD review, datasets and baselines

먼저 원래의 VLAD 계산과 그 이후의 변형을 설명하고 성능 평가에 사용될 데이터 집합과 어휘 작성에 사용될 데이터 집합 (VLAD 계산에 필요한 클러스터 센터를 얻는)을 간략하게 개관합니다. 

#### 2.1. VLAD 

​	VLAD는 다음과 같이 구성됩니다. 영역은 아핀 불변 검출기를 사용하여 이미지에서 추출되고 128-D SIFT 설명자를 사용하여 설명됩니다. 그런 다음 각 디스크립터는 k 크기의 어휘집 (k는 일반적으로 64 또는 256이므로 클러스터가 매우 거칠다)의 가장 가까운 클러스터에 할당됩니다. k 개의 클러스터 각각에 대해, 나머지 (디스크립터와 클러스터 센터 사이의 벡터 차이)가 누적되고, k 128-D의 레지 듀얼 합이 단일 k × 128 차원 디스크립터로 연결됩니다. 우리는이를 비정규화 된 VLAD(unnormalized VLAD) 라고 부릅니다. VLAD는 피셔 벡터(Fisher vectors) [11] 및 수퍼 벡터 코딩(Super-vector coding) [20]과 같은 잔차를 기록하는 다른 디스크립터와 유사하다는 점에 주목하십시오. 피셔 벡터와 VLAD의 관계는 [12]에서 논의된다. 

​	원래의 기법 [8]에서 VLAD 벡터는 L2 정규화되었다. 그 결과, 부호가있는 제곱근 (SSR) 표준화가 도입되었는데 [5, 9], Perronnin et al. [12] 피셔 벡터의 경우. SSR 정규화 된 VLAD를 얻기 위해, 정규화되지 않은 VLAD의 각 요소는 사인 제곱근 (즉, 요소 $$x_i$$는 $$sign (x_i) \sqrt{| x_i |}$$으로 변형된다)이고 변환 된 벡터는 L2 정규화된다. 우리는 후속편에서 이러한 정규화와 비교할 것이고,이를 우리의 접근 방식에 대한 기준선으로 사용할 것입니다. 

​	Chen et al. [2] 잔차에 대한 다른 정규화 기법을 제안하고 클러스터 경계에 가까운 SIFT 기술자를 생략하는 방법을 연구한다. J'egou and Chum [5]은 VLAD를 두 가지 방식으로 확장한다. 첫째, PCA를 사용하고 저 차원 표현을 무의식 화시키기 위해 미백(whitening)을 사용한다. 둘째, 양자화 손실을 극복하기 위해 다중 (4) 클러스터링을 사용합니다. 둘 다 무시할 수있는 추가 계산 비용으로 상당한 검색 성능 향상을 제공하며 여기에서 사용합니다

#### 2.2. Benchmark datasets and evaluation procedure

​	성능은 옥스포드 건물 및 휴일의 표준 및 공개 이미지 검색 벤치 마크에서 측정됩니다. 두 가지 모두에 대해 손으로 주석을 붙인 근거 진리를 사용하는 사전 정의 된 쿼리 세트가 사용되며 검색 성능은 평균 평균 정밀도 (map)로 측정됩니다. 

​	**Oxford 건물(Oxford buildings)** [13]에는 Flickr에서 다운로드 한 5062 개의 이미지가 포함되어 있으며 Oxford 5k라고도합니다. 이미지 및 직사각형 관심 영역에 의해 지정된 55 개의 쿼리가 있습니다. 대규모 검색을 테스트하기 위해 100k Flickr 이미지로 확장되어 Oxford 105k 데이터 세트를 구성합니다. 

​	**Holidays** [6]에는 500 개의 검색어가 포함 된 개인 휴가 사진이 포함 된 1491 개의 고해상도 이미지가 포함되어 있습니다. 대규모 검색을 위해 1 백만 개의 Flickr 이미지 (Flickr1M [6])가 추가되어 Holidays + Flickr1M이 형성됩니다. 모든 벤치 마크에 대해 [5]의 표준 실험 시나리오를 따른다 : Oxford 5k와 105k의 경우 검출기와 SIFT 서술자는 [10]과 같이 계산된다. Holidays (+ Flickr1M)의 경우 공개적으로 사용 가능한 SIFT 설명자가 사용됩니다. 

​	**어휘 소스(Vocabulary sources)**. (i) Paris 6k [14]는 옥스포드 건물 데이터 세트와 유사하며 옥스포드 빌딩의 독립적 인 데이터 세트로 종종 사용됩니다 [1, 3, 5, 14]; (ii) Flickr에서 다운로드 한 60,000 개의 이미지를 포함하고 Holidays 데이터 세트 [6, 7, 8]의 독립 데이터 세트로 사용되는 Flickr60k [6]; Holidays 데이 터 세트의 첫 번째 k (k는 어휘 크기 임) SIFT 설명자를 단순히 사용하는 'iii) no-vocabulary'. 가장 작은 데이터 세트 (공휴일)는 170 만 SIFT 디스크립터를 포함하는 반면 k는 일반적으로 256보다 크지 않기 때문에이 어휘는 모든 데이터 세트에서 독립적으로 간주 될 수 있습니다.



### 3. Vocabulary adaptation

​	이 섹션에서는 VLAD에 사용 된 클러스터 센터가 데이터 세트와 일치하지 않는 경우 (예 : 다른 데이터 세트에서 얻은 경우 또는 새 데이터가 데이터 세트에 추가 된 경우) 검색 성능을 향상시키기위한 클러스터 적응을 소개합니다. 앞에서 설명한 바와 같이 (섹션 2.1) VLAD는 로컬 디스크립터와 거친 클러스터 센터 간의 차이점을 집계 한 다음 L2 정규화로 구성됩니다. 클러스터를 학습하는 데 사용되는 데이터 세트 (k-means)는 전체 데이터 세트에서 클러스터에 할당 된 모든 벡터의 평균이 클러스터 센터라는 점에서 일관됩니다. 개별 VLAD (단일 이미지에서)의 경우 이것은 물론 또는 과정이 아니며, 일반적으로 다른 데이터 세트를 통해 계산 된 VLAD의 경우도 마찬가지입니다. 아래에서 볼 수 있듯이 불일치는 성능에 심각한 영향을 줄 수 있습니다. 이상적인 솔루션은 현재 데이터 집합을 재구성하는 것이지만 비용이 많이 들며 원본 SIFT 설명자에 액세스해야합니다. 대신, 우리가 제안하는 방법은 재 클러스터링없이 문제를 완화합니다.



![0](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/3-Figure1-1.png)

그림 1 : **서로 다른 클러스터링 하에서의 VLAD 유사도 측정.** Voronoi 셀은 VLAD 디스크립터를 구성하는 데 사용되는 거친 클러스터링을 보여줍니다. 붉은 십자가와 푸른 색 동그라미는 두 개의 서로 다른 이미지에서 추출한 로컬 디스크립터에 해당하는 반면 빨간색과 파란색 화살표는 잔차 (디스크립터와 클러스터 센터 간의 차이)의 합계에 해당합니다. (a)의 클러스터링이 양호하고 (b)의 클러스터링은 그렇지 않은 반면, (a)의 클러스터링이 우수하고 (즉, 대표적이고 데이터 세트 디스크립터와 일치하는) 것으로 가정한다. 클러스터링을 (a)에서 (b)로 변경하면 두 이미지 간의 유사도 (잔차 간 각도의 코사인)의 부호가 음수에서 양수로 극적으로 변경됩니다. 그러나 클러스터 중심 적응을 수행함으로써 잔차가 더 잘 추정되어 (c), (a)에서 클러스터링에 의해 유도 된 이미지 유사성과 일치하는 이미지 유사성을 더 잘 추정 할 수 있습니다.

​	VLAD 기술자 사이의 유사성은 그들 사이의 스칼라 곱으로 측정되며, 이것은 각각의 거친 클러스터에 대한 집계 된 잔차의 스칼라 곱의 합으로 독립적으로 분해됩니다. 하나의 특정 거친 클러스터 k에 대한 유사성에 대한 기여를 고려하십시오. 우리는 x (1) k와 x (2) k를 이미지 1과 2의 모든 디스크립터 집합으로 나타내며, 이들은 동일한 거친 클러스터 k에 할당된다. 두 VLAD 벡터의 전반적인 유사성에 대한 기여도는 다음과 같습니다.
$$
\frac{1}{C^{(1)}}\sum_i(x_{k,i}^{(1)}-\mu_k)^T-\frac{1}{C^{(2)}}(x_{k,j}^{(2)}-\mu_k)
$$
여기서 μk는 클러스터의 중심이며 C (1)과 C (2)는 모든 VLAD 설명자가 단위 표준을 갖도록하는 정규화 상수입니다. 따라서, VLAD 기술자에 의해 유도 된 유사성 척도는 잔차들 사이의 스칼라 곱이 양수이면 증가하고 그렇지 않으면 감소한다. 예를 들어, 그림 1a에 묘사 된 디스크립터 세트는 매우 다르기 때문에 클러스터 센터의 반대편에있다. 따라서 두 이미지의 유사성에 부정적인 영향을 준다. 

​	VLAD 유사성 측정은 클러스터 중심에 의해 크게 영향을 받는다는 것이 분명합니다. 예를 들어, 다른 중심이 사용되면 (그림 1b), 이제 두 세트의 디스크립터는 유사하게 간주되어 두 이미지의 유사성에 긍정적 인 기여를한다. 따라서, 다른 클러스터링은 완전히 다른 유사도 값을 산출 할 수있다. 

​	일관성없는 어휘에 대한 잔차 추정을 개선하기 위해 클러스터 센터 적응을 도입합니다. 즉, 원래 클러스터 센터 μk 대신 계산 잔차 (방정식 (1))와 일치하는 새로운 적응 클러스터 센터 μk를 사용합니다. 알고리즘은 다음 두 단계로 구성됩니다. (i) 적응 클러스터 센터 μk를 동일한 클러스터 k에 할당 된 데이터 세트의 모든 로컬 설명자의 평균으로 계산합니다. (ii) 로컬 디스크립터와 적응 된 센터 μk 간의 차이를 모아서 모든 VLAD 디스크립터를 재 계산한다. 클러스터에 대한 할당이 변경되지 않고 모든 이미지와 각 클러스터에 대한 설명자 합계 만 저장하면되므로 모든 로컬 설명자를 실제로 저장하거나 다시 계산하지 않고도 (ii) 단계를 수행 할 수 있습니다. 

​	그림 1c는 중심 적응을 통해 달성 된 개선을 보여줍니다. 이제 잔차 및 유사성 점수는 그림 1a의 원본 클러스터링을 사용하여 얻은 것과 비슷합니다. 적응 클러스터링의 경우 클러스터 센터는 실제로 데이터 세트에서 클러스터 센터에 할당 된 모든 설명 자의 평균과 같습니다. 따라서, 우리의 클러스터 적응 방식은 원하는대로 일관된 클러스터를 사용하여 얻은 VLAD에 아무런 영향을주지 않습니다.

​	적응의 힘을 설명하기 위해 Flickr60k 어휘를 옥스포드 5k 데이터 세트에 사용하고 원래 어휘와 적응 된 어휘 간의 차이를 측정하는 간단한 테스트가 수행됩니다. k = 256의 적응과 원래 클러스터 중심 사이의 변위의 평균 크기는 0.209이며, 이는 RootSIFT 서술자 자체가 모두 단위 크기를 갖는다는 것을 명심하면서 매우 큽니다. 비교를 위해 파리 어휘를 사용하면 평균 차이는 0.022에 불과합니다.

​	

![all about vlad1](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/4-Figure2-1.png)

​	그림 2 : **검색 성능.** 6 가지 방법, 즉 (i) 기준선 : 표준 VLAD, (ii) 내부 표준화, innorm (4), (3) 중앙 적응, 3 (4) 적응, v) 기준선 : 부호있는 제곱근 SSR, (vi) 지원 기준선 : 적응 후 SSR. 각각의 결과는 네 가지 다른 시험 실행 (4 개의 다른 클러스터링에 해당)에서 얻은 평균 결과에 해당하는 반면 오류 막대는 하나의 표준 편차에 해당합니다. 결과는 RootSIFT [1] 디스크립터와 크기 k = 256의 어휘를 사용하여 생성되었습니다.



​	**결과.** 그림 2는 어휘에 대한 다양한 데이터 세트 소스에서 표준 VLAD와 비교하여 클러스터 센터 적응 (적응)을 사용할 때 얻은 검색 성능의 향상을 보여줍니다. 센터 적응은 어휘가 매우 다른 이미지 데이터베이스에서 계산되거나 전혀 계산되지 않은 경우 특히 결과를 향상시킵니다. 예를 들어, 파리 어휘가있는 공휴일의 경우, mAP는 0.432에서 0.474로 9.7 % 증가합니다. 반면에 no-vocabulary의 경우 mAP는 0.380에서 0.509로 34 % 향상됩니다. 디스크립터 분포가 Holidays 데이터 세트와 유사하기 때문에 Flickr60k 어휘를 사용할 때 개선 효과는 작지만 여전히 존재합니다 (3.297 %에서 0.616 %까지). 개선 추세는 Oxford 5k 벤치 마크에서도 비슷합니다. 



​	**대규모 검색 응용 프로그램.** 시간이 지남에 따라 이미지가 데이터베이스에 추가되는 실제 규모의 대규모 검색의 경우를 생각해보십시오. 예를 들어, 사용자가 Flickr 또는 Facebook에 이미지를 업로드하거나 새로운 웹 사이트에서 Google 색인 이미지를 업로드하는 경우입니다. 이 시나리오에서는 데이터베이스가 커짐에 따라 너무 자주 다시 계산할 수 없기 때문에 (스토리지 및 처리 요구 사항으로 인해) 실용적이지 않고 새로 생성 된 클러스터에 모든 설명자를 재 할당하기 때문에 고정 된 미리 계산 된 어휘를 사용해야합니다. 이 경우, 획득 된 클러스터가 불일치하여 VLAD 유사성 측정치가 나쁘다. 클러스터 중심 적응을 사용하면 클러스터에 대한 설명자 할당이 변경되지 않기 때문에 모든 로컬 설명자를 다시 계산하거나 저장할 필요없이 더 나은 유사성 평가를 계산할 수있는 방법을 제공하므로이 시나리오를 완벽하게 충족시킵니다.



### 4. Intra-normalization

​	이 절에서는 간단한 L2 정규화 [8]와 부호있는 제곱 루팅 [12]과 같은 VLAD 기술자를 표준화하는 현재의 방법이 버스트 한 시각적 특징에 너무 많은 비중을 두는 경향이있어 이미지 유사성의 차선책을 초래한다는 것이 밝혀졌다 . 이 문제를 해결하기 위해 VLAD 정규화를위한 새로운 방법을 제안합니다. 

​	버스트 시각 요소의 문제는 시각적 단어 (BoW) 설정 [7]에 처음 언급되었습니다. 이미지 설명자 벡터의 인위적으로 큰 구성 요소 (예 : 타일링 된 바닥)은 다른 중요한 차원의 기여도가 크게 줄어들 기 때문에 두 이미지 간의 유사성 측정에 크게 영향을 줄 수 있습니다. 이 문제는 BoW 벡터를 요소별로 정사각형으로 제곱하고 큰 값을 할인하여 다시 정규화함으로써 완화되었습니다. 비슷한 방식으로 VLAD는 부호가있는 제곱근 (SSR) 정규화 [5, 9]입니다. 그림 3은 이러한 정규화가 VLAD 벡터의 각 차원에서 전달되는 평균 에너지에 미치는 영향을 보여줍니다. 

​	본 명세서에서는, 잔여 물의 합이 각각의 VLAD 블록 내에서 정규화 된 L2 (즉, 거친 클러스터 내의 잔여 물들의 합) 인 독립적 인 새로운 표준화를 내부 정규화 (intra-normalization)라고 부른다. 원래 VLAD 및 SSR에서와 마찬가지로 전체 벡터의 L2 정규화가 뒤 따른다. 이러한 방식으로, 버스트 한 이미지 특징의 양과 상관없이, VLAD 유사성에 대한 그들의 영향은 그들의 거친 클러스터에 국한되며, 다른 클러스터로부터의 다른 모든 기여와 유사한 정도이다. SSR은 버스트 효과를 줄이는 반면, SSR은 버스트 효과를 할인한다는 사실로 제한됩니다. 대조적으로, 내부 표준화는 에너지 스펙트럼에서 절대적으로 피크를 보여주지 않는도 3c에서 나타난 바와 같이, 버스트를 완전히 억제한다. 

​	**토론.** Intra-normalization의 기하학적 해석은 두 개의 VLAD 벡터의 유사도가 해당 클러스터의 잔차 간의 각도에 의존한다는 것입니다. 이것은 방정식 (1)의 스칼라 곱으로부터 따릅니다 : 잔차가 이제 L2 정규화 되었기 때문에 스칼라 곱은 그 크기가 아닌 잔차의 각도 차이의 코사인에만 의존합니다. Chen et al. [2] 또한 sum 대신에 잔차의 클러스터 별 평균이 계산되는 대체 정규화를 제안했다. 결과 표현은 클러스터의 크기에 크게 영향을받는 잔차의 크기에 따라 달라 지지만 내부 표준화에서는 그렇지 않습니다. 클러스터 중심 적응 (3 절)에 찬성하여 만들어진 모든 주장은 내부 정규화의 영향을받지 않는다는 점에 유의해야한다. 특히, 클러스터 중심 적응에 의해 다루어지는 거친 클러스터링의 품질에 대한 VLAD 유사도 측정의 의존성이 아니라 식 (1)에서 C (1) 및 C (2)의 값만이 변한다.



![all about vlad2](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/5-Figure3-1.png)

그림 3 : **VLAD에 대한 다양한 정규화 기법의 효과.** 플롯은 휴일 데이터 세트의 모든 이미지에서 VLAD의 각 차원에 대한 값의 표준 편차 (즉, 에너지)를 보여줍니다. 녹색 선은 각 클러스터 센터와 연관된 VLAD의 블록을 구분합니다. 원래의 L2 정규화 기법 (3a) 하에서 VLAD 벡터의 몇 가지 구성 요소들 주위에서만 에너지가 강하게 집중된다는 것을 알 수있다. 이러한 피크는 VLAD 유사성 점수에 크게 영향을 미치며 SSR은 실제로 그 효과를 할인 할 수 있습니다 (3b). 그러나 SSR을 사용하더라도 동일한 수의 구성 요소가 상당한 양의 에너지에 책임이 있으며 유사성 점수를 편향시킬 수 있습니다. (c) 내부 표준화는이 효과를 완전히 완화시킨다 (4 절 참조). 검색 성능 (mAP)의 상대적 향상은 SSR 및 VLAD에 비해 각각 innorm을 사용하여 7.2 % 및 13.5 %입니다. 세 가지 실험은 클러스터 중심 적응을 통해 파리에서 배운 크기 k = 64 (구성 요소를 볼 수 있도록 작음)의 어휘로 휴일에 수행되었습니다. 

​	**결과.** 그림 2에서 볼 수 있듯이 센터 적응 (적응)과 결합 된 인트라 정규화 (innorm)는 항상 검색 성능을 향상시키고 다른 VLAD 정규화 기법, 즉 L2 정규화 및 SSR이 적용된 원래의 VLAD보다 지속적으로 성능이 우수합니다. 내부 표준화 (adapt + innorm)를 사용한 센터 적응은 차선책 (adapt + SSR)보다 월등히 뛰어납니다. Oxford 5k와 Holidays의 평균 상대 개선율은 각각 4.7 %와 6.6 %입니다. 센터 적응이없는 SSR과 비교할 때 우리의 개선 사항은 훨씬 더 분명합니다 : Oxford 5k 및 휴일 각각 35.5 % 및 27.2 %.



### 5. Multiple VLAD descriptors

​	이 섹션에서는 이미지를 단일 VLAD로만 표시하는 대신 VLAD로 이미지 타일링의 이점을 조사합니다. 앞에서와 같이 우리의 제약 조건은 메모리 공간이며 모든 성능 향상은 이미지의 원래 SIFT 설명자로 돌아가서는 안됩니다. 이미지의 작은 부분만을 대상으로하는 개체를 대상으로합니다 (VLAD는 BoW에 비해 성능이 떨어지는 것으로 알려져 있음). 먼저 검색을 향상시키는 방법을 설명하고 두 번째로 해당 지역 및 규모를 예측하는 방법을 설명합니다. VLAD는 공간 정보를 저장하지 않습니다.) 

​	여러 VLAD 설명자 (MultiVLAD)는 3 개의 배율로 일반 3 × 3 격자에서 추출됩니다. 가장 정밀한 스케일 9 개 (3 × 3), 중형 스케일 4 개 (2 × 2) (각 타일은 가장 정밀한 스케일의 2 × 2 타일로 구성), 전체 이미지를 커버하는 VLAD 디스크립터가 추출됩니다. 런타임에 질의 된 객체를 다루는 질의 이미지와 관심 영역 (ROI)이 주어지면 단일 VLAD가 ROI를 통해 계산되고 데이터베이스 VLAD 설명자를 통해 일치합니다. 데이터베이스의 이미지에는 해당 VLAD 설명자와 쿼리 사이의 최대 유사성과 동일한 점수가 지정됩니다. 

​	아래에서 볼 수 있듯이 VLAD 디스크립터를 미세한 스케일로 계산하면 작은 객체를 검색 할 수 있지만 스토리지 (메모리) 요구 사항이 증가합니다. 그러나 이미지 당 20 바이트 [8] 인 이미지 당 14 개의 VLAD는 1 억 개의 이미지에 대해 28GB이며, 이는 상품 서버의 주 메모리에 쉽게 저장할 수있는 관리 가능한 데이터 양입니다.

​	 검색 성능을 평가하기 위해 Oxford 5k 데이터 세트에 추가 ROI 주석이 제공됩니다. 원본은 쿼리 이미지에 대한 ROI 만 지정하기 때문입니다. 객체가 300 × 300 픽셀 제곱보다 작 으면 객체는 작다고 간주됩니다. Oxford 5k의 일반적인 이미지는 1024 × 768이므로 임계 값은 이미지의 약 11 %까지 차지하는 객체에 해당합니다. 표준 옥스포드 5k 쿼리를 사용하여 이러한 작은 객체를 포함하는 이미지를 검색하는 평균 평균 정밀도를 측정합니다. 

​	이미지 당 하나의 128-D VLAD와 14 × 128 = 1792-D VLAD의 두 가지베이스 라인을 비교합니다. 후자는 MultiVLAD가 14 배 더 많은 저장 공간을 요구하기 때문에 공정한 비교를 위해 포함됩니다. MultiVLAD는 0.102의 mAP를 달성하며 이는 0.025의 mAP만을 산출하는 단일 128-D VLAD 기술자와 0.073의 mAP, 즉 39.7 % 개선을 얻는 1792-D VLAD를 능가합니다. MultiVLAD는 4002보다 작은 임계 값에 대해 지속적으로 1792-D VLAD를 능가하며 이미지의 상당 부분 (20 % 이상)을 점유하는 객체보다 성능이 우수합니다.

​	**구현 세부 사항.** 3 × 3 그리드는 수평 및 수직 축을 세 개의 동일한 부분으로 분할하여 생성됩니다. 이미지 테두리 근처의 잠재적 인 특징없는 영역 (예 : 많은 이미지의 상단에있는 하늘에는 종종 관심 지점 감지가 포함되지 않음)을 설명하기 위해 모든 관심 지점을 포함하는 가장 작은 경계 상자로 그리드의 외부 경계를 조정합니다. 이미지에 대한 모든 여러 VLAD는 비표준 VLAD의 통합 이미지를 사용하여 효율적으로 계산할 수 있습니다.

![All about vlad3](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/6-Figure4-1.png)

그림 4 : **영역 중첩으로 인한 VLAD 유사성의 변이** (b) 각 지점 (x, y)에 플롯 된 값은 (a)의 관심 영역 (VLI)의 VLAD와 200x200에서 추출 된 VLAD 사이의 VLAD 유사성 (두 VLAD 간의 스칼라 곱)에 해당합니다. (x, y)를 중심으로 한 픽셀 패치. (c) (a)의 ROI가 적용되는 (b)의 각 패치의 비율. (d) (c) ~ (b)의 선형 회귀에 의해 얻어진 잔차. (e) (b)와 (c) 중간을 가로 지르는 1 차원 수평 슬라이스. (d)와 (e)의 잔차는 매우 작기 때문에 VLAD 유사점은 영역 중첩에 대한 매우 우수한 선형 추정치입니다. 

![Figure 5: Fine versus greedy localization. Localized object: ground truth annotation (green); greedy method (red dashed rectangles); best location using the fine method of section 5.1 (yellow solid rectangles).](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/6-Figure5-1.png)

그림 5 : **Fine versus greedy localization.** 지역화 된 대상 : 지상 진실 주석 (녹색); 탐욕스러운 방법 (붉은 점선의 직사각형); 가장 좋은 위치는 섹션 5.1의 미세한 방법 (노란색 솔리드 직사각형)을 사용하는 것입니다.

### 5.1. Fine object localization

​	결과 ROI와 결과 이미지의 Multi-VLAD에 포함 된 모든 VLAD 사이의 유사도 점수를 통해 결과 이미지 내의 해당 위치 추정치를 얻는 방법을 보여줍니다. 이 방법을 유도하기 위해 이미지의 각 200x200 서브 윈도우에 대해 (타겟 ROI의 VLAD에 대한) VLAD 유사도가 (타겟 ROI와의) 오버랩과 비교되는 그림 4를 고려하십시오. 상관 관계가 분명하며 선형 회귀를 사용하여이를 모델링합니다. 절차는 시각적 인 위치 파악을위한 [17]의 보간법과 유사합니다. 

​	**구현 세부 사항.** 유사성 스코어 벡터 s는 질의 ROI VLAD와 결과 이미지의 Multi-VLAD의 이미지 타일에 해당하는 VLAD 사이에서 계산됩니다. 그런 다음 이미지 타일에서의 중복이 선형 스케일링에서 이러한 유사도 점수와 일치하는 결과 이미지에서 ROI를 찾습니다. 여기서, ROI r과 이미지 타일 간의 오버랩 v (r)은 ROI에 의해 커버되는 이미지 타일의 비율로서 계산된다. 최상의 ROI 인 rbest는 다음과 같이 잔차를 최소화하여 결정됩니다.
$$
r_{best} = \underset{r}{argmin} \ \underset{\lambda}{min}||\lambda v(r)-s||
$$
어떤 부정적인 유사점도 0으로 클리핑됩니다. 회귀 된 중첩 점수는 그림 4d 및 4e의 작은 잔차로 나타낸 것처럼 유사성 점수를 매우 잘 모방합니다. 

​	임의의 ROI r에 대해 쉽게 계산 된 주어진 오버랩 스코어 v (r)는 (2)의 내부 최소화가 간단한 최소 제곱 문제이므로 닫힌 형태의 솔루션을 사용하여 최적으로 풀 수 있음을 유의하십시오. 주어진 r에 대한 표현을 최소화하는 것은 λ = sTv (r) v (r) Tv (r)이다. 

​	전체 최소화 문제를 해결하기 위해 우리는 가능한 모든 직사각형 ROI의 이산 공간에서 무차별 대항 탐색을 수행합니다. 이산화 된 공간은 모서리가 이미지 상에 중첩 된 매우 미세한 (30 x 30) 정규 그리드와 일치하는 모든 직사각형으로 구성된다. 즉, x 및 y 좌표 각각에 대해 고려되는 31 개의 별개의 값이있다. 0이 아닌 영역을 가진 가능한 모든 사각형의 수는 31 2 2이며 이는 216k입니다. 

​	검색 절차는 최소 제곱 피팅이 단순한 14-D 스칼라 곱 계산으로 수행되므로 전체 프로 세 스는 단일 코어 3GHz 프로세서에서 이미지 당 14ms를 소비하므로 매우 효율적입니다. 

​	**현지화 정확성.** 로컬 리 제이션 품질을 평가하기 위해 옥스포드 5k 데이터 세트에서 지상 진리 및 추정 된 객체 위치 및 스케일을 오버랩 스코어 (즉, 2 개의 ROI의 교차 및 결합 영역 사이의 비율)로 비교한다. 검색 성능 평가를위한 평균 평균 정밀도 (map) 점수 계산과 유사한 방식으로, 지역화 평가를 위해 각 쿼리에 대해 평균 중첩 점수가 계산되고 평균 평균 중첩 점수를 얻기 위해 여러 쿼리에 대해 평균이 계산됩니다.

​	지역 설명자의 경우 우리는 파리에서 훈련되고 128-D까지 투영 된 여러 어휘로 중심 적응 및 내부 표준화를 사용하는 Multi-VLAD 설명자를 사용합니다. 이 설정은 Oxford 5k에서 0.518의 mAP를 산출합니다. 정밀 로컬 리 제이션 방법은 욕심 및 전체 이미지의 두 가지 기준선과 비교됩니다. 전체 이미지 기준선은 전체 이미지 위에 놓인 ROI를 반환하므로 항상 "안전한 선택"으로 돌아가고 0이 아닌 오버랩 스코어를 생성합니다. 욕심 많은 기준선의 경우 Multi-VLAD 검색 시스템은 VLAD 설명 자의 유사성 측면에서 가장 유사한 타일을 쿼리에 반환합니다. 

![all about vladì ëí ì´ë¯¸ì§ ê²ìê²°ê³¼](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/7-Table1-1.png)

표 1 : **풀 사이즈 이미지 기술자 (즉, 차원 감소 이전) : 최첨단 기술과의 비교.** 중간 크기 (20k-D ~ 32k-D)의 이미지 디스크립터는 Oxford 5k 및 휴일 벤치 마크에서 검색 성능 (mAP) 측면에서 비교됩니다. 참조 결과는 J'egou et al. [9]. 공정한 비교를 위해 Oxford 5k 벤치 마크에서 중요한 개선점을 제공하는 탐지기 [10] 및 설명자 [1]를 사용하여 VLAD + SSR을 구현했습니다. 평균 결과는 네 가지 다른 실행 (단어 생성을위한 k-means의 다른 무작위 초기화에 해당)에 대해 평균을 낸다. 단일 최상의 결과는 가장 높은 mAP을 가진 어휘에서 나온 것이다

​	세 시스템의 평균 평균 중첩 점수는 전체 이미지에 대해 0.342, 0.369 및 0.429이며, 탐욕스러운 벌금입니다. 정밀한 방법은 2 개의베이스 라인을 25 %와 16 % 향상시킵니다. 또한 추정 ROI의 중심이 지상 진실 ROI 내에있는 평균 평균 횟수를 측정하고 미세 방법은 0.897의 점수를 달성하여 다시 다른 기업을 훨씬 능가합니다 (0.897). 전체 이미지와 욕심, 각각. 그림 5는 정교하고 탐욕적인 현지화를 질적으로 비교 한 것입니다.



### Results and discussion

​	다음 섹션에서는 VLAD 기술자의 두 가지 개선점, 즉 클러스터 중심 적응 및 내부 표준화를 최첨단 기술과 비교합니다. 먼저 전체 크기 VLAD 디스크립터의 검색 성능을 평가 한 후 차원 축소를 통해 얻은보다 압축 된 디스크립터에 대한 테스트를 수행 한 다음 서로 다른 데이터 세트에서 학습 한 어휘를 사용하여 성능 차이를 평가합니다. 마지막으로 우리는 작은 기술 어를 가진 대규모 실험을보고한다. 이 모든 테스트에서 우리는 k = 256 개의 거친 클러스터로 묶인 RootSIFT 서술자를 사용했으며 Oxford 5k (+ 100k) 및 Holidays (+ Flickr1M)의 경우 파리와 Flickr60k에서 각각 어휘를 학습했습니다. 

​	**전체 크기의 VLAD 설명자.** 표 1은 매체 차원 (20k-D ~ 30k-D)의 디스크립터에 대한 현재의 최첨단 기술에 대한 본 방법의 성능을 보여줍니다. 인트라 - 노멀 라이 제이션이 뒤 따르는 클러스터 센터 적응은 이전의 모든 방법을 능가합니다. Holidays 데이터 셋의 경우 평균 3.2 %, 가장 좋은 경우 4.3 %로 가장 우수한 방법 (향상된 Fisher 벡터 [12])을 얻었으며 옥스포드 5k는 평균 및 최고 사례에서 4.3 % 및 4.9 %의 향상을 달성했습니다.

![Table 2: Low dimensional image descriptors: comparison with state-of-the-art. 128-D dimensional image descriptors are compared in terms of retrieval performance (mAP) on the Oxford 5k and Holidays benchmarks. Most results are obtained from the paper of JeÌgou et al. [9], apart from the recent multiple vocabulary (Multivoc) method [5]. The authors of Multivoc do not report the performance of their method using VLAD on Oxford 5k, so we report results of our reimplementation of their method.](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/25d0fa49ca846370ff4796a6ac6688a42cf50f77/7-Table2-1.png)

표 2 : **저 차원 이미지 기술자 : 최첨단 기술과의 비교.** 128-D 차원 이미지 디스크립터는 Oxford 5k 및 Holidays 벤치 마크에서 검색 성능 (mAP) 측면에서 비교됩니다. 대부분의 결과는 J'egou et al. [9], 최근의 다중 어휘 (Multivoc) 방법 [5]을 제외하고는. Multivoc의 저자는 Oxford 5k에서 VLAD를 사용하여 메서드의 성능을보고하지 않으므로 메서드를 다시 구현 한 결과를보고합니다. 

​	**작은 이미지 설명자 (128-D).** 우리는 하나의 이미지에 대해 여러 개의 VLAD (SSR 포함) 설명을 얻고 PCA를 사용하여 차원 감소를 수행하고 매우 작은 이미지 설명자를 생성하기 위해 여러 가지 어휘를 사용하는 최신 기술 [5] (Multivoc)을 사용합니다 (128-D). 우리는 [5]의 실험 설정을 모방하고 옥스포드 5k 테스트를 위해 파리 6k에서 어휘와 PCA를 배웁니다. Holidays 테스트의 경우 PCA 학습을 위해 사용되는 10k Flickr 이미지 세트를 지정하지 않습니다. 우리는 Flickr1M [6] 데이터 세트의 마지막 10k를 사용합니다. 	표 2에서 알 수 있듯이, 우리의 방법은 모든 현재의 최첨단 방법을 능가합니다. 옥스포드 5k의 경우 5.4 %, 휴일 인 경우 1.8 %입니다. 

**다른 데이터 세트에서 학습 된 어휘 사용의 효과.** 다양한 어휘를 사용할 때 검색 성능이 어떻게 변하는 지 평가하기 위해 각 방법에 대해 달성 된 이상적인 mAP (즉, 어휘가 벤치 마크 데이터 세트 자체에 구축 될 때)의 비율을 측정합니다. 

​	첫째, 표 3에 풀 사이즈 VLAD를 사용하여 옥스포드 5k에 대한 결과를보고합니다. 기본 (VLAD 및 VLAD + SSR)은 부적절한 (Flickr60k) 어휘가 최상의 기준에 대해 이상적인 성능의 68 % VLAD + SSR). adapt + innorm을 사용하면 모든 어휘에 대해 일반적으로 mAP를 개선하는 것 외에도이 점수를 최대 86 %까지 끌어 올릴 수 있습니다. 공휴일 벤치 마크에서도 비슷한 추세가 관찰됩니다 (그림 2 참조).

​	우리는 다음으로 모든 경우에 Multivoc [5]가 PCA와 함께 사용되어 차원 감소 및 미백을 수행하는 128-D 설명 자에 대한 결과를보고합니다. 불일치하는 어휘로 인한 잔여 문제 외에도, PCA가 다른 데이터 세트에서 학습 된 추가 문제가 있습니다. Oxford 5k 용 adapt + innorm과 Flickr60k 어휘를 사용하면 이상적인 성능의 59 %를 얻을 수 있습니다. 위의 전체 크기 벡터로 얻은 86 %보다 훨씬 나쁩니다. 성능 저하에도 불구하고 adapt + innorm은 여전히 최상의 기준 (VLAD + SSR)보다 4 % 뛰어납니다. 향후 연구 방향은 부적절한 PCA 훈련 세트의 영향을 완화하는 방법을 조사하고, 소형 VLAD 기술자에 대한 상대적 성능을 개선하는 것입니다. 

​	**대규모 검색.** 최대 1 백만 개의 이미지와 콤팩트 이미지 디스크립터 (128-D)의 데이터 세트로 철저한 최근 접 탐색을 수행 할 수 있습니다. 예를 들어 [5]에서 철저한 검색은 12 코어 3GHz 시스템에서 쿼리 당 6ms를보고하는 1 백만 개의 128-D 차원 벡터에서 수행됩니다. 효율적인 근접 근사 방법을 사용하면 1 백만 개 이상의 이미지로 확장 할 수 있습니다. 위에서 설명한 것처럼 동일한 128-D 설명자 (Multivoc을 사용하여 128-D로 축소 된 adapt + innorm VLAD)가 사용됩니다. Oxford 105k에서 우리는 Multipoc VLAD + SSR의 재 구현 인 최상의 기준선보다 5.6 % 개선 된 0.374의 mAP를 달성합니다. 비교할 데이터 세트에 대해 소형 이미지 설명자에 대해 이전에보고 된 결과가 없습니다. Holidays + Flickr1M에서 adapt + innorm은 Multivoc VLAD + SSR 0.370에 비해 0.378입니다. 이 데이터 세트에 대해 이전에보고 된 가장 우수한 mAP는 0.370입니다 (풀 사이즈 VLAD 및 근사 이웃 탐색 [9]을 사용하는 VLAD + SSR 사용). 따라서 우리는 여기에서 두 가지 데이터 세트에 새로운 최첨단 기술을 도입했습니다.



### 7. Conclusions and recommendations

​	우리는 클러스터 중심 적응, 인트라 정규화 및 MultiVLAD와 같은 다양한 측면에서 표준 VLAD 기술자를 향상시키는 세 가지 방법을 제시했다. 

​	**클러스터 센터 적응**은 컨텐츠가 추가되면 이미지 데이터베이스가 시간이 지남에 따라 커지는 대규모 검색 작업에 유용한 방법입니다. 모든 로컬 설명자를 다시 계산하거나 저장할 필요없이 나쁜 시각적 어휘 사용의 영향을 어느 정도 완화합니다. 

​	**Intra-normalization**은 버스트 한 시각적 요소를 완전히 억제하고 VLAD 디스크립터 간의 유사성을보다 잘 측정하기 위해 도입되었습니다. 그것은 최고의 VLAD 정규화 계획으로 입증되었습니다. 그러나 일관된 클러스터가 사용되고 센터 적응이 수행되지 않을 때 SSR에 의해 내부 표준화가 종종 수행되기 때문에 적절한 시각적 어휘 또는 센터 적응과 함께 내부 표준화를 항상 사용하는 것이 좋습니다. 이 논문의 범위를 벗어나지 만, 인트라 표준화 된 VLAD는 원래 VLAD 공식보다 이미지 분류 성능을 향상시킵니다. 



### References

[1] R. Arandjelovi´c and A. Zisserman. Three things everyone should know to improve object retrieval. In Proc. CVPR, 2012. 

[2] D. Chen, S. Tsai, V. Chandrasekhar, G. Takacs, H. Chen, R. Vedantham, R. Grzeszczuk, and B. Girod. Residual enhanced visual vectors for on-device image matching. In Asilomar, 2011. 

[3] O. Chum, A. Mikulik, M. Perdoch, and J. Matas. Total recall II: ˇ Query expansion revisited. In Proc. CVPR, 2011. 

[4] O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman. Total recall: Automatic query expansion with a generative feature model for object retrieval. In Proc. ICCV, 2007. 

[5] H. J´egou and O. Chum. Negative evidences and co-occurrences in image retrieval: the benefit of PCA and whitening. In Proc. ECCV, 2012. 

[6] H. J´egou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency for large scale image search. In Proc. ECCV, 2008. 

[7] H. J´egou, M. Douze, and C. Schmid. On the burstiness of visual elements. In Proc. CVPR, Jun 2009. 

[8] H. J´egou, M. Douze, C. Schmid, and P. P´erez. Aggregating local descriptors into a compact image representation. In Proc. CVPR, 2010. 

[9] H. J´egou, F. Perronnin, M. Douze, J. S´anchez, P. P´erez, and C. Schmid. Aggregating local images descriptors into compact codes. IEEE PAMI, 2012. 

[10] M. Perdoch, O. Chum, and J. Matas. Efficient representation of loca ˇ l geometry for large scale object retrieval. In Proc. CVPR, 2009. 

[11] F. Perronnin and D. Dance. Fisher kernels on visual vocabularies for image categorization. In Proc. CVPR, 2007. 

[12] F. Perronnin, Y. Liu, J. Sanchez, and H. Poirier. Large-scale image retrieval with compressed fisher vectors. In Proc. CVPR, 2010. 

[13] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object retrieval with large vocabularies and fast spatial matching. In Proc. CVPR, 2007. 

[14] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Lost in quantization: Improving particular object retrieval in large scale image databases. In Proc. CVPR, 2008.

 [15] K. Simonyan, A. Vedaldi, and A. Zisserman. Descriptor learning using convex optimisation. In Proc. ECCV, 2012. 

[16] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc. ICCV, 2003. 

[17] A. Torii, J. Sivic, and T. Pajdla. Visual localization by linear combination of image descriptors. In International Workshop on Mobile Vision, 2011. 

[18] J. C. van Gemert, J. M. Geusebroek, C. J. Veenman, and A. W. M. Smeulders. Kernel codebooks for scene categorization. In Proc. ECCV, 2008. 

[19] S. Winder, G. Hua, and M. Brown. Picking the best daisy. In Proc. CVPR, 2009. 

[20] X. Zhou, K. Yu, T. Zhang, and T. S. Huang. Image classification using super-vector coding of local image descriptors. In Proc. ECCV, 2010.