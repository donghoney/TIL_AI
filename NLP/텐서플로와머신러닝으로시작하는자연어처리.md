## 텐서플로와 머신러닝으로 시작하는 자연어처리

-로지스틱 회귀부터 트랜스포머 챗봇까지



#### 2. 자연어 처리 개발 준비



#### tf.data

머신러닝에서 문제를 해결할 때 많은 시간이 데이터를 다루는 데 소용된다.

데이터 분석, 전처리, 파이프 라인을 만드는 과정에서 약 70~80%의 시간을 소비한다고 해도 과언이 아니다.

GPU가 병목현상 없이 효율적으로 데이터를 처리할 수 있게 최적화를 지원한다.

tf.data는 다양한 데이터 접근 방식이 존재한다. 그중 하나인 사용법이 직관적인 tf.data.Dataset.from_tensor_slinces를 채택하여 사용한다.

```python
import os
import tensorflow as tf
import numpy as np

from tensorflow.keras import preprocessing

samples = ['너 오늘 이뻐보인다',
          '나는 오늘 기분이 더러워',
          '끝내주는데, 좋은 일이 있나봐',
          '나 좋은 일이 생겼어',
          '환상적인데, 정말 좋은 것 같아']
label = [[1],[0],[1],[1],[0],[1]]

tokenizer = preprocessing.text.Tokenizer() # 객체 생성
tokenizer.fit_on_texts(samples) # 시퀀스 (딕셔너리) 생성 : 사전이 될 매핑이 된 딕셔너리
sequences = tokenizer.texts_to_sequences(samples) # 데이터 담기

word_index = tokenizer.word_index # 숫자로 바뀐 데이터

print('수치화된 텍스트 데이터 : \n',sequences)
print('각 단어의 인덱스 : \n',word_index)
print('라벨 :',label)
```

```
수치화된 텍스트 데이터 :
[[4 1 5 6]
[7 1 8 9 ]
...
[17 18 19 20]]
각 단어의 인덱스 :
{'오늘' : 1, '좋은' : 2, '일이':3, ... '좋은거' : 19, '같아':20}
라벨 : [[1],[0],[1],[1],[0],[1]]
```



시퀀스로 수치화시킨 데이터와 레이블을 한세트로 묶어 tf.data를 활용해 처리해보자.

```python
dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
iterator = dataset.make_one_shot_iterator()
next_data =iterator.get_next()
```

* tf.data.Dataset.from_tensor_slices() : 주어진 데이터 x,y를 묶어서 조각으로 만들고 함께 쓸수있게 해준다.
* make_one_shot_iterator() : 데이터를 하나씩 사용할 수 있게 만든다. 데이터는 이터레이터 형식으로 정의된다.
* get_next() : 이터레이터의 함수로 데이터가 하나씩 나오게 되는 구조다.

한번 next_data 객체를 이용해서 세션을 실행하고 계속 불러오자

```python
with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```

* 배치사이즈로 묶기

```python
BATCH_SIZE = 2 # 배치사이즈
dataset = tf.data.Dataset.from_tensor_slices((sequences, label))
dataset = dataset.batch(BATCH_SIZE) # batch() 함수로 원하는 배치사이즈 만큼의 인자를 받아 묶을 수 있다. 
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```

* 데이터를 셔플하기

```python
dataset = tf.data.Dataset.from_tensor_slices((sequences, label))
dataset = dataset.shuffle(len(sequences)) # shuffle() 함수를 적용할 때 인자 값으로 데이터의 전체 길이를 넣으면 된다. 
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
	while True:
		try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```

* 에폭 설정하기(전체 데이터를 몇번 사용하는지를 설정하기)

```python
EPOCH = 2

dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
dataset = dataset.repeat(EPOCH) # repeat() 함수에 인지값에 epoch값을 넣어준다.
iteraotr = dataset.make_one_shot_iterator()
next_data = iterator.get_next()
with tf.Session() as sess:
	while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOf RangeError:
            break
```

* 모델에 따라 입력값이 하나가 아니라 두개 이상이 될 수도 있는데, 이 때 라벨을 제외한 나머지 데이터를 하나의 입력값으로 묶기 위해 매핑 과정을 거친다.

1. 하나의 입력값을 딕셔너리에 넣기

```python
def mapping_fn(X,Y=None):
    input = {'x':X}
    label = Y
    return input, label

dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
dataset = dataset.map(mapping_fn)
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.error.OutOfRangeError:
            break
```

2. 두개의 입력값을 딕셔너리에 넣기

```python
def mapping_fn(X1,X2,Y=None):
    input = {'x1':X1,'x2':X2}
    label = Y
    return input, label
```



* 배치, 셔플, 반복, 매핑 과정을 하나로 묶어서 사용하는 과정

```python
BATCH_SIZE = 2
EPOCH = 2

def mapping_fn(X, Y=None):
    input = {'x':X}
    label = X
    return input, label

dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
dataset = dataset.map(mapping_fn)
dataset = dataset.shuffle(len(sequences))
dataset = dataset.batch(BATCH_SIZE)
dataset = dataset.repeat(EPOCH)
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```





#### 2.3 KoNLPy

대표적인 한글 토크나이징 라이브러리 : KoNLPy
- 형태소 단위 토크나이징
- 형태소 분석 및 품사 태깅 기능
- KoNLPy는 다음과 같은 다양한 형태소 분석, 태깅 라이브러리를 파이썬에서 쉽게 사용할 수 있도록 모아놓았다.
  - Hannanum: 한나눔. KAIST Semantic Web Research Center 개발.
    - <http://semanticweb.kaist.ac.kr/hannanum/>
  - Kkma: 꼬꼬마. 서울대학교 IDS(Intelligent Data Systems) 연구실 개발.
    - <http://kkma.snu.ac.kr/>
  - Komoran: 코모란. Shineware에서 개발.
    - <https://github.com/shin285/KOMORAN>
  - Mecab: 메카브. 일본어용 형태소 분석기를 한국어를 사용할 수 있도록 수정.
    - <https://bitbucket.org/eunjeon/mecab-ko>
  - Okt(Open Korean Text): 오픈 소스 한국어 분석기. 과거 트위터 형태소 분석기.
    - <https://github.com/open-korean-text/open-korean-text>
- `nouns` : 명사 추출
- `morphs` : 형태소 추출
- `pos` : 품사 부착

```python
from konlpy.tag import Okt

#okt = Okt()
#okt.morphs() : 텍스트를 형태소 단위로 나눈다. 옵션은 norm과 stem이 있음
#okt.nouns() : 텍스트에서 명사만 뽑아낸다.
#okt.phrases() : 텍스트에서 어절을 뽑아낸다.
#okt.pos() : 각 품사를 태깅하는 역할을 한다. 품사를 태깅한다는 것은 주어진 텍스트를 형태소 단위로 나누고, 나눠진 각 형태소를 그에 해당하는 품사와 함께 리스트화하는 것을 의미한다. 이 함수에서도 옵션을 설정할 수있다.
#norm, stem 옵션이 있고, 추가적으로 join 함수가 있다. 이 옵션을 True로 설정하면 나눠진 형태소와 품사를 형태소/품사 형태로 같이 붙여서 리스트화 한다.

text = '한글 자연어 처리는 재밌다 이제부터 열심히 해야지 ㅎㅎㅎ'
```



```python
print(okt.morphs(text))
print(okt.morphs(text, stem=True)) # 형태소 단위로 나눈 후 어간을 추출

> > > ['한글','자연어','처리','는','재밌다','이제','부터','열심히','해야지','ㅎㅎㅎ']
> > > ['한글','자연어','처리','는','재밌다','이제','부터','열심히','하다','ㅎㅎㅎ']
```

```python
print(okt.nouns(text))
print(okt.phrases(text))

> > > ['한글','자연어','처리','이제']
> > > ['한글','한글 자연어 처리','이제','자연어','처리']
```

```python
print(okt.pos(text))
print(okt.pos(text, join=True)) # 형태소와 품사를 붙여서 리스트화

> > > [('한글','Noun'),('자연어','Noun'),('처리','Noun'),('는','Josa'),('재밌다','Adjective'),('이제','Noun'),('부터','Josa'),('열심히','Adverb'),('해야지','Verb'),('ㅎㅎㅎ','KoreanParticle')]
> > > 이부분은 '한글/Noun'. ... 이런식으로...
```



```python
from konlpy.tag import Okt
from konlpy.tag import Kkma
from konlpy.corpus import kolaw
from konlpy.corpus import kobill


kkma = Kkma()
okt = Okt()

text = '한글 자연어 처리는 재밌다. 열심히 해야겠다 정말 열심히 해야겠다!! ㅎㅎㅎ'

#print(okt.morphs(text))
#print(okt.morphs(text, stem=True))

#print(okt.nouns(text))
#print(okt.phrases(text))
#print(okt.pos(text))
#print(okt.pos(text,join=True))
#print(okt.pos(text,norm=True))

#print(kolaw.open('constitution.txt').read()[:])
print(kobill.open('1809890.txt').read())
```

## 어떤 분석기를 써야할까?

말뭉치 종류나 상황에 따라 다른 분석기를 써야할 겁니다. 제 실험을 일반화해서 적용하기엔 무리가 있습니다. 다만 제 느낌상 아래와 같은 상황에 해당 분석기를 쓰면 좋을 것 같습니다.

- 빠른 분석이 중요할 때 : **옥트**
- 정확한 품사 정보가 필요할 때 : **꼬꼬마**
- 정확성, 시간 모두 중요할 때 : **코모란**

가장 좋은 방법은 보유하고 있는 말뭉치 일부를 테스트용으로 형태소 분석을 해보는 것입니다. 아래 예제코드가 있으니 참고하시기 바랍니다.

```python
import time
from konlpy.tag import Kkma, Okt, Komoran
pos_taggers = [('kkma', Kkma()), ('okt', Okt()), ('Komoran', Komoran())]
results = []
for name, tagger in pos_taggers:
    tokens = []
    process_time = time.time()
    for text in texts:
        tokens.append(tagger.pos(text))
    process_time = time.time() - process_time
    print('tagger name = %10s, %.3f secs' % (name, process_time))
    results.append(tokens)
```



### 4. 텍스트 분류

#### 4.1 영어 텍스트 분류

영화 리뷰데이터 분류 
Bag of Words Meets Bag of Popcorn

#### 4.1 문제 소개

데이터 : 영화리뷰 텍스트, 긍정 혹은 부정 감정
목표 : 데이터 로드, 데이터 전처리 , 모델링

#### 4.1 데이터 분석 및 전처리

IMDB 전처리 순서 : 
1. 데이터 로드 
2. EDA(탐색적 자료 분석) 
3. HTML 및 문장 부호 제거 
4. 불용어 제거 
5. 단어 최대 길이 설정 
6. 단어 패딩 
7. 벡터 표상화
8. 모델링

* Kaggle API 사용 데이터 다운로드

```
kaggle competitions download -c word2vec -nlp-tutorial
```

```python
import zipfile

DATA_IN_PATH = './data_in/'

file_list = ['labeledTrainData.tsv.zip','unlabeledTrainData.tsv.zip','testData.tsv.zip']

for file in file_list:
    zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')
    zipRef.extractall(DATA_IN_PATH)
    zipRef.close()

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline # 그래프를 주피터 노트북에서 바로 그리도록 함

train_data = pd.read_csv(DATA_IN_PATH+'labeledTrainData.tsv',header=0, delimeter'\t\, quoting=3)
train_data.head()
```

여기까지 IMDB데이터를 kaggle에서 다운로드하고 압축을 풀어보았다.

데이터 분석은 다음과 같은 순서로 진행한다.

* 데이터 분석 순서

```
1. 데이터 크기
2. 데이터 개수
3. 각 리뷰의 문자 길이 분포
4. 많이 사용된 단어
5. 긍정, 부정 데이터의 분포
6. 각 리뷰의 단어 개수 분포
7. 특수문자 및 대문자, 소문자 비율
```



우선 학습데이터 개수를 한번 보자.

```python
print('파일 크기 :')
for file in os.listdir(DATA_IN_PATH):
    if 'tsv' in file and 'zip' not in file:
        print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')

print('전체 학습 데이터의 개수 : {}'.format(len(train_data)))

> > > 전체 학습 데이터의 개수 : 25000
```



학습 데이터 개수는 25000개이다. 
이제 각 데이터 문자의 길이를 알아보자.

```python
train_length = train_data['review'].apply(len)
train_length.head()

> > > 0   2304 
> > > 1   948
> > > 2   2451
> > > 3   2247
> > > 4   2233
> > > Name : review, dtype : int64
```



해당 변수에는 각 리뷰의 길이가 담겨있다. 이 변수를 사용해 히스토그램을 그려보자. matplotlib을 사용한다.

```python
# 그래프에 대한 이미지 크기 선언

# figsize : (가로, 세로) 형태의 튜플로 입력

plt.figure(figsize=(12,5))

# 히스토그램 선언

# bins : 히스토그램 값에 대한 버킷 범위

# range : x 축 값의 범위

# alpha : 그래프 색상 투명도

# color : 그래프 색상

# label : 그래프에 대한 라벨

plt.hist(train_length, bins = 200, alpha=0.5, color = 'r', label='word')
plt.yscale('log', nonposy ='clip')

# 그래프 제목

plt.title('Log-Gistogram of length of review')

# 그래프 x축 레이블

plt.xlabel('Length of review')

# 그래프 y축 레이블

plt.ylabel('Number of review')

print('리뷰 길이 최댓값 : {}'.format(np.max(train_length)))
print('리뷰 길이 최솟값 : {}'.format(np.min(train_length)))
print('리뷰 길이 평균값 : {}'.format(np.mean(train_length)))
print('리뷰 길이 표준편차 :{}'.format(np.std(train_length)))
print('리뷰 길이 중간값 :{}'.format(np.median(train_length)))

# 사분위에 대한 경우 0~100 스케일로 되어 있음

print('리뷰 길이 제 1사분위 : {}'.format(np.percentile(train_length, 25)))
print('리뷰 길이 제 3사분위 : {}'.format(np.percentile(train_length, 75)))

> > > 리뷰 길이 최댓값 : 13710
> > > 리뷰 길이 최솟값 : 54
> > > 리뷰 길이 평균값 : 1329.71
> > > 리뷰 길이 표준편차 : 1005.21
> > > 리뷰 길이 중간값 : 983.0
> > > 리뷰 길이 제 1사분위 : 705.0
> > > 리뷰 길이 제 3사분위 : 1619.0

plt.figure(figsize=(12,5))

# 박스 플롯 생성

# 첫 번째 인자 : 여러 분포에 대한 데이터 리스트를 입력

# labels : 입력한 데이터에 대한 라벨

# showmeans : 평균값을 마크함

plt.boxplot(train_length, labels=['counts'], showmeans=True)
```



박스 플롯 그래프를 통해 데이터를 살펴보면 데이터 길이가 대부분 2000 이하로 평균이 1500이하인데, 길이가 4000이상인 이상치 데이터도 많이 분포되어 있음을 알수 있다.

이제부터는 많이 사용된 단어를 확인해보자

```python
#pip install WordCloud
from wordcloud import WordCloud
cloud = WordCloud(width = 800, height=600).fenerate(' '.join(train_data['review']))
plt.figure(figsize=(20,15))
plt.imshow(cloud)
plt.axis('off')

fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(6,3)
sns.countplot(train_data['sentiment'])
```



