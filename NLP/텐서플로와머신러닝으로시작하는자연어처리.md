## 텐서플로와 머신러닝으로 시작하는 자연어처리

-로지스틱 회귀부터 트랜스포머 챗봇까지



#### 2. 자연어 처리 개발 준비



#### tf.data

머신러닝에서 문제를 해결할 때 많은 시간이 데이터를 다루는 데 소용된다.

데이터 분석, 전처리, 파이프 라인을 만드는 과정에서 약 70~80%의 시간을 소비한다고 해도 과언이 아니다.

GPU가 병목현상 없이 효율적으로 데이터를 처리할 수 있게 최적화를 지원한다.

tf.data는 다양한 데이터 접근 방식이 존재한다. 그중 하나인 사용법이 직관적인 tf.data.Dataset.from_tensor_slinces를 채택하여 사용한다.

```python
import os
import tensorflow as tf
import numpy as np

from tensorflow.keras import preprocessing

samples = ['너 오늘 이뻐보인다',
          '나는 오늘 기분이 더러워',
          '끝내주는데, 좋은 일이 있나봐',
          '나 좋은 일이 생겼어',
          '환상적인데, 정말 좋은 것 같아']
label = [[1],[0],[1],[1],[0],[1]]

tokenizer = preprocessing.text.Tokenizer() # 객체 생성
tokenizer.fit_on_texts(samples) # 시퀀스 (딕셔너리) 생성 : 사전이 될 매핑이 된 딕셔너리
sequences = tokenizer.texts_to_sequences(samples) # 데이터 담기

word_index = tokenizer.word_index # 숫자로 바뀐 데이터

print('수치화된 텍스트 데이터 : \n',sequences)
print('각 단어의 인덱스 : \n',word_index)
print('라벨 :',label)
```

```
수치화된 텍스트 데이터 :
[[4 1 5 6]
[7 1 8 9 ]
...
[17 18 19 20]]
각 단어의 인덱스 :
{'오늘' : 1, '좋은' : 2, '일이':3, ... '좋은거' : 19, '같아':20}
라벨 : [[1],[0],[1],[1],[0],[1]]
```



시퀀스로 수치화시킨 데이터와 레이블을 한세트로 묶어 tf.data를 활용해 처리해보자.

```python
dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
iterator = dataset.make_one_shot_iterator()
next_data =iterator.get_next()
```

* tf.data.Dataset.from_tensor_slices() : 주어진 데이터 x,y를 묶어서 조각으로 만들고 함께 쓸수있게 해준다.
* make_one_shot_iterator() : 데이터를 하나씩 사용할 수 있게 만든다. 데이터는 이터레이터 형식으로 정의된다.
* get_next() : 이터레이터의 함수로 데이터가 하나씩 나오게 되는 구조다.

한번 next_data 객체를 이용해서 세션을 실행하고 계속 불러오자

```python
with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```

* 배치사이즈로 묶기

```python
BATCH_SIZE = 2 # 배치사이즈
dataset = tf.data.Dataset.from_tensor_slices((sequences, label))
dataset = dataset.batch(BATCH_SIZE) # batch() 함수로 원하는 배치사이즈 만큼의 인자를 받아 묶을 수 있다. 
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```

* 데이터를 셔플하기

```python
dataset = tf.data.Dataset.from_tensor_slices((sequences, label))
dataset = dataset.shuffle(len(sequences)) # shuffle() 함수를 적용할 때 인자 값으로 데이터의 전체 길이를 넣으면 된다. 
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
	while True:
		try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```

* 에폭 설정하기(전체 데이터를 몇번 사용하는지를 설정하기)

```python
EPOCH = 2

dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
dataset = dataset.repeat(EPOCH) # repeat() 함수에 인지값에 epoch값을 넣어준다.
iteraotr = dataset.make_one_shot_iterator()
next_data = iterator.get_next()
with tf.Session() as sess:
	while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOf RangeError:
            break
```

* 모델에 따라 입력값이 하나가 아니라 두개 이상이 될 수도 있는데, 이 때 라벨을 제외한 나머지 데이터를 하나의 입력값으로 묶기 위해 매핑 과정을 거친다.

1. 하나의 입력값을 딕셔너리에 넣기

```python
def mapping_fn(X,Y=None):
    input = {'x':X}
    label = Y
    return input, label

dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
dataset = dataset.map(mapping_fn)
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.error.OutOfRangeError:
            break
```

2. 두개의 입력값을 딕셔너리에 넣기

```python
def mapping_fn(X1,X2,Y=None):
    input = {'x1':X1,'x2':X2}
    label = Y
    return input, label
```



* 배치, 셔플, 반복, 매핑 과정을 하나로 묶어서 사용하는 과정

```python
BATCH_SIZE = 2
EPOCH = 2

def mapping_fn(X, Y=None):
    input = {'x':X}
    label = X
    return input, label

dataset = tf.data.Dataset.from_tensor_slices((sequences,label))
dataset = dataset.map(mapping_fn)
dataset = dataset.shuffle(len(sequences))
dataset = dataset.batch(BATCH_SIZE)
dataset = dataset.repeat(EPOCH)
iterator = dataset.make_one_shot_iterator()
next_data = iterator.get_next()

with tf.Session() as sess:
    while True:
        try:
            print(sess.run(next_data))
        except tf.errors.OutOfRangeError:
            break
```





#### 2.3 KoNLPy

대표적인 한글 토크나이징 라이브러리 : KoNLPy
- 형태소 단위 토크나이징
- 형태소 분석 및 품사 태깅 기능
- KoNLPy는 다음과 같은 다양한 형태소 분석, 태깅 라이브러리를 파이썬에서 쉽게 사용할 수 있도록 모아놓았다.
  - Hannanum: 한나눔. KAIST Semantic Web Research Center 개발.
    - <http://semanticweb.kaist.ac.kr/hannanum/>
  - Kkma: 꼬꼬마. 서울대학교 IDS(Intelligent Data Systems) 연구실 개발.
    - <http://kkma.snu.ac.kr/>
  - Komoran: 코모란. Shineware에서 개발.
    - <https://github.com/shin285/KOMORAN>
  - Mecab: 메카브. 일본어용 형태소 분석기를 한국어를 사용할 수 있도록 수정.
    - <https://bitbucket.org/eunjeon/mecab-ko>
  - Okt(Open Korean Text): 오픈 소스 한국어 분석기. 과거 트위터 형태소 분석기.
    - <https://github.com/open-korean-text/open-korean-text>
- `nouns` : 명사 추출
- `morphs` : 형태소 추출
- `pos` : 품사 부착

```python
from konlpy.tag import Okt

#okt = Okt()
#okt.morphs() : 텍스트를 형태소 단위로 나눈다. 옵션은 norm과 stem이 있음
#okt.nouns() : 텍스트에서 명사만 뽑아낸다.
#okt.phrases() : 텍스트에서 어절을 뽑아낸다.
#okt.pos() : 각 품사를 태깅하는 역할을 한다. 품사를 태깅한다는 것은 주어진 텍스트를 형태소 단위로 나누고, 나눠진 각 형태소를 그에 해당하는 품사와 함께 리스트화하는 것을 의미한다. 이 함수에서도 옵션을 설정할 수있다.
#norm, stem 옵션이 있고, 추가적으로 join 함수가 있다. 이 옵션을 True로 설정하면 나눠진 형태소와 품사를 형태소/품사 형태로 같이 붙여서 리스트화 한다.

text = '한글 자연어 처리는 재밌다 이제부터 열심히 해야지 ㅎㅎㅎ'
```



```python
print(okt.morphs(text))
print(okt.morphs(text, stem=True)) # 형태소 단위로 나눈 후 어간을 추출

> > > ['한글','자연어','처리','는','재밌다','이제','부터','열심히','해야지','ㅎㅎㅎ']
> > > ['한글','자연어','처리','는','재밌다','이제','부터','열심히','하다','ㅎㅎㅎ']
```

```python
print(okt.nouns(text))
print(okt.phrases(text))

> > > ['한글','자연어','처리','이제']
> > > ['한글','한글 자연어 처리','이제','자연어','처리']
```

```python
print(okt.pos(text))
print(okt.pos(text, join=True)) # 형태소와 품사를 붙여서 리스트화

> > > [('한글','Noun'),('자연어','Noun'),('처리','Noun'),('는','Josa'),('재밌다','Adjective'),('이제','Noun'),('부터','Josa'),('열심히','Adverb'),('해야지','Verb'),('ㅎㅎㅎ','KoreanParticle')]
> > > 이부분은 '한글/Noun'. ... 이런식으로...
```



```python
from konlpy.tag import Okt
from konlpy.tag import Kkma
from konlpy.corpus import kolaw
from konlpy.corpus import kobill


kkma = Kkma()
okt = Okt()

text = '한글 자연어 처리는 재밌다. 열심히 해야겠다 정말 열심히 해야겠다!! ㅎㅎㅎ'

#print(okt.morphs(text))
#print(okt.morphs(text, stem=True))

#print(okt.nouns(text))
#print(okt.phrases(text))
#print(okt.pos(text))
#print(okt.pos(text,join=True))
#print(okt.pos(text,norm=True))

#print(kolaw.open('constitution.txt').read()[:])
print(kobill.open('1809890.txt').read())
```

## 어떤 분석기를 써야할까?

말뭉치 종류나 상황에 따라 다른 분석기를 써야할 겁니다. 제 실험을 일반화해서 적용하기엔 무리가 있습니다. 다만 제 느낌상 아래와 같은 상황에 해당 분석기를 쓰면 좋을 것 같습니다.

- 빠른 분석이 중요할 때 : **옥트**
- 정확한 품사 정보가 필요할 때 : **꼬꼬마**
- 정확성, 시간 모두 중요할 때 : **코모란**

가장 좋은 방법은 보유하고 있는 말뭉치 일부를 테스트용으로 형태소 분석을 해보는 것입니다. 아래 예제코드가 있으니 참고하시기 바랍니다.

```python
import time
from konlpy.tag import Kkma, Okt, Komoran
pos_taggers = [('kkma', Kkma()), ('okt', Okt()), ('Komoran', Komoran())]
results = []
for name, tagger in pos_taggers:
    tokens = []
    process_time = time.time()
    for text in texts:
        tokens.append(tagger.pos(text))
    process_time = time.time() - process_time
    print('tagger name = %10s, %.3f secs' % (name, process_time))
    results.append(tokens)
```

