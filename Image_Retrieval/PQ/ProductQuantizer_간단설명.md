# Product Quantizers for k-NN Tutorial

이 글은 https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/ 을 한글로 정리하였습니다.



### 목차

* Exhaustive Search with Approximate Distances(대략적인 거리를 포함한 철저한 검색)
* Explanation by Example(예제별 설명)
* Dataset Compression(데이터셋 압축)
* Nearest Neighbor Search
* Compression Terminology
* Pre-filtering



Product Quantizer는 "Vector Quantizer"의 한 종류로 Approximate Nearest neighbor search를 가속화하는 데 사용할 수 있습니다. 2017년 3월 출시된 페이스북 AI 유사성 검색(FAISS) 도서관의 핵심 요소여서 특히 관심을 끌고 있습니다. 본 튜토리얼의 파트 1에서, 나는 Approximate nearest neighbors search(ANN)을 구현하는 데 사용되는 가장 기본적인 형태로 Product Quantizer에 대한 설명을 제공할 것입니다. 그 다음 파트 2에서 FAISS의 "IndexIVFPQ" 인덱스에 대해 설명합니다. 이 인덱스는 기본 Product Quantizer 위에 몇 가지 기능을 더 추가합니다.



## Exhaustive Search with Approximate Distances

ANN에 사용되는 트리 기반 인덱스와 달리, Product Quantizer를 사용한 K-NN 검색은 여전히 "철저한 검색"을 수행합니다. 즉, Product Quantizer는 여전히 쿼리 벡터를 데이터베이스의 모든 벡터와 비교해야 합니다. 핵심은 거리 계산을 근사하게 하고 크게 단순화한다는 것입니다.



FAISS의 IVFPQ 인덱스는 Product Quantizer를 사용하기 전에 데이터셋의 사전 필터링을 수행합니다. 이 작업은 파트 2에서 다룹니다.



## Explaination by Example

Product Quantizer 접근방식의 저자들은 신호 처리와 압축 기술에 대한 배경을 가지고 있기 때문에, 만약 여러분이 기계 학습에 초점을 맞춘다면, 그들의 언어와 용어는 아마도 이국적인 느낌을 받을 것입니다. 다행히도 k-means 클러스터링에 익숙하다면(그리고 우리는 압축 명명법을 모두 생략합니다!) 예를 들어 Product Quantizer의 기본을 쉽게 이해할 수 있습니다. 이 후 다시 돌아와 압축 용어에 대해 알아보겠습니다.



## Dataset Compression

50,000개의 이미지를 수집했다고 가정해보겠습니다. 이미 CNN을 통해 feature 추출을 수행했고, 이제 각각 1,024개의 구성 요소를 갖춘 50,000개의 feature 벡터 데이터셋을 확보했습니다.

![pq1](../../Image/pq_1.png)

먼저 데이터셋을 압축합니다. 벡터 수는 동일하게 유지되지만 각 벡터에 필요한 Storage 사이즈를 줄일 수 있습니다. 우리가 할 일은 **'차원 감소(Dimensionality Reduction)'** 와 같지 않습니다. 이는 압축 벡터의 값이 실제로는 숫자보다 심볼이기 때문에 압축 벡터를 서로 직접 비교할 수 없기 때문입니다.

데이터셋을 압축할 때 두가지 중요한 이점은 (1) 메모리 액세스 시간이 일반적으로 처리 속도를 제한하는 요소이며  (2) 메모리 용량이 큰 데이터셋에 문제가 될 수 있다는 것입니다.

압축은 다음과 같습니다. 예를 들어 벡터를 길이 128개(서브 벡터 x 128개 = 1,024개)의 8개의 서브 벡터로 나눕니다. 따라서 데이터셋은 각각 [50K x 128]인 8개의 매트릭스로 나뉩니다.



![pq1](../../Image/pq_2.png)

k = 256인 이 8개의 매트릭스 각각에 대해 k-means 클러스터링을 별도로 실행합니다. 벡터의 8개 하위 섹션 각각에 대해 256개의 센터가 있습니다. 각각 8개의 256개의 센터 그룹이 있습니다.



![pq1](../../Image/pq_3.png)

이 중심부들은 "Prototypes"와 같습니다. 데이터 집합 하위 벡터에서 가장 일반적으로 발생하는 패턴을 나타냅니다.

이러한 센터를 사용하여 100만 벡터 데이터 세트를 압축할 예정입니다. 효과적으로, 벡터의 각 하위 영역을 가장 근접하게 일치하는 중심부로 교체하여 원래와는 다르지만 그래도 가까운 벡터를 제공할 것입니다.

이렇게 하면 벡터를 훨씬 효율적으로 저장할 수 있습니다. 원래 부동 소수점 값을 저장하는 대신 클러스터 ID만 저장합니다. 각 하위 벡터에 대해 가장 가까운 중심점을 찾아 해당 중심점의 ID를 저장합니다.

각 벡터는 8개의 centroid id 시퀀스로 대체됩니다. 우리가 어떻게 중심점 ID를 선택하는지 추측할 수 있을 것 같습니다. 각 하위 벡터를 사용하여 가장 가까운 중심점을 찾은 다음 그 중심점 ID로 대체합니다.

각 하위 섹션에 대해 서로 다른 세트의 중심점을 배웁니다. 또한 가장 가까운 중심부의 ID로 서브벡터를 교체할 때 벡터의 해당 하위섹션에 대한 256개의 중심부와만 비교합니다.

256개의 중심점이 있기 때문에 중심점 ID를 저장하려면 8비트만 있으면 됩니다. 처음에 1,024개의 32비트 부동( 4,096바이트)의 벡터였던 각 벡터는 이제 8비트 정수의 시퀀스(벡터당 총 8바이트)가 되었습니다.



![pq1](../../Image/pq_4.png)



## Nearest Neighbor Search

좋습니다. 벡터를 압축했지만 이제 압축 벡터에서 L2 거리를 직접 계산할 수 없습니다. 중심점 ID 사이의 거리는 임의적이고 무의미합니다. (이것은 압축과 차원성 감소를 구분합니다.)

가장 가까운 이웃 검색을 수행하는 방법은 다음과 같습니다. 여전히 철저한 검색(모든 벡터에 대한 거리를 계산한 다음 거리를 정렬할 예정)이지만, 단지 테이블 조회와 약간의 추가 작업만 사용하여 훨씬 효율적으로 거리를 계산할 수 있을 것입니다.

질의 벡터가 있고 가장 가까운 이웃을 찾고 싶다고 가정해 보겠습니다.

이렇게 하는 한 가지 방법은 데이터 집합 벡터의 압축을 푼 다음 L2 거리를 계산하는 것입니다. 즉, 다른 중심부를 결합하여 벡터를 재구성합니다. 우리는 이것을 효과적으로 할 것입니다. 하지만 벡터들을 실제로 분해하는 것보다 훨씬 더 계산적으로 효율적인 방법으로요.

먼저 벡터의 각 하위 섹션과 해당 하위 섹션의 256 중심 요소 사이의 제곱 L2 거리를 계산합니다.

즉, 256개의 행(각 중심선마다 하나씩)과 8개의 열(전방 하위 섹션 하나)을 사용하여 하위 벡터 거리를 표로 작성합니다. 이 테이블을 만드는 데 얼마나 많은 노력이 필요합니까? 생각해 보면, 이 작업은 쿼리 벡터와 256개의 데이터 세트 벡터 사이의 L2 거리를 계산하는 것과 동일한 수의 수학 연산이 필요합니다.

이 표를 구하면 각 50K 데이터베이스 벡터의 대략적인 거리 값을 계산할 수 있습니다.

각 데이터베이스 벡터는 이제 단지 8 centroid id의 시퀀스에 불과합니다. 주어진 데이터베이스 벡터와 쿼리 벡터 사이의 대략적인 거리를 계산하기 위해, 우리는 단지 그 중심선 ID를 사용하여 표의 부분적인 거리를 찾고, 그것들을 합칠 뿐입니다!

그 부분적인 가치들을 요약하는 것이 정말 효과가 있을까요? 네! 제곱근 작동은 안 된다는 의미인 L2 거리를 두고 작업하고 있다는 것을 기억하세요. 제곱 L2는 각 구성 요소 간의 모든 제곱 차이를 합산하여 계산하므로 이러한 추가를 수행하는 순서는 중요하지 않습니다.

따라서 이 테이블 접근 방식은 압축 해제된 벡터에 대한 거리를 계산하는 것과 동일한 결과를 제공하지만 훨씬 더 낮은 계산 비용을 제공합니다.

마지막 단계는 일반적인 가장 가까운 이웃 검색과 동일합니다. 우리는 가장 작은 거리를 찾기 위해 거리를 분류합니다. 그들은 가장 가까운 이웃입니다. 그게 전부입니다!



## Compression Terminology

이제 PQ의 작동 방식을 이해하게 되었으니, 다시 돌아가서 용어를 배우는 것은 쉽습니다.

**Quantizer**란 가장 넓은 의미에서 변수가 가지는 가능한 값의 수를 줄이는 것입니다. 예를 들어 이미지의 색 수를 줄이기 위해 룩업 테이블을 만드는 것이 좋습니다. 가장 일반적인 256색상을 찾아 24비트 RGB 색상 값을 8비트 정수로 매핑하는 테이블에 놓습니다.

데이터베이스 벡터의 처음 128개 값(8개의 하위 섹션 중 첫 번째 부분)을 취하여 256개의 중심점을 학습하기 위해 클러스터링했을 때, 이 256개의 중심점은 **"코드북"**이라고 하는 것을 형성합니다. 각 중심선(구성요소가 128개인 부동 소수점 벡터)을 **"코드"**라고 합니다.

이러한 중심은 데이터베이스 벡터를 나타내는 데 사용되는 것이기 때문에 코드를 **"재생산 값"** 또는 **"재구성 값"**이라고도 합니다. 해당 코드(중심점)를 연결하여 ID가 있는 경우 데이터베이스 벡터를 중심점 순서에서 재구성할 수 있습니다.

8개 서브섹션마다 k-means를 따로 운영했기 때문에 실제로 8개의 코드북을 따로 만들었습니다.

하지만 이 8개의 코드북을 사용하면 코드를 결합하여 256^8개의 벡터를 만들 수 있습니다! 그래서 사실상 256^8 코드를 가진 아주 큰 코드북을 하나 만들었습니다. 그 크기의 코드북 하나를 직접 배우고 저장하는 것은 불가능합니다. 그래서 그것이 바로 제품 퀀타이저의 마술입니다.



## Pre-filtering

튜토리얼의 2부에서는 색인을 다루겠습니다.

FAISS의 IVFPQ: 제품 quantizer를 사용하면서도 데이터 세트를 분할하여 각 쿼리에 대해 데이터 중 일부만 검색하면 됩니다. 

FAISS는 2017년에 출시되었지만, 제품 퀀타이저 접근법과 인덱스에 사용된 기법은 다음과 같습니다.

IVFPQ는 그들의 인기 있는 2011년 논문에서 처음 소개되었습니다.